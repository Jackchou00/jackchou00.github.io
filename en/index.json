[{"content":"Background Requirements Images processed and generated in Python are usually stored as NumPy arrays, and writing these arrays out to files is the final step of the pipeline. In the past, the most common approach was using PIL to encode them as JPEG. As needs around HDR, high bit-depth, and higher compression ratios have grown, I started looking for ways to encode numerical data directly into AVIF images.\nlibavif and libheif Almost all related libraries are Python bindings for libavif or libheif. In practice, you can absolutely export a JPEG or a high bit-depth TIFF first, then call a compiled libavif directly for encoding, passing in all parameters (quality, speed, colour configuration). If all you need is image compression, there’s really no need to route back through Python.\nSo the core requirement is actually to find a good Python binding for libavif that supports passing various parameters, and if the API is elegant, even better.\nLimited options AVIF encoding in Python has seen some progress. PIL added official AVIF support in version 11.3.0, but both reading and writing are limited to 8-bit, and there is no interface for passing parameters. The following two plugins can read/write high bit-depth images and accept parameters.\nBefore PIL 11.3.0, the way to use PIL for AVIF encoding was the pillow-avif-plugin plugin, where advanced parameters could be passed in via keyword arguments.\nAnother choice with PIL is pillow-heif, which is a binding for libheif. After version 1.0.0 it removed AVIF support, with the developer’s reasoning being that PIL already has native AVIF support. The last version with AVIF support is 0.22.0, and advanced parameters also need to be passed via keyword arguments.\nImageIO and OpenImageIO both provide read/write support for various image formats, but they don’t seem to support high bit-depth and advanced parameters (or more accurately, their APIs are a bit complex and I haven’t fully figured them out yet, but it’s either unsupported or a hassle).\nA new choice: ImageCodecs ImageCodecs is another library that provides read/write support for various image formats, using libavif as the backend for AVIF.\nIts strengths lie in its very clean API design. It previously did not support passing colour space parameters, but support was added in the 2025.11.11 release. (If you need something, feel free to “reach out” reasonably in the Issue.)\nfrom imagecodecs import avif_version, avif_encode, AVIF import numpy as np print(AVIF.available) print(avif_version()) array = np.ones((100, 100), dtype=np.uint16) * 65535 # 10-bit, BT.2020 with BT.2100 PQ transfer function # The numeric values for primaries/transfer are defined in H.273 encoded: bytes = avif_encode( array, bitspersample=10, primaries=9, transfer=16 ) with open(\u0026#34;output.avif\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(encoded) All of its image formats follow similar function naming. You just need to check availability and version, then call encode or decode.\nWatch out for the flashbang (sorry, might be too late).\nPeak RGB and white in BT.2020 primaries with the BT.2100 PQ transfer function, 10-bit, encoded with ImageCodecs.\n","permalink":"https://jackchou.top/en/posts/imagecodecs-avif/","summary":"(Flash news) ImageCodecs now supports colour space and transfer function parameters.","title":"A New Python AVIF Encoding Option: ImageCodecs"},{"content":"Here is a temporary page for CIC presentation usage, after the conference, will be re-organized as a paper reivew.\nAll images below are encoded as ISO 22028-5 format using PQ EOTF-inverse curve, should be HDR if you are using MacBook Pro with MiniLED display and Chrome or Safari (macOS Tahoe).\nInput Image, only basic colour conversion has been applied, absolute luminance of the image has been scaled down for less clipping.\nInput Adopted White, which is a blur version of the image above, blur amount here is 3 (a relative number, larger means less blur).\nOutput Adopted White, which is a blur version of the input image, blur amount here is 2 (means more blur, already close to flat). This represents how size will affect, if the display size is larger, more local adaptation should be considered.\nOutput Image, after forward transformation and adjustment in CAM16-UCS and reverse transformation using adopted white above. This should be close to observer\u0026rsquo;s result.\nHere is the ground-truth from observers and a comparison.\nMy pleasure to have you here! Please contact me using e-mail if you have any questions.\n","permalink":"https://jackchou.top/en/posts/cic33-paper/","summary":"Visual Matching Experiment and Tone Mapping based on CAM16-UCS","title":"(Temp) CIC 33 Paper Demonstration"},{"content":"\nThis one has up to 16x headroom\nThis one has up to 16x headroom\n","permalink":"https://jackchou.top/en/photos/hongkong-2025/","summary":"(5 Images) Keep updating during CIC 33","title":"HongKong"},{"content":"Why WSL Various CLI-based AI programming tools, like Codex, are not very good at running commands in PowerShell to edit files precisely. This is partly because they were likely trained more on Unix-like syntax, and partly because PowerShell lacks tools for precise file content manipulation. Therefore, installing WSL and then installing Codex within WSL is a better choice, and it\u0026rsquo;s also the approach recommended by OpenAI.\nTo understand why, we need to compare how the two environments handle files and commands.\nWhen modifying code, an AI agent can generate commands like sed, awk, diff, and patch. These tools act like surgical scalpels, allowing for stream editing of files with precision down to the line, column, or pattern. In contrast, the standard operation in PowerShell is a \u0026ldquo;read-modify-writeback\u0026rdquo; cycle: Get-Content reads a file into memory, you modify the elements, and then Set-Content writes it back. This process is not only more cumbersome but also performs poorly with large files. The AI agent would need to expend more effort to reliably generate complex PowerShell code blocks to accomplish the same modifications.\nWSL2 allows the use of the Linux kernel\u0026rsquo;s native namespace technology to create a secure sandbox. This means the AI agent can run in an isolated environment where its file access is strictly confined to the project directory, and network access can be disabled. This greatly enhances the security of automated code modification.\nError: WSL2 Not Supported Even after enabling virtualisation in the BIOS and ticking the Hyper-V box in Windows Features as per online tutorials (which is actually unnecessary), the installation still fails. It reports that the system does not support WSL2, citing a virtualisation error and that Hyper-V is not installed.\nWsl/InstallDistro/Service/RegisterDistro/CreateVm/HCS/HCS_E_HYPERV_NOT_INSTALLED\nMost solutions online for this issue suggest manually checking and enabling Hyper-V via the command line, or using systeminfo to check for Hyper-V support, but none of these solved my problem.\nThe Invisible Hyper-V In reality, running the wsl --install command should automatically enable Hyper-V and related components, as stated on Microsoft\u0026rsquo;s website:\nThe wsl --install command does the following:\nEnables the optional WSL and Virtual Machine Platform components Downloads and installs the latest Linux kernel Sets WSL 2 as the default Downloads and installs the Ubuntu Linux distribution The failure might be due to some \u0026ldquo;hidden\u0026rdquo; Hyper-V features not being enabled. Even if all Hyper-V related items appear ticked in the \u0026lsquo;Turn Windows features on or off\u0026rsquo; GUI, this could be misleading.\nLet\u0026rsquo;s use the command line to view all features. Note that dism is case-insensitive:\ndism /online /get-features These are usually shown as enabled and correspond to the Hyper-V options you see in the GUI.\nMicrosoft-Hyper-V-All Microsoft-Hyper-V Microsoft-Hyper-V-Tools-All Microsoft-Hyper-V-Management-PowerShell Microsoft-Hyper-V-Hypervisor Microsoft-Hyper-V-Services Microsoft-Hyper-V-Management-Clients However, there are two other features with \u0026lsquo;Hyper-V\u0026rsquo; in their names that don\u0026rsquo;t appear in the GUI (perhaps due to a translation issue from Microsoft) nor are they enabled by the wsl --install command. These are likely the main cause of the failure.\nHyperV-KernelInt-VirtualDevice HyperV-Guest-KernelInt Use the command line to enable these two features, and then restart your system.\ndism /online /enable-feature /featurename:HyperV-KernelInt-VirtualDevice /all /norestart dism /online /enable-feature /featurename:HyperV-Guest-KernelInt /all /norestart After restarting, try the installation again. To avoid re-downloading the distribution (which can be very time-consuming, especially with internet conditions in mainland China), you can first run the wsl command without any arguments or wsl --status to check if the core components are working correctly. If there are no more errors, you can then proceed to install your desired distribution with wsl --install -d \u0026lt;DistroName\u0026gt;.\nIn Conclusion This was a very rare issue. I had never failed to install WSL with wsl --install before, but it happened. Perhaps a future WSL update will enable these two features—which may have been introduced in a recent update—by default.\nDocumenting this kind of tricky problem helps me get a blog post out of it, and it might also help some other unlucky person who runs into the same installation failure. It reminds me of how, back in the day, I\u0026rsquo;d often find solutions to obscure technical problems on old internet forums.\n","permalink":"https://jackchou.top/en/posts/wsl2-hyperv-install-fix/","summary":"Notes in installation of WSL2 for Codex, unknown issue about Hyper-V","title":"Solving WSL2 Installation Failure Caused by 'Invisible' Hyper-V"},{"content":"This is my attempt to turn an interesting paper I\u0026rsquo;ve read into some notes. I will introduce a recently published article and share my findings from trying to replicate it.\nReference: Y. Wang, M. Wei, Z. Ye, T. Hu, and H. Zheng, “An adaptive tone mapping method for PQ ‐ and HLG ‐encoded HDR contents,” Color Res. Appl., p. col.70002, July 2025, doi: 10.1002/col.70002.\nThis is an adaptive tone mapping algorithm that maps 0-1000 nits HDR content to 0-100 nits SDR. It consists of three parts: tone compression, detail enhancement, and dark area enhancement.\nA Special Kind of Luminance The concept of Luminance in this algorithm is quite special. It is not the Y from the XYZ colour space, but rather the maximum of the RGB values, similar to the Value in HSV.\nThis definition of luminance is used to ensure that values always stay within the RGB space during compression, preventing drastic hue shifts that could be caused by going out of gamut. However, it is worth noting that even if the chromaticity coordinates remain the same, a change in luminance can still alter perceived hue and chroma. An example of this is the Bezold–Brücke phenomenon, which describes how our perception of hue can change with light intensity.\nThroughout the paper, \u0026rsquo;luminance\u0026rsquo; refers to this specific concept. The RGB space is always linear Rec. 2020. This article does not cover gamut compression; the output is an image in the Rec. 2020 space but with an SDR luminance range.\nThe Tone Compression Curve The tone compression curve is applied to the special luminance mentioned earlier, compressing the range from 0-1000 down to 0-100. This curve is formed by a linear segment and a rational function, creating an overall shape that resembles a roll off. The entire curve lies below the y=x line, which means no part of the image is brightened in this step.\nThe curve is designed around a few specific luminance points, with the function\u0026rsquo;s coefficients calculated to connect them smoothly.\nIn Appendix 4.2 of the ITU-R BT.2408 report, it is noted that skin tones on an HLG display with a peak luminance of 1000 nits should correspond to 45% to 56% of the HLG signal (with an average of 50%). On an SDR display with a peak luminance of 100 nits, they should correspond to 61% to 82% of the SDR signal (with an average of 70%). Therefore, the luminance of skin tones is approximately 50.7 nits in HDR content and 42.5 nits in SDR content. This 50.7 nits value is also used as the knee point between the linear segment and the curve.\nAnother pair of luminance points is for diffuse white. In a 1000-nit HDR system, the recommended luminance for diffuse white is 203 nits. In an SDR system, it is typically set to a 92% signal value, which is about 81.7 nits.\nDetail Decomposition and Enhancement Compressing luminance leads to a loss of detail, especially in the highlights where textures become hard to distinguish due to reduced contrast. Before compressing the luminance, a bilateral filter is used to separate the base layer from the detail layer. Tone compression is applied only to the base layer. The detail layer, on the other hand, undergoes an additional enhancement process to suppress potential halos around high contrast edges in the image.\nThe paper provides a set of recommended empirical values for the three main parameters of the bilateral filter: window size, spatial variance, and range variance.\nThe additional detail enhancement is based on the local variance of the area surrounding a pixel and the global variance of the entire image. However, the relevant formulae in the paper are almost all incorrect and the textual descriptions are brief, so I have not yet been able to fully replicate this part.\n$$ \\sigma_G = \\sqrt{\\frac{\\sum_{p \\in I} \\left( L_{\\mathrm{HDR}}(p) - \\overline{L}_{\\mathrm{HDR},I} \\right)}{N}} $$$$ \\sigma_L(p) = \\sqrt{\\frac{\\sum_{q_i \\in \\Omega_i} \\left( L_{\\mathrm{HDR}}(q_i) - \\overline{L}_{\\mathrm{HDR},\\Omega_i} \\right)}{N_i}} $$The two formulae above are from the paper, but their results are always 0. If you add squares to make them variance or standard deviation calculations, the subsequent steps still do not work correctly.\nBroadly speaking, the idea is to first calculate the variance (or possibly standard deviation) of all pixels in the entire image. Then, for each pixel, you calculate the variance or standard deviation of the surrounding neighbourhood. A threshold is set based on the global variance, and only the details corresponding to areas exceeding this threshold are enhanced.\nDark Region Enhancement After the first two steps, we have an image suitable for an SDR display. Most images do not need this final step. In my tests, the dark area enhancement is only triggered when the average luminance of the input image is below about 1 nit. The clustering I implemented with SciPy is very slow, but the brightening effect from this step is quite good.\nSince the tone compression curve is always below the y=x line, it reduces the image\u0026rsquo;s overall luminance. If the original content is already quite dark, the output image might become too dim, a problem exacerbated by the poor performance of SDR transfer functions in dark regions. The authors therefore designed an optional module to enhance dark areas. This module is activated only if the average luminance of the mapped image is below a certain threshold; otherwise, the image is output directly.\nFirst, all pixels in the image are grouped into 64 clusters based on their luminance. The average luminance of each cluster is calculated and stored in a 64 element vector. Two gamma values are set, for instance, 1 (for the brightest cluster) and 2.2 (for the darkest cluster). The other 62 gamma values are derived through linear interpolation, with the intervals determined by the average luminance of the clusters.\nThen, the reciprocal of these gamma values is applied to the luminance of the pixels in each respective cluster, brightening the image. This acts as a dynamically adaptive system gamma.\nResults I tested the algorithm using a post processed photo. During editing, I controlled the luminance of different elements in the image to adhere as closely as possible to HDR production standards. If your display is not capable of showing the full HDR luminance range of the image, Chrome should perform Tone Mapping. Apart from the first image, all subsequent images are SDR.\nIf we simply clip any values above the nominal SDR luminance, parts of the image become pure white.\nWith the partially replicated detail enhancement turned off, which means only applying the tone compression curve, the highlights in the image are recovered quite well.\nBy modifying the detail enhancement formula to calculate standard deviation and using a manually specified threshold, I got a more complete result. You can see some processing along the high contrast edges.\nAlthough I couldn\u0026rsquo;t fully replicate the detail enhancement, the tone compression curve applied to the special luminance already produces good results and is very fast. In contrast, the bilateral filter and local variance calculations needed for detail enhancement are computationally expensive.\nIt must be pointed out, however, that the core of this algorithm is the tone compression curve, which is designed based on HDR and SDR production standards and experience. If the HDR content was not created according to these standards and has a more \u0026ldquo;freestyle\u0026rdquo; luminance distribution, the results may not be as effective.\nOnce all the details and potential errors in the algorithm are confirmed, I will refine the article. It would be ideal if the code could be made open source.\n","permalink":"https://jackchou.top/en/posts/wang-et-al-adaptive-tone-mapping/","summary":"Breaking down an innovative tone mapping algorithm and exploring how it balances luminance compression with detail preservation.","title":"Decoding an Adaptive HDR Tone Mapping Algorithm"},{"content":"I prefer tools that remain \u0026lsquo;pure\u0026rsquo;. They should focus on their core functions, allowing users to complete tasks with a smooth learning curve, rather than expanding infinitely through plugins to achieve functions beyond their core purpose. For example, using Obsidian plugins to implement complex features that Markdown does not natively support. For me, Zotero\u0026rsquo;s core responsibility is to efficiently manage the metadata of my references and provide necessary assistance during reading.\nRecently, I discovered that nearly a quarter of the entries in my library had incomplete or incorrect metadata. So, I took some time to tidy it up and used this opportunity to document my current Zotero configuration for future reference.\nA foreword: We sometimes fall into the \u0026rsquo;tool trap\u0026rsquo;. With an ever-increasing number of products, it seems every new app wants to be associated with AI to attract more investment. This inadvertently creates a kind of tool anxiety, as if we\u0026rsquo;ll be left behind if we don\u0026rsquo;t master a certain new tool. However, in most cases, this is not true. The majority of tools only offer minor assistance, and the time spent tinkering with them often outweighs the actual benefits they bring. Installing a hundred plugins for Zotero will not lead to a significant increase in efficiency.\nThat said, tinkering with tools is, for me, a form of positive reinforcement. Just like fiddling with screwdrivers and spanners, playing with these digital tools gives me a sense of accomplishment. Therefore, spending this weekend on this is more meaningful than lying in bed scrolling through short videos.\nFile and Data Syncing Zotero\u0026rsquo;s synchronisation is split into two parts: data and files. For data syncing, you just need to register and log in to a Zotero account. As for file syncing, since Zotero only provides 300 MB of free storage, I opt to use the WebDAV protocol. I tried using a NAS, but considering the convenience of external network access, I eventually chose Jianguoyun. Jianguoyun offers free users 1 GB of upload and download traffic per month, and the speed limit is sufficient for daily use. Although you might hit the traffic limit during the initial sync of a large number of files, subsequent incremental syncs are generally problem-free.\nSome PDF files can be unusually large. For instance, I once came across a paper of just over ten pages that was 48 MB. This is usually because the images within it have not been compressed, for example, high-resolution original images have been embedded directly. In such cases, you can use Adobe Acrobat\u0026rsquo;s file optimisation feature, setting the maximum image PPI to 250-300 and the JPEG compression quality to \u0026lsquo;High\u0026rsquo;, to significantly reduce the file size.\nThe Plugin App Store Add-on Market for Zotero\nThe traditional way to install Zotero plugins is to go to a webpage, download an XPI file, and then manually install it through the settings menu. This plugin, however, provides an integrated app store that supports one-click download, installation, and updates. It makes managing plugins as simple as installing an app on your phone. All the plugins mentioned below can be quickly installed through it.\nHigh-Quality Translation Translate for Zotero\nThis is perhaps the most-starred of all Zotero plugins. Its function is simple and direct: it provides translation for selected text within the Zotero reader interface. The plugin is actively maintained and integrates various translation service APIs, enabling high-quality and fast translation of academic papers.\nLLMs are currently one of the best methods for translation. They can better understand context and provide more accurate translations. In particular, models with short CoT manage to balance both speed and translation quality. It\u0026rsquo;s worth mentioning a specific model within the LLM family called Qwen MT. It focuses on translation tasks, and its API differs from normal OpenAI completion or others. This plugin has been specially adapted for it.\nPDF Downloader Sci PDF\nThis plugin allows you to say goodbye to the tedious process of copying the DOI, going to Sci-Hub to download the PDF, and then manually adding it to Zotero. After you add a reference entry, it will automatically try to find and download the corresponding full-text PDF.\nA Linter for Metadata Linter for Zotero\nThink of it as a \u0026rsquo;linter\u0026rsquo; for your Zotero metadata. It can retrieve, complete, and format all metadata fields using a DOI or ISBN, thus maintaining the integrity and tidiness of your library\u0026rsquo;s information.\nAdditionally, it offers a feature to fill in journal abbreviations to meet the requirements of specific reference formats. For Chinese content, it can also populate the location field based on university names to comply with citation standards like GB/T 7714-2015.\nJasminum Jasminum\nThis plugin is mainly for handling Chinese literature. It can retrieve metadata from CNKI and generate a table of contents for Chinese PDFs. Even if you mostly read English papers, it is particularly useful for managing theses and dissertations from Chinese institutions, as it can accurately fetch information and create a table of contents.\nHigh-Focus Reading Bionic for Zotero\nBionic Reading is a method designed to help non-native speakers improve their reading efficiency. It works by bolding the first few letters of each word, guiding the visual focus to allow for more efficient reading of English text.\nWhile similar extensions exist for Chrome and work well, this plugin\u0026rsquo;s application to PDFs is less than ideal, perhaps due to parsing issues. I only enable it occasionally when I\u0026rsquo;m struggling to concentrate. But usually, in such situations, putting the reading aside and doing something else might be a better choice.\nA Not-So-Awesome GPT Awesome GPT\nThis plugin is like an LLM chat tool with integrated preset prompts. It allows you to ask questions about the current paper within Zotero, such as asking for a summary. It uses RAG technology behind the scenes, but the PDF parsing, vector embedding, and question-answering performance are not ideal, and the user interface could be more intuitive. In comparison, you often get better results by feeding the PDF to more specialised products like NotebookLM. In my actual use, its core function has been reduced to almost exclusively \u0026lsquo;summarise the full text\u0026rsquo;. Its other features are also not as user-friendly as the text-selection features in Cherry Studio.\nReturning to the Essence: Focusing on Reading After all that, the fundamental reason for using Zotero in research and study is to better manage and read articles. No matter how advanced the management and retrieval tools are, or how good the auxiliary and translation methods, they are useless if you don\u0026rsquo;t read. Installing a hundred plugins in Zotero and collecting a thousand beautifully organised papers in it won\u0026rsquo;t increase your actual knowledge one bit. Therefore, once the tools are set up, let\u0026rsquo;s forget about them and return to reading itself.\nAs Gemini says: After all, tools are just the boat and oars; we still have to row ourselves to the shores of knowledge.\n","permalink":"https://jackchou.top/en/posts/zotero-workflow-and-philosophy/","summary":"A guide to my Zotero setup, focusing on \u0026lsquo;simplicity\u0026rsquo; and \u0026rsquo;efficiency\u0026rsquo; with plugins","title":"My Zotero: Configuration and Core Plugins"},{"content":"\n","permalink":"https://jackchou.top/en/photos/goodnight-azukisan/","summary":"\u003cp\u003e\u003cimg alt=\"Azukisan Pin\" loading=\"lazy\" src=\"https://img.jackchou.top/jack-img/2025/09/a3c9f6b1900fb16d04e4f921ff460d02.webp\"\u003e\u003c/p\u003e","title":"Good Night, Azukisan"},{"content":"Schrödinger\u0026rsquo;s Optimal Pigments Yes, that Schrödinger.\nAs early as 1920, Schrödinger published a paper titled Theorie der Pigmente von größter Leuchtkraft. The original text was in German, but thankfully, we now have multilingual LLMs to help. The paper broadly discusses the reflective properties of various pigments, why the colour saturation achievable by pigments is less than that of pure spectral light, and deduces a theoretical \u0026ldquo;optimal pigment\u0026rdquo;. This pigment has a binary reflectance spectrum with respect to wavelength—it either reflects light completely or not at all.\nAt that time, the CIE had not yet introduced the CIE 1931 XYZ colour matching functions, and this was also before the 1924 $V(\\lambda)$ function. The definition of \u0026ldquo;luminance\u0026rdquo; (Helligkeit) in the paper was related to a \u0026ldquo;quantity\u0026rdquo; (Quantität), which was a combination of three colour quantities—a precursor to later concepts of colour matching functions and luminance. Discussions about optimal colours began even when fundamental concepts like luminance were still vague, making it one of the topics with the richest history in colour science.\nMacAdam\u0026rsquo;s Optimal Colours By 1935, with quantified standards for luminance and colour established, MacAdam published a paper proposing the concept of optimal colours, described in the original text as \u0026ldquo;Maximum Visual Efficiency of Colored Materials\u0026rdquo;.\nMacAdam\u0026rsquo;s optimal colours form the outer surface of the \u0026ldquo;theoretical object colour solid\u0026rdquo; in the luminance dimension. That is, they represent the upper limit of luminance for the set of colours that any object relying solely on reflection (non-fluorescent, non-emissive) can theoretically produce under a specific illuminant. This solid is defined by all physically possible reflectance spectra and contains all real object colours, with its surface representing the theoretical limits.\nAn Optimal Colour refers to the colour corresponding to the maximum achievable luminance for each set of chromaticity coordinates (x, y or u\u0026rsquo;, v\u0026rsquo;), i.e., \u0026ldquo;the theoretical object colour with the highest luminance for a given chromaticity\u0026rdquo;. For example, here is a false-colour plot of the luminance of MacAdam\u0026rsquo;s optimal colours under a certain illuminant.\nThe reflectance functions required to achieve these optimal colours take the form of a \u0026ldquo;binary square-wave\u0026rdquo; with only two transition points. Although such reflectance spectra are impossible to achieve in reality, they constitute the theoretical ceiling for object colours.\nProof of the Square-Wave Reflectance Both Schrödinger and MacAdam proved that to achieve an optimal colour, its reflectance must be a binary square-wave with no more than two transition points. Their proofs were based on physical analysis and intuition.\nBelow, we will use the calculus of variations and the Lagrange multiplier method to prove mathematically that for any given chromaticity coordinates, the reflectance that achieves maximum luminance is a \u0026ldquo;square-wave\u0026rdquo;.\nLet\u0026rsquo;s first express these conditions and the problem in mathematical language:\nThe Problem Statement Reflectance R, a function of wavelength $\\lambda$, with values between 0 and 1. Colour Matching Functions (CMFs), used to convert a spectral power distribution into tristimulus values XYZ, are also functions of wavelength, denoted as $\\bar{x}$, $\\bar{y}$, and $\\bar{z}$. The spectral power distribution of the illuminant I and the reflected spectral power distribution P. Any given chromaticity coordinates $x_0$, $y_0$. $$ P(\\lambda) = R(\\lambda)I(\\lambda), \\quad \\text{where } 0 \\le R(\\lambda) \\le 1 $$$$ \\begin{aligned} X \u0026= k \\int P(\\lambda) \\bar{x}(\\lambda) \\, d\\lambda \\\\ Y \u0026= k \\int P(\\lambda) \\bar{y}(\\lambda) \\, d\\lambda \\\\ Z \u0026= k \\int P(\\lambda) \\bar{z}(\\lambda) \\, d\\lambda \\end{aligned} $$$$ x_{0} = x = \\frac{X}{X + Y + Z}, \\quad y_{0} = y = \\frac{Y}{X + Y + Z} $$To prove: for given chromaticity coordinates $(x_{0}, y_{0})$, there exists a reflectance function $R(\\lambda)$ that maximises $Y$. At this maximum, the value of $R(\\lambda)$ can only be 0 or 1.\nDetailed Proof We can prove using the calculus of variations and the Lagrange multiplier method that for any given chromaticity, the reflectance spectrum that achieves maximum luminance must be a \u0026ldquo;square-wave\u0026rdquo; function, taking values of only 0 or 1.\nTo make the derivation clearer, we will proceed in two steps: first, proving the conclusion under an idealised equal-energy illuminant; then, generalising this conclusion to any illuminant to show that the illuminant\u0026rsquo;s spectral distribution does not affect the \u0026ldquo;square-wave\u0026rdquo; form of the optimal reflectance.\nStep 1: Proof under an Equal-Energy Illuminant Let\u0026rsquo;s first assume the illuminant has an equal-energy spectrum, meaning its spectral power distribution $I(\\lambda)$ is a constant. To simplify calculations, we can set $I(\\lambda)=1$ and $k=1$. The tristimulus values are then:\n$$ X = \\int R(\\lambda) \\bar{x}(\\lambda) \\, d\\lambda, \\quad Y = \\int R(\\lambda) \\bar{y}(\\lambda) \\, d\\lambda, \\quad Z = \\int R(\\lambda) \\bar{z}(\\lambda) \\, d\\lambda $$Our goal is to maximise luminance $Y$ subject to the chromaticity coordinate constraints $(x_0, y_0)$. The objective function is:\n$$ \\text{Maximise} \\quad Y = \\int R(\\lambda) \\bar{y}(\\lambda) \\, d\\lambda $$The chromaticity constraints $x=x_0$ and $y=y_0$ can be transformed into two equivalent linear constraints. For notational simplicity, we define $z_0 = 1 - x_0 - y_0$. The constraints are:\n$$ \\int R(\\lambda) \\left( \\bar{x}(\\lambda) - \\frac{x_0}{y_0} \\bar{y}(\\lambda) \\right) d\\lambda = 0 $$$$ \\int R(\\lambda) \\left( \\bar{z}(\\lambda) - \\frac{z_0}{y_0} \\bar{y}(\\lambda) \\right) d\\lambda = 0 $$This is a constrained optimisation problem. We introduce Lagrange multipliers $\\mu_1, \\mu_2$ to construct an auxiliary functional $J[R]$. By combining the objective function and the constraints, the problem is transformed into maximising $J[R]$:\n$$ J[R] = \\int R(\\lambda) f(\\lambda) \\, d\\lambda $$where the core part of the integrand, $f(\\lambda)$, is given by:\n$$ f(\\lambda) = \\bar{y}(\\lambda) - \\mu_1 \\left( \\bar{x}(\\lambda) - \\frac{x_0}{y_0} \\bar{y}(\\lambda) \\right) - \\mu_2 \\left( \\bar{z}(\\lambda) - \\frac{z_0}{y_0} \\bar{y}(\\lambda) \\right) $$Rearranging the above equation by the colour matching functions (CMFs), we get:\n$$ f(\\lambda) = c_1 \\bar{x}(\\lambda) + c_2 \\bar{y}(\\lambda) + c_3 \\bar{z}(\\lambda) $$Here, $c_1, c_2, c_3$ are constants that depend only on $(x_0, y_0)$ and the multipliers $(\\mu_1, \\mu_2)$. This shows that $f(\\lambda)$ is always a linear combination of the CMFs.\nTo maximise the integral $J[R]$ under the physical constraint $0 \\le R(\\lambda) \\le 1$, we must independently maximise the integrand term $R(\\lambda) f(\\lambda)$ at each wavelength $\\lambda$:\nIf $f(\\lambda) \u003e 0$, we should choose $R(\\lambda) = 1$. If $f(\\lambda) \u003c 0$, we should choose $R(\\lambda) = 0$. If $f(\\lambda) = 0$, the value of $R(\\lambda)$ does not affect the result, and these points form the transition points from 0 to 1. Therefore, the optimal reflectance $R(\\lambda)$ can only take values of 0 or 1. Its graph has a \u0026ldquo;square-wave\u0026rdquo; shape, and its transition points are the zero-crossing points of this linear combination of CMFs.\nStep 2: Generalisation to Any Illuminant Now, let\u0026rsquo;s consider an arbitrary illuminant $I(\\lambda)$, where $I(\\lambda) \u003e 0$. The tristimulus value formulae become:\n$$ X = k \\int R(\\lambda) I(\\lambda) \\bar{x}(\\lambda) \\, d\\lambda $$and similar forms for $Y$ and $Z$.\nRepeating the derivation above, each integral term in the objective function and constraints will be multiplied by $I(\\lambda)$. The final auxiliary functional becomes:\n$$ J[R] = \\int R(\\lambda) I(\\lambda) f(\\lambda) \\, d\\lambda $$where the definition of $f(\\lambda)$ is identical to that in the first step. It is still a linear combination of the CMFs, and its form does not depend on the illuminant.\nTo maximise $J[R]$, we need to maximise $R(\\lambda) I(\\lambda) f(\\lambda)$ at each wavelength. Since the illuminant\u0026rsquo;s spectral distribution $I(\\lambda)$ is always positive, the sign of this term is determined entirely by the sign of $f(\\lambda)$. Therefore, the strategy for choosing $R(\\lambda)$ is exactly the same as in the first step.\nThis proves that for any illuminant, the optimal colour at a specific chromaticity must correspond to a square-wave reflectance spectrum with values of only 0 and 1. Its \u0026ldquo;shape\u0026rdquo; (i.e., the positions of the 0/1 transition points) is determined solely by the linear combination of the CMFs and is independent of the illuminant.\nCalculation Method for MacAdam\u0026rsquo;s Optimal Colours One direct approach is: for a given set of chromaticity coordinates xy, solve for the corresponding Lagrange multipliers $\\mu_1, \\mu_2$, then find the zero-crossing wavelengths of the linear combination of CMFs, and use these to construct the square-wave reflectance and calculate the corresponding luminance. However, solving for the Lagrange multipliers from xy is a non-linear process with no explicit solution. Instead, the problem can be approached from another angle to quickly solve for the optimal colour\u0026rsquo;s luminance.\nThe key insight is that a linear combination of the CMFs can, in most cases, produce at most two zero-crossing points (excluding the endpoints), making the square-wave reflectance either a band-pass or a band-stop filter.\nUsing this property, we can quickly iterate through all possible reflectances to build a two-dimensional look-up table that returns the maximum luminance for a given set of chromaticity coordinates.\nThe specific procedure is to define two cut-off wavelengths, $\\lambda_1$ and $\\lambda_2$. One moves from short to long wavelengths, while the other moves from long to short wavelengths. For each pair, we calculate the chromaticity coordinates and luminance for both the band-pass and band-stop reflectances they define. This generates a look-up table of the form $Y=f(x,y)$.\nSignificance MacAdam\u0026rsquo;s optimal colours serve as a theoretical benchmark for measuring the colour reproduction capabilities of devices like printers. The gamut of any reflective device cannot exceed the boundaries defined by the MacAdam limits, providing a theoretical ceiling for the development of colour management and wide-gamut technologies.\nIn the age of HDR, the concept of MacAdam\u0026rsquo;s optimal colours seems poised to find new applications and offer fresh inspiration.\n","permalink":"https://jackchou.top/en/posts/macadam-optimal-colour/","summary":"A mathematical proof: what does the reflectance of a \u0026lsquo;perfect pigment\u0026rsquo; look like?","title":"MacAdam's Optimal Colours: The Boundary of Physical Colours"},{"content":"Standardised Gainmap The previously mentioned Google UltraHDR, OPPO’s early proprietary format, and Apple’s former proprietary format may have different names, but they’re essentially the same approach: HDR images based on Gainmap.\nTo standardise such Gainmap-based images, Adobe, Apple and others jointly developed the ISO 21496-1 standard, formally published in July 2025. Its official title is: Gain map metadata for image conversion. It mainly defines the gain map and related metadata, and how to use it.\nCompared to the previous Wild West of proprietary formats, this standard uses gain maps in much the same way; it mainly differs in how data is organised in the file. Today, almost all Gainmap HDR images use this standard. For example, UltraHDR added support in version 1.1; the same image file can carry both the UltraHDR XMP metadata and ISO 21496-1-compliant metadata.\nIn terms of display, as long as it’s detected and decoded correctly, it looks no different from the older proprietary formats — it’s just a different way of storing the metadata.\nIdentifying ISO 21496-1 metadata For JPEG files, ISO 21496-1 content is stored in the JPEG APP2 marker segment. Both the main image and the gain map have this APP2 segment; the main image stores version info, and the gain map also stores the actual data.\nReference: https://www.iso.org/standard/86775.html\nAccording to the table “C.4.6 Gain map metadata”, the APP2 segment is laid out as follows:\nAPP2 marker (0xFFE2): 2 bytes Marker segment length: 2 bytes Unique identifier (URN): 28 bytes. The value must be urn:iso:std:iso:ts:21496:-1, terminated by a null character. Metadata: remainder (segment length minus 30 bytes) Main image information In the APP2 marker segment of the main image (called the Baseline Image in the ISO), the metadata takes four bytes, holding two unsigned 16-bit integers: minimum version and writer version. In the initial edition of ISO 21496-1, the minimum version should be 0, and the writer version must be greater than or equal to the minimum version.\nGain map data In the gain map, besides the version info, you’ll find the core metadata needed to create and parse the gain map. In UltraHDR, this lives in an XMP packet in the APP1 segment, recorded as text tags, such as the gain map’s maximum and minimum values.\nIn the ISO standard, the storage differs: instead of XMP’s “name + value” pairs, the data is stored in a fixed field order and types, as pairs of numerator and denominator. This keeps numerical precision and calculation accuracy, but it isn’t human-readable like XMP and needs dedicated parsing.\nIn short: first indicate whether the gain map is single-channel or multi-channel; record each channel’s maximum, minimum, gamma and offset. There’s also headroom info, whether it uses the baseline image’s colour space, and so on.\nI wrote a simple Python script that parses the APP2 segment’s binary data into JSON for easier reading, for reference.\nJacksBlog Example: ISO 21496-1 parsing script\nUsage across standards Images that contain only ISO 21496-1 metadata are fairly rare; most are extensions of earlier schemes, so they carry multiple kinds of metadata.\nImages with only ISO 21496-1 metadata include:\nUltraHDR output from Phocus 4.0 (HNCS HDR): although it’s called UltraHDR, it doesn’t include the UltraHDR metadata, only ISO 21496-1. Adobe Sample Gallery: as a promoter of the standard, Adobe provides sample images that carry only the standard metadata. Images with both UltraHDR and ISO 21496-1:\nHasselblad X2D II out-of-camera JPEG. Images with both Adobe Gainmap and ISO 21496-1:\nHDR JPEGs exported by Adobe Camera Raw: have XMP‑labelled metadata and ISO data, but no GContainer. Sigma BF out-of-camera JPEG. JPEGs shot via Project Indigo. Images with only UltraHDR / Adobe Gainmap:\nOPPO Find X6 Pro / X8 Ultra JPEG. Additionally, formats such as HEIF can also comply with ISO 21496-1, but how to detect that isn’t very clear — at least it’s not via the URN. For example, HEIF photos shot on iPhone can be recognised as ISO 21496-1 in the Gain Map Demo app.\nWhat has long been divided will surely unite The idea of storing HDR images with a Gainmap appeared long ago. For example, the 2007 paper below proposed a two-layer approach: one SDR layer plus a residual layer to encode HDR.\nOkuda, M.; Adami, N. Two-Layer Coding Algorithm for High Dynamic Range Images Based on Luminance Compensation. Journal of Visual Communication and Image Representation 2007, 18 (5), 377–386. https://doi.org/10.1016/j.jvcir.2007.06.004.\nOnly in the past few years has the HDR trend reached the tiny phone screen. Early movers put forward their own ways to encode dual-layer images. It wasn’t until this year that the format finally achieved international standardisation, ending the era of a hundred competing approaches. Despite plenty of misunderstandings about HDR images and Gainmap, I believe adoption will only grow as standardisation progresses.\n","permalink":"https://jackchou.top/en/posts/iso-21496-1-gainmap-hdr/","summary":"Say goodbye to messy proprietary formats — ISO 21496-1 brings a unified specification for Gainmap HDR images","title":"HDR Image Formats Explained (IV): ISO 21496-1 — the unified Gainmap standard"},{"content":"The JPEG File Format of HNCS HDR Let\u0026rsquo;s set aside the specific implementation details of HNCS HDR for a moment and look at how Hasselblad chose the file format to store its HDR output.\nThe X2D II 100C is the first medium format camera to support end-to-end HDR capabilities, capable of outputting HDR HEIF or Ultra HDR JPEG photos directly in-camera.\nDPReview released a sample gallery for the X2D II immediately, providing in-camera straight-out JPG and RAW downloads. According to Hasselblad, this JPG complies with the UltraHDR standard.\nLet\u0026rsquo;s examine the internal details of the file:\nThe file contains four complete JPEG structures, with resolutions as follows:\n11656x8742, 5828x4370, 3888x2918, 2592x1944.\nThe first and third are SDR images, while the other two are the corresponding Gainmaps. An XMP data packet, which conforms to the UltraHDR specification and features the Google Container field to identify the existence and size of the Gainmap, can be found in the APP1 segment of each JPEG structure.\nHave you noticed that these resolutions are a bit strange? Gainmaps are generally full resolution or half resolution (half width and height). The Gainmap provided by Hasselblad for the full-resolution SDR image is missing one pixel on the short side, meaning the aspect ratio is not strictly the same, and the Gainmap provided for the lower-resolution SDR image has two-thirds of the original width and height, which is quite unusual.\nIn DPReview\u0026rsquo;s review, they wrote:\nThe company says this gives images with up to three additional stops of dynamic range compared to standard sRGB JPEGs, using a technique adopted by Google in its Pixel phones, and by Sigma in its BF Mirrorless camera.\nThis refers to UltraHDR, and the three additional stops of dynamic range likely means the X2D II specifies HDRCapacityMax = 3 in the Gainmap Metadata, providing up to 3 stops of luminance boost via the Gainmap.\nIn addition to the UltraHDR XMP data packet, the JPEG file also contains various data required by the ISO 21496-1 international standard Gain Map HDR format. This makes it not only compliant with the UltraHDR standard but also a future-proof international standard format, and it satisfies the requirements of UltraHDR v1.1.\nRegarding the Gainmap itself, it has only one channel, meaning it is a single-channel Gainmap, which can only handle luminance mapping. This is slightly less advanced compared to the Sigma BF\u0026rsquo;s three-channel Gainmap. As for other metadata, it does not use the commonly seen 1/64 Offset but sets it directly to 0, and the Gamma is 1, which are all normal.\nConcurrently, the color space of this JPEG file has changed to Display P3, which is also mentioned on their official website:\nTo fully present the magnificent brilliance of HDR, Hasselblad goes one step further than sRGB, expanding the color space to the broader P3.\nFile Format in Phocus Hasselblad immediately updated its Phocus software for macOS and iOS, adding support for HDR editing and output.\nThe sample images provided on its official website, judging from the file names, appear to be processed and output by Phocus, rather than straight-out JPEGs. Indeed, after exploring the file content, this is confirmed, and they are significantly different from the straight-out JPEGs.\nFirstly, they no longer contain the low-resolution images likely used for in-camera preview. There is only one SDR image and one full-resolution Gainmap. Furthermore, this Gainmap is three-channel, and the Capacity is no longer limited to three stops, reaching four stops or more.\nA more important difference is that this JPEG no longer features the UltraHDR structure. It doesn\u0026rsquo;t even have an XMP data packet; it\u0026rsquo;s a file with only ISO 21496-1 metadata, using MPF to identify the Gainmap location. From this perspective, the term UltraHDR used on the Phocus output panel is misleading.\nTemporary Note Such a major update in the camera industry is exciting. I wanted to share the parts I\u0026rsquo;ve already written with everyone. I will add images later and conduct further analysis if I can acquire more sample images.\n","permalink":"https://jackchou.top/en/posts/x2dii-preview/","summary":"Finished the UltraHDR article yesterday, only to have the X2D II release today and use it.","title":"Jumping on the Trend: A Quick Analysis of the X2D II 100C File Format"},{"content":" Reference: https://developer.android.com/media/platform/hdr-image-format\nIdentifying UltraHDR The first JPEG stream in the file is the primary image. In the primary image\u0026rsquo;s APP1, look for an XMP packet, and inside it check for hdrgm:Version=\u0026quot;1.0\u0026quot;, where hdrgm is the namespace identifier http://ns.adobe.com/hdr-gain-map/1.0/. If present, the file can be considered compliant with UltraHDR. For strict UltraHDR (Google\u0026rsquo;s spec), the XMP packet should also include GContainer content, which describes the parts contained in the container, such as the Primary and the GainMap, and their data lengths.\nFor example, below is the first XMP packet from a JPG shot on an OPPO Find X6 Pro, which meets UltraHDR and GContainer requirements.\n\u0026lt;x:xmpmeta xmlns:x=\u0026#34;adobe:ns:meta/\u0026#34; x:xmptk=\u0026#34;Adobe XMP Core 5.1.2\u0026#34;\u0026gt; \u0026lt;rdf:RDF xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#\u0026#34;\u0026gt; \u0026lt;rdf:Description xmlns:Container=\u0026#34;http://ns.google.com/photos/1.0/container/\u0026#34; xmlns:Item=\u0026#34;http://ns.google.com/photos/1.0/container/item/\u0026#34; xmlns:hdrgm=\u0026#34;http://ns.adobe.com/hdr-gain-map/1.0/\u0026#34; hdrgm:Version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;Container:Directory\u0026gt; \u0026lt;rdf:Seq\u0026gt; \u0026lt;rdf:li rdf:parseType=\u0026#34;Resource\u0026#34;\u0026gt; \u0026lt;Container:Item Item:Semantic=\u0026#34;Primary\u0026#34; Item:Mime=\u0026#34;image/jpeg\u0026#34;/\u0026gt; \u0026lt;/rdf:li\u0026gt; \u0026lt;rdf:li rdf:parseType=\u0026#34;Resource\u0026#34;\u0026gt; \u0026lt;Container:Item Item:Semantic=\u0026#34;GainMap\u0026#34; Item:Mime=\u0026#34;image/jpeg\u0026#34; Item:Length=\u0026#34;401160\u0026#34;/\u0026gt; \u0026lt;/rdf:li\u0026gt; \u0026lt;/rdf:Seq\u0026gt; \u0026lt;/Container:Directory\u0026gt; \u0026lt;/rdf:Description\u0026gt; \u0026lt;/rdf:RDF\u0026gt; \u0026lt;/x:xmpmeta\u0026gt; Old Man: MPF Compared with the custom GContainer, the more general Multi-Picture Format (MPF) was proposed by CIPA in 2009 to label and organise multiple frames within a single JPEG file, such as low-resolution previews, 3D imaging, burst frames, and so on. Today, MPF is also commonly used to tag and locate the Gainmap in an HDR image. MPF information is stored in the primary JPEG\u0026rsquo;s APP2 segment, and is organised similarly to EXIF\u0026rsquo;s IFD structure.\nThe basic layout of an APP2 segment is as follows:\nStarts with marker FF E2, followed by a 2-byte segment length. Then 4 bytes MPF identifier \u0026ldquo;4D 50 46 00\u0026rdquo; (i.e. \u0026ldquo;MPF\\0\u0026rdquo;). From the position after the identifier, define the MPF offset base (base, taken as 0). Next 4 bytes are the endianness tag, same as TIFF (for example big-endian is 4D 4D 00 2A). After that, 4 bytes give the offset to the first IFD (relative to the MPF base); if the IFD follows immediately, this value is 8. Inside the MPF IFD:\n2 bytes for the number of entries. Each IFD entry takes 12 bytes; common entries include: MPFVersion (type UNDEFINED, count 4, usually ASCII \u0026ldquo;0100\u0026rdquo;) NumberOfImages (type LONG, count 1) MPEntry (type UNDEFINED; the entry itself stores a relative offset to the actual MP Entry array) At the end there are 4 bytes for the offset to the \u0026ldquo;next IFD\u0026rdquo; (0 if none). The MP Entry array describes each image in the file. Each entry is a fixed 16 bytes, typically containing: 4-byte attributes, 4-byte image data length, 4-byte data offset (relative to the MPF base, pointing to that image\u0026rsquo;s SOI), and two 2-byte indices for dependencies (0 if none).\nBelow is an example (hex) with brief comments:\nFF E2 00 58 # APP2 and segment length 4D 50 46 00 # \u0026#34;MPF\\0\u0026#34; identifier (the position after it is defined as MPF offset base 0) 4D 4D 00 2A # Big-endian tag (same as TIFF) 00 00 00 08 # Offset to the first IFD (relative to base), 8 means immediately following 00 03 # Number of IFD entries: 3 B0 00 00 07 00 00 00 04 30 31 30 30 # MPFVersion: type 07 (UNDEFINED), count 4, \u0026#34;0100\u0026#34; B0 01 00 04 00 00 00 01 00 00 00 02 # NumberOfImages: type 04 (LONG), value 2 B0 02 00 07 00 00 00 20 00 00 00 32 # MPEntry: type 07, count 32, data offset 0x32 00 00 00 00 # Next IFD offset: none (0) # MP Entry array (16 bytes per entry) 00 03 00 00 00 5B ED A0 00 00 00 00 00 00 00 00 # Entry 1 (primary): length 0x005BEDA0, offset 0 (primary SOI lies before MPF; offset cannot be negative, so record 0) 00 00 00 00 00 06 1F 08 00 5B E4 87 00 00 00 00 # Entry 2 (Gainmap): length 0x00061F08, offset 0x005BE487 (relative to MPF base, points to the second JPEG stream\u0026#39;s SOI) In this example:\nThe second image (Gainmap) length 0x00061F08 matches the Gainmap size recorded in XMP (401160); its data offset 0x005BE487 points to the second JPEG stream\u0026rsquo;s SOI, relative to the MPF base. The first (primary) lists the full codestream length 0x005BEDA0; because the primary SOI appears before the MPF base, the data offset cannot be negative, so it is written as 0. For more detail, see CIPA\u0026rsquo;s official \u0026ldquo;DC-x007-2009 Multi-Picture Format\u0026rdquo;.\nDeep Dive into Gainmap Pedantry corner: UltraHDR refers to using GContainer in the primary to mark locations; the Gainmap with XMP is not defined by UltraHDR, but by Adobe\u0026rsquo;s standard.\nOnce you\u0026rsquo;ve determined whether the image conforms to UltraHDR and whether the Gainmap is located via GContainer or MPF, you can then find the Gainmap and its metadata.\nA Gainmap also uses XMP to store metadata. In the located Gainmap JPEG stream, look for an APP1 segment with XMP.\n\u0026lt;x:xmpmeta xmlns:x=\u0026#34;adobe:ns:meta/\u0026#34; x:xmptk=\u0026#34;Adobe XMP Core 5.1.2\u0026#34;\u0026gt; \u0026lt;rdf:RDF xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#\u0026#34;\u0026gt; \u0026lt;rdf:Description xmlns:hdrgm=\u0026#34;http://ns.adobe.com/hdr-gain-map/1.0/\u0026#34; hdrgm:Version=\u0026#34;1.0\u0026#34; hdrgm:GainMapMin=\u0026#34;0\u0026#34; hdrgm:GainMapMax=\u0026#34;2.16048\u0026#34; hdrgm:Gamma=\u0026#34;1\u0026#34; hdrgm:OffsetSDR=\u0026#34;0\u0026#34; hdrgm:OffsetHDR=\u0026#34;0\u0026#34; hdrgm:HDRCapacityMin=\u0026#34;0\u0026#34; hdrgm:HDRCapacityMax=\u0026#34;2.16048\u0026#34; hdrgm:BaseRenditionIsHDR=\u0026#34;False\u0026#34; hdrgm:OplusScale=\u0026#34;4.47065\u0026#34;/\u0026gt; \u0026lt;/rdf:Description\u0026gt; \u0026lt;/x:xmpmeta\u0026gt; Again, there\u0026rsquo;s a namespace hdrgm pointing to Adobe\u0026rsquo;s Gainmap standard, which records the metadata a Gainmap needs (OplusScale is OPPO\u0026rsquo;s private field).\nNext, using the Gainmap encoding process as a guide, let\u0026rsquo;s briefly explain these parameters. First, this is a single-channel Gainmap; encoding generally uses the Gainmap to record differences in luminance. Luminance is computed from RGB colour primaries as a weighted combination of RGB pixel values, essentially the second row of the RGB-\u0026gt;XYZ matrix (i.e. computing a Y value). For a three-channel case, you operate directly on RGB per channel.\nLet Yhdr and Ysdr be the HDR and SDR after conversion to luminance. Both need to be converted to linear space and be on the same scale.\n$$ \\text{pixel gain} = \\frac{Y_{\\text{hdr}} + \\text{offset hdr}}{Y_{\\text{sdr}} + \\text{offset sdr}} $$The two offsets serve a few purposes; commonly you can use 1/64 as the offset:\nEnsure Y hdr + offset hdr is always positive for the logarithm. Avoid division issues when Y sdr is 0. Improve encoding precision in dark regions Then take log base 2 of pixel gain, and record the GainMapMin and GainMapMax at this point.\nWhen normalising pixel gain to 0-1, UltraHDR uses max/min content boost instead of the recorded Gainmap min/max. The difference stems from the offsets. Personally, I think using the Gainmap values is better. After that, clamp anything outside 0-1.\nWhat is content boost: this controls the brightness of the HDR content and can be defined by the creator. Google uses the term \u0026ldquo;implementation-defined\u0026rdquo;. Compared with objectively computing the HDR/SDR brightness ratio, this user-defined parameter enables subjective effects. For example, if you want every HDR pixel to be brighter than its SDR counterpart, set the min content boost to 1.\nNext, apply a power function (Gamma) to the normalised pixel gain. In most cases Gamma can be 1; if the Gainmap contains lots of detail, you can use a slightly larger Gamma.\nFinally, stretch the 0-1 Gainmap to 0-255 and encode it as a JPEG image. A JPEG quality no lower than 85-90 is recommended.\nAs for HDR Capacity Min/Max, they represent the display\u0026rsquo;s HDR capability. I\u0026rsquo;m not entirely sure why a display-related quantity is stored in the image; perhaps the idea is similar to the \u0026ldquo;grading display\u0026rdquo; in HDR video workflows, recording the creator\u0026rsquo;s display to assist tone mapping. Most of the time, these values are set the same as Gainmap Min/Max.\nThere\u0026rsquo;s also a field indicating whether the base image is SDR or HDR, BaseRenditionIsHDR, usually False.\nDecoding and tone-mapping Here we introduce another variable: Display Boost. It records the ratio between the HDR white point and the SDR white point of the current display, similar to Apple\u0026rsquo;s Headroom. For example, if SDR white is 100 nits and HDR white is 1600 nits, the Display Boost is 16.\nIt is used during UltraHDR decoding, mainly when Display Boost is less than Content Boost, i.e. the device cannot fully display the HDR brightness of the content. A weight is introduced to control the brightness of the HDR portion so that it remains within the screen\u0026rsquo;s capability.\nThe weight is calculated as follows, with all quantities in log2 non-linear space.\n$$ \\text{weight} = \\frac{\\text{max display}-\\text{min capacity}}{\\text{max capacity}-\\text{min capacity}} $$Then clamp it to the 0-1 range, where 1 means the device can display the full HDR content. It is applied to G, the log2 result computed earlier from HDR, SDR, and the two offsets.\nDuring encoding:\n$$ G = \\log_2 \\frac{\\text{HDR}+k_{\\text{hdr}}}{\\text{SDR}+k_{\\text{sdr}}} $$During decoding:\n$$ \\text{HDR} = (\\text{SDR}+k_{\\text{sdr}}) \\cdot 2^{G \\cdot \\text{weight}}-k_{\\text{hdr}} $$When the weight is 1, the computed HDR equals the previously encoded HDR. When the weight is less than 1, the HDR you get reduces the Gainmap contribution according to the screen\u0026rsquo;s actual capability, producing an \u0026ldquo;in-between\u0026rdquo; version, down to weight 0 which shows a fully SDR version.\nCompatibility with ISO 21496-1 ISO 21496-1 is the international standard for gain-map type HDR still images. It no longer uses XMP in APP1 to store Gainmap metadata; instead it defines a dedicated metadata segment in APP2 to follow the standard. The metadata itself is basically the same.\nUltraHDR 1.1 is compatible with ISO 21496-1, which in practice means you have both the XMP in APP1 and the standardised segment in APP2.\nThe pioneer that ended the Warring States era Before UltraHDR, dual-layer HDR still images were in a Warring States-era free-for-all: each phone brand had its own standard, and they were mutually incompatible.\nGoogle introduced the UltraHDR standard in Android 13. By using GContainer and MPF to indicate the Gainmap\u0026rsquo;s location, it helps decoders correctly find the second JPEG frame defined by Adobe\u0026rsquo;s Gainmap standard. With a very lean approach and minimal changes, it elegantly achieves backwards compatibility.\nAt present, almost all phone galleries (devices shipping with Android 13 or newer, or iPhones with OLED screens) and Chromium-based browsers can handle this format and most of its variants.\nAlthough it is limited by JPEG\u0026rsquo;s relatively low compression efficiency, 8-bit depth, and lossy compression, UltraHDR is still a solid upgrade to JPG.\nFrom here, dual-layer HDR formats should gradually move to HEIF, AVIF, or even JPEG XL under the ISO 21496-1 standard. But JPEG, as the most widely used image format, will remain in long-term, heavy use.\n","permalink":"https://jackchou.top/en/posts/hdr-format-ultrahdr-mpf/","summary":"A backwards-compatible HDR image scheme based on dual-layer JPEG and Gainmap, now widely adopted","title":"HDR Image Formats Explained (III): UltraHDR and MPF"},{"content":"What is a JPG To be precise, JPEG is a compression method that describes how to turn an image into a byte stream. What we commonly call a JPG file is actually a JFIF file, whose binary structure is a hierarchical sequence composed of multiple \u0026ldquo;Markers\u0026rdquo;. However, just as in everyday communication, the following text will frequently mix these concepts.\nSimply put, a JPEG file is a collection of marker segments and compressed image data. Each marker segment begins with a specific marker that defines the purpose of that segment\u0026rsquo;s data.\nFor example, the beginning of a file is a Start of Image (SOI) marker, with the binary code FF D8, indicating that what follows is a JPG file. Its corresponding End of Image (EOI) marker is FF D9, but due to various modifications to the JPEG format, the EOI marker doesn\u0026rsquo;t necessarily appear at the very end of the file.\nAfter the SOI marker, there are some APP marker segments that provide additional information. Their identifier is the binary code FF Ex, where x can be from 0-15. The two bytes following the identifier record the length of this segment (including these two length-identifying bytes), followed by the content. Note that they are not necessarily sequential or unique; for instance, a file can have two APP1 segments.\nAPP0 is the JFIF application segment. Although it\u0026rsquo;s supposed to be mandatory, many files don\u0026rsquo;t have it. APP1 usually stores EXIF or XMP information, used for camera parameters or image-related metadata. APP2 typically holds an ICC profile. In the UltraHDR specification, HDR-related information is stored as XMP metadata in an APP1 segment.\nSimilarly, the Quantization Tables and Huffman Tables generated during JPEG compression are also placed in marker segments: DQT, with the binary code FF DB, defines the Quantization Table, and DHT, with the binary code FF C4, defines the Huffman Table.\nThen comes the Start of Frame 0 (SOF0) marker for baseline DCT (which must come after DQT), with the binary code FF C0. It stores the image\u0026rsquo;s width, height, bit depth, and other information. And then the Start of Scan (SOS) marker (which must come after the above marker segments), with the binary code FF DA. The encoded image data immediately follows the SOS part, so the two bytes in SOS that record its own length are very important, as they define the boundary between the metadata and the actual image data.\nWithin the byte stream, some FF data might appear. To prevent them from being interpreted as markers, they are followed by a 00 byte. At the end of the byte stream, the EOI marker FF D9 is used to signify the end. But other data can still follow.\nMulti-frame JPEGs In some files, you can find another SOI after an EOI, followed by another complete JFIF structure. This simple concatenation allows for several complete JPEGs to be stored in one file. Some are used as a gain map to achieve HDR, while others store a separate low-resolution image for quick previews on the camera. The biggest advantage of this concatenation is backward compatibility. Image viewers that don\u0026rsquo;t recognise multiple frames will only read the first image without throwing an error, gracefully handling backward compatibility.\nNote that \u0026ldquo;multi-frame\u0026rdquo; here does not refer to a thumbnail. Although a thumbnail also has a complete JPG structure, it\u0026rsquo;s usually placed inside the APP0 (JFIF) or APP1 (EXIF) segment as part of the main image. Therefore, you can\u0026rsquo;t split multi-frame JPEGs simply by searching for SOI and EOI markers, because you\u0026rsquo;ll be misled by the markers within the thumbnail. In a multi-frame structure, the next SOI doesn\u0026rsquo;t necessarily follow immediately after an EOI, so you can\u0026rsquo;t just search for FF D9 FF D8 to detect a multi-frame structure either.\nIf you just want a simple way to identify multi-frame structures in a JPG file, one simple strategy is to use a \u0026ldquo;stack\u0026rdquo;. Scan from the beginning; when you find an SOI, push it onto the stack. When you find an EOI, pop an SOI from the top of the stack to pair with it. This handles the nested structure caused by thumbnails. However, it\u0026rsquo;s very likely that other marker segments also contain byte sequences identical to SOI and EOI markers, so this method is not very reliable and isn\u0026rsquo;t recommended.\nThere is a standard for multi-frame JPG files called MPF (Multi-Picture Format), developed by CIPA, which specifies the size and offset of subsequent frames in the APP2 segment of the first JFIF. However, this standard is rarely followed, and most cases are just simple, unmarked concatenations.\nOther Data In JPG images taken with an OPPO phone, you can find two complete JFIF structures. After the second EOI, you can still find some other data, including the phone model name, a JSON snippet, and so on.\n[ { \u0026#34;length\u0026#34;: 4, \u0026#34;name\u0026#34;: \u0026#34;private.emptyspace\u0026#34;, \u0026#34;offset\u0026#34;: 51, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;length\u0026#34;: 47, \u0026#34;name\u0026#34;: \u0026#34;watermark.device\u0026#34;, \u0026#34;offset\u0026#34;: 47, \u0026#34;version\u0026#34;: 1 } ] These are likely fields used for handling watermarks in the phone\u0026rsquo;s built-in gallery app. This proprietary data can also interfere with logic that relies solely on the EOI marker to determine the end of the file.\nCode 02 JPEG Structure\njpeg_parser.py: To extract data needed for image processing and colour science from JPEGs, and considering that existing Python libraries don\u0026rsquo;t handle concatenated multi-frame JPEGs well, I wrote a simple JPEG parsing tool. It can currently parse multi-frame JPEGs correctly and extract XMP from the APP1 segment of each frame.\ncheck_soi_eoi.py: A script that uses a stack-based approach to pair SOI and EOI markers. It can correctly find all JFIF structures within a file without parsing other markers, but it cannot handle cases where identical byte codes might appear in other marker segments.\nNext, we will use this information to explore the encoding and decoding of HDR images stored in the JPEG format.\n","permalink":"https://jackchou.top/en/posts/jpeg-structure/","summary":"A deep dive into the structure of JPEG and the modifications made to support new features","title":"HDR Image Format Analysis (II): JPEG and Its Modifications"},{"content":"Getting the Right Perspective on LLMs Large Language Models (LLMs) aren\u0026rsquo;t all-powerful magic, but rather powerful tools. It\u0026rsquo;s crucial to have the right perspective on LLMs, understand their limitations and characteristics, and choose the right model for the right job. We shouldn\u0026rsquo;t use them for tasks they aren\u0026rsquo;t good at, or try to force a square peg into a round hole.\nMathematical Calculations LLMs are not inherently good at precise mathematical calculations in a native conversation. Many models might fail to directly compare 9.11 and 9.9, or even, when analysing the pack14 function below, might make an absurd claim like 6 * 14 = 7 * 8 just to make its logic seem consistent.\nA brief explanation of the principle: This is a result of both the tokenisation mechanism and the fundamental nature of the model. LLMs break text down into \u0026ldquo;tokens\u0026rdquo;. A number like 9.11 might be split into three tokens: 「9」, 「.」, and 「11」. When the model processes this, it sees a sequence pattern, not a single numerical value. It is fundamentally a language pattern matcher, not a symbolic calculator. Although it can \u0026ldquo;memorise\u0026rdquo; simple calculation results (like 2+2=4) by learning from vast amounts of text, it can easily make mistakes with slightly more complex, uncommon, or multi-step calculations.\nTherefore, rather than asking it to perform high-risk calculations directly, it\u0026rsquo;s better to leverage its coding abilities.\nFor example, the following is a poor way to ask. CIECAM16 involves many computational steps, and even Gemini 2.5 Pro cannot calculate it directly and accurately, and it takes a long time.\nXYZ = [19.01, 20.00, 21.78] XYZ_w = [95.05, 100.00, 108.88] L_A = 318.31 Y_b = 20.0 surround = \u0026#34;Average\u0026#34; Please calculate the CIECAM16 model\u0026#39;s predicted appearance attributes based on the input above. It\u0026rsquo;s better to just ask it to write a Python function. A better way to ask is:\nPlease write a Python function that takes the CIECAM16 model\u0026#39;s input parameters (XYZ, XYZ_w, L_A, Y_b, surround) and returns the calculated appearance attributes. Please use the NumPy library for numerical operations. This way, a top-tier model like Gemini 2.5 Pro can provide code that is quite complete and very close to correct, but for such niche and complex formulae, asking the LLM to answer purely from its own knowledge is asking too much.\nA small tip: Gemini parses PDFs via native multimodality. It turns each page into a number of tokens rather than directly parsing the text or content. This works well for poor layouts with lots of figures (especially academic papers). You can find more technical details in the docs. For example, if you feed the CAM16 paper to Gemini, it can accurately locate all the formulae and reproduce them correctly.\nReasoning: The Value of Thought For complex problems that require multi-step analysis, choosing a model that excels at reasoning is highly valuable. The core value lies in the Chain-of-Thought (CoT), where the model shows its step-by-step thinking process, which is sometimes more valuable than the answer itself.\nA clear and complete chain of thought allows you to:\nVerify its logic: Understand how it arrived at the conclusion, thereby judging the reliability of the conclusion. Spot errors: If the model makes a mistake in a certain step, you can clearly see where the problem lies. Learn new approaches: Observing the model\u0026rsquo;s thought process can sometimes offer you new perspectives for solving a problem. DeepSeek\u0026rsquo;s R1 is a good choice; its chain of thought is complete and detailed without being excessively long-winded. For other models, you can try adding the phrase \u0026ldquo;Let\u0026rsquo;s think step by step.\u0026rdquo;\nCut Your Losses Nowadays, LLMs have increasingly large context windows, with some models even offering millions of tokens of context length. But this doesn\u0026rsquo;t mean they can always maintain a high level of performance in long conversations. In fact, a long context is a double-edged sword, especially when the model starts making mistakes.\nWhen you try to repeatedly correct a model that is making errors in a conversation, its previous incorrect answers are packaged into the new context as history. This creates a contaminated context, leading the model into a vicious cycle of logical confusion.\nYou will observe that the model may start to get stuck on its own flawed reasoning. Even if you point out the problem, it struggles to break free. A very clear \u0026ldquo;red flag\u0026rdquo; is when the model, after making repeated mistakes, starts to apologise frequently and intensely, using emotionally charged words like \u0026ldquo;I\u0026rsquo;m so sorry,\u0026rdquo; \u0026ldquo;I was completely wrong,\u0026rdquo; or \u0026ldquo;Let me try again.\u0026rdquo; This usually means its reasoning chain has been thoroughly corrupted.\nAt this point, the wisest course of action is to cut your losses. Don\u0026rsquo;t waste any more tokens and time \u0026ldquo;pushing\u0026rdquo; or \u0026ldquo;teaching\u0026rdquo; it; this will most likely only get you more incorrect information.\nThe correct approach is:\nEdit and retry: If your tool supports it, simply delete the conversation turns starting from where the error occurred. Then, modify your prompt with more explicit constraints or directly rule out the line of reasoning where it previously failed, and ask again. Start from scratch: This is the cleanest method. Open a new chat and design a better initial prompt. Incorporate what you learned from the previous failure, such as giving the model more background information, clearer instructions, or even telling it to be wary of certain potential pitfalls. Collaborating with an LLM is more like setting the initial parameters for a complex computation than teaching a student. Your goal is to initiate a correct chain of thought, not to fix one that is already in disarray.\nKnowledge and Hallucinations Without access to the internet or external tools, an LLM\u0026rsquo;s knowledge is stored entirely within its vast model parameters, which is known as parametric knowledge. This knowledge consists of patterns it has \u0026ldquo;memorised\u0026rdquo; from its massive training data. For a niche field like colour science, models with over 400B parameters and a rich store of world knowledge tend to have a relatively comprehensive understanding.\nThis leads to the problem of hallucination. When you ask a model for specific paper information or request it to write a professional literature review, it is very likely to invent bibliographic information.\nThe correct approach is to use tools and internet access, such as Retrieval-Augmented Generation (RAG). Many modern LLM products (like Gemini with its integrated Google Search, or some Deep Research tools) have the ability to search the web. They will first conduct a web search based on your question and then organise and answer based on reliable, real-time sources, which greatly improves the accuracy and timeliness of the answers. Gemini and Grok perform relatively well in this regard.\nAdditionally, asking the model questions like \u0026ldquo;Who are you?\u0026rdquo; or \u0026ldquo;What\u0026rsquo;s the date today?\u0026rdquo; doesn\u0026rsquo;t reflect its true performance, as these answers are typically hardcoded in the product\u0026rsquo;s system prompt, otherwise the model wouldn\u0026rsquo;t be able to answer.\nAssessing a Model\u0026rsquo;s True Capabilities When a new model is launched, claiming to be the new state-of-the-art (SOTA), how can you quickly test its capabilities and see if it performs well in colour science and image processing? Here are some test questions I\u0026rsquo;ve accumulated to quickly try out a model.\nLogical Trap in a Bit-Packing Function import numpy as np def pack10(data : np.ndarray) -\u0026gt; np.ndarray: # Function to pack 10-bit data into an 8-bit array out = np.zeros((data.shape[0], int(data.shape[1]*(1.25))), dtype=np.uint8) out[:, ::5] = data[:, ::4] \u0026gt;\u0026gt; 2 out[:, 1::5] = ((data[:, ::4] \u0026amp; 0b0000000000000011) \u0026lt;\u0026lt; 6) out[:, 1::5] += data[:, 1::4] \u0026gt;\u0026gt; 4 out[:, 2::5] = ((data[:, 1::4] \u0026amp; 0b0000000000001111) \u0026lt;\u0026lt; 4) out[:, 2::5] += data[:, 2::4] \u0026gt;\u0026gt; 6 out[:, 3::5] = ((data[:, 2::4] \u0026amp; 0b0000000000111111) \u0026lt;\u0026lt; 2) out[:, 3::5] += data[:, 3::4] \u0026gt;\u0026gt; 8 out[:, 4::5] = data[:, 3::4] \u0026amp; 0b0000000011111111 return out def pack12(data : np.ndarray) -\u0026gt; np.ndarray: # Function to pack 12-bit data into an 8-bit array out = np.zeros((data.shape[0], int(data.shape[1]*(1.5))), dtype=np.uint8) out[:, ::3] = data[:, ::2] \u0026gt;\u0026gt; 4 out[:, 1::3] = ((data[:, ::2] \u0026amp; 0b0000000000001111) \u0026lt;\u0026lt; 4) out[:, 1::3] += data[:, 1::2] \u0026gt;\u0026gt; 8 out[:, 2::3] = data[:, 1::2] \u0026amp; 0b0000001111111111 return out def pack14(data : np.ndarray) -\u0026gt; np.ndarray: # Function to pack 14-bit data into an 8-bit array out = np.zeros((data.shape[0], int(data.shape[1]*(1.75))), dtype=np.uint8) out[:, ::7] = data[:, ::6] \u0026gt;\u0026gt; 6 out[:, 1::7] = ((data[:, ::6] \u0026amp; 0b0000000000000011) \u0026lt;\u0026lt; 6) out[:, 1::7] += data[:, 1::6] \u0026gt;\u0026gt; 8 out[:, 2::7] = ((data[:, 1::6] \u0026amp; 0b0000000000001111) \u0026lt;\u0026lt; 4) out[:, 2::7] += data[:, 2::6] \u0026gt;\u0026gt; 6 out[:, 3::7] = ((data[:, 2::6] \u0026amp; 0b0000000000111111) \u0026lt;\u0026lt; 2) out[:, 3::7] += data[:, 3::6] \u0026gt;\u0026gt; 8 out[:, 4::7] = ((data[:, 3::6] \u0026amp; 0b0000000000001111) \u0026lt;\u0026lt; 4) out[:, 4::7] += data[:, 4::6] \u0026gt;\u0026gt; 6 out[:, 5::7] = ((data[:, 4::6] \u0026amp; 0b0000000000111111) \u0026lt;\u0026lt; 2) out[:, 5::7] += data[:, 5::6] \u0026gt;\u0026gt; 8 out[:, 6::7] = data[:, 5::6] \u0026amp; 0b0000000011111111 return out Please explain in detail what these three Python functions do. This code is from the PiDNG library and is intended to compress high-bit-depth data (10-bit, 12-bit, 14-bit) into an 8-bit uint8 array. The implementations for pack10 and pack12 are correct.\npack10: 4 x 10-bit values (40 bits) -\u0026gt; 5 x 8-bit values (40 bits). pack12: 2 x 12-bit values (24 bits) -\u0026gt; 3 x 8-bit values (24 bits).\nHowever, the pack14 function is incorrect. It tries to pack 6 x 14-bit values (6 * 14 = 84 bits) into 7 x 8-bit bytes (7 * 8 = 56 bits), which is mathematically impossible. The correct implementation should pack 4 x 14-bit values (4 * 14 = 56 bits) into 7 bytes.\nCommon mistake: Explaining the code of pack14 line by line without realising its logical error. Or, in an attempt to make the code seem consistent, fabricating an incorrect mathematical explanation, such as claiming that 6 * 14 equals 56.\nConcept of an RGB Gamut Cube How to determine if a given XYZ tristimulus value is within the gamut of an RGB space. The RGB space is defined by four CIE xy coordinates, representing red, green, blue, and white (where the white point\u0026#39;s luminance is normalised to 1.0). Please provide the corresponding Python code implementation. This question tests whether the LLM understands that an RGB gamut is a three-dimensional cube (or parallelepiped), not a two-dimensional triangle.\nThe correct solution:\nConstruct the transformation matrix: Use the xy coordinates of the three primaries (red, green, blue) and the white point to calculate the 3x3 transformation matrix M from the RGB space to the CIE XYZ space. Invert the matrix: Calculate the inverse matrix M_inv, which is the transformation matrix from XYZ to RGB. Transform the coordinates: Left-multiply the given XYZ value by M_inv to get the corresponding RGB values. Check the range: Check if the calculated R, G, and B components are all within the closed interval of [0, 1]. If they all are, the XYZ value is within the RGB gamut; otherwise, it is out of gamut. A common incorrect solution:\nConverting the input XYZ value into xy coordinates as well. Then, checking on the CIE xy chromaticity diagram if this point lies within the triangle formed by the xy coordinates of the R, G, and B primaries. This method completely ignores the colour\u0026rsquo;s luminance (Y) information and is incorrect. A colour might have the correct chromaticity but be out of the target gamut because it is too bright or too dark. CIECAM16 Code Implementation Please write a Python function that takes the CIECAM16 model\u0026#39;s input parameters (XYZ, XYZ_w, L_A, Y_b, surround) and returns the calculated appearance attributes. Please use the NumPy library for numerical operations. In the main function, calculate for: XYZ = [19.01, 20.00, 21.78] XYZ_w = [95.05, 100.00, 108.88] L_A = 318.31 Y_b = 20.0 surround = \u0026#34;Average\u0026#34; This tests the model\u0026rsquo;s world knowledge and programming ability. For a complex model like CIECAM16, it\u0026rsquo;s better to provide the LLM with the full PDF standard, but large-parameter models are capable of writing correct code directly. The best performers are Gemini 2.5 Pro, GPT 5 (high), and DeepSeek R1 0528, all of which made only minor mistakes in 1-2 formulas. The correct output is:\n{ \u0026#34;J\u0026#34;: 41.73120790512664, \u0026#34;C\u0026#34;: 0.10335573870906986, \u0026#34;h\u0026#34;: 217.067959767393, \u0026#34;Q\u0026#34;: 195.37170899282242, \u0026#34;M\u0026#34;: 0.10743677233590453, \u0026#34;s\u0026#34;: 2.3450150729795514 } GPT 5 Pro is now callable via the API, it\u0026rsquo;s very expensive and can take a few minutes to produce a reply. That said, it gives the most complete and closest-to-correct result.\n6 November 2025 — Documenting humanity’s doomed moment: the leaked gemini-3-pro-preview-11-2025 is the first large language model capable of writing a completely error-free implementation of CIECAM16 from scratch, with no reference.\nThe 3D shape of xyY In a finite RGB space where each of the three components ranges from 0 to 1, the values can be converted to XYZ through a 3×3 matrix. If plotted in a 3D coordinate system, RGB forms a cube, XYZ forms a parallelepiped. XYZ can then be converted to xyY. When we plot xyY in a 3D coordinate system, using xy as the base plane and Y as the z-axis, what shape do we get? The linear transformation from RGB to XYZ is straightforward: it stretches a unit cube into a parallelepiped. The conversion to xyY is a non-linear transformation, which can be seen as a projective transformation. The resulting volume is a more complex 3D shape.\nFor the black point at (0, 0, 0), there is an issue of non-definability when computing chromaticity coordinates. One approach is to define the entire base plane as black, or to treat the base of the volume as an “open set”.\nThe line segments connecting pure red, green and blue are perpendicular to the base plane; their projections form a triangle. The secondary colours cyan, magenta and yellow, obtained by mixing the primaries, lie on the three edges of this triangle. The connections between a primary and the mixtures of the other two primaries are curves. For example, red-yellow-green is linked by two curves. Black-red-yellow-green: these four points form the vertices of a “pentagon” (with a black segment as the bottom edge). This lies in a plane perpendicular to the base. The other two corresponding faces are similar. Red-yellow-white-magenta: these four points form the vertices of a four-edged curved surface in space. The other two corresponding faces are similar, together making a tent-like top. Altogether they form a solid with seven faces, three of which are curved surfaces converging to the white point at the top, and three are planes perpendicular to the base that include both line segments and curves. The bottom is black and effectively collapses to a single point. Overall, it looks like a tent supported by three poles. All models, including gemini-3-pro-preview-11-2025, claim that it is a solid with six curved surfaces, while in reality three of those faces are planes.\nNews: The gemini-3-pro-preview officially appearing in AI Studio and GCP on 11.18 can fully answer this question correctly. Here is its response (translated from Chinese version by Gemini 3 Pro Preview):\nThis is a very classic geometric transformation problem in colour science. After transforming an RGB cube into the xyY colour space, the resulting shape is not a simple regular geometric body, but a **Triangular Prism with a Tent-like Top**. The following is a detailed analysis of the geometric structure and the derivation process: ### 1. Base Projection: Triangle (The Gamut Triangle) * **Origin**: In the RGB cube, any ray originating from the origin (black) — i.e., a colour with a constant R:G:B ratio — is projected as the same single point on the xy chromaticity diagram (2D plane). * **Boundaries**: The three faces connected to the origin in the RGB cube (the $R=0$, $G=0$, and $B=0$ planes) represent the mixture of two primaries (e.g., the $B=0$ plane is a mixture of red and green). In XYZ space, these are planes passing through the origin. * **Transformation Result**: In xyY space, planes passing through the XYZ origin convert into **straight lines** on the xy plane. Consequently, the three RGB primary points (Red, Green, Blue) constitute three vertices on the xy plane, and the straight lines connecting them form the outline of the base — this is what we commonly call the **Gamut Triangle**. ### 2. Sides: Vertical Walls * **Principle**: The $x$ and $y$ coordinates in the xyY system depend only on the ratios of RGB, not on the absolute intensity of light (Luminance, Y). This means that if you dim a colour (linearly scaling the RGB values), its xy coordinates remain unchanged; only the height along the Y-axis decreases. * **Geometric Representation**: Therefore, for any point on the boundary of the gamut triangle (fully saturated colours), from the darkest point ($Y=0$) to the maximum luminance of that colour, its $(x,y)$ coordinates remain constant. This forms a \u0026#34;wall\u0026#34; perpendicular to the base in three-dimensional space. * **Conclusion**: The sides of this shape are three vertical planes formed by extending the edges of the gamut triangle upwards. ### 3. Top: Tent-like Curved Surface (The Tent-like Curved Roof) * **Origin**: The other three faces of the RGB cube that do not pass through the origin (the $R=1$, $G=1$, and $B=1$ planes) represent the maximum intensity boundaries achievable by the display device. * **Geometric Representation**: * **Highest Point**: The three planes intersect at $(1,1,1)$, i.e., the **White Point**. This is the highest point of the entire shape along the Y-axis (the peak). * **Ridges**: The White Point connects to the three secondary colours (Cyan, Magenta, Yellow). Secondary colours are points where two components are 1 (e.g., Yellow is R=1, G=1). These connecting lines form \u0026#34;ridges\u0026#34; extending downwards from the peak in xyY space. * **Curvature**: Although surfaces like $R=1$ are planes in RGB and XYZ space, in xyY space, because the conversion between $Y$ and $x,y$ is non-linear (involving division by $1/(X+Y+Z)$), these planes become **hyperbolic curved surfaces**. * **Edge Height**: The edges of the top undulate along the top of the vertical walls. * The lowest points are usually located at the vertices of the three primaries (R, G, B). * Transitioning from a primary to a secondary colour (e.g., Red -\u0026gt; Yellow), the luminance Y increases (since $Y_{yellow} = Y_{red} + Y_{green}$), forming a sagging curve similar to a suspension bridge cable or the edge of a tent. ### Summary: What does it look like? If you plot this shape in a 3D coordinate system (with xy as the base and Y as the height), it looks like: 1. **Plan View (Top-down)**: A standard **triangle** (i.e., the common chromaticity diagram). 2. **Elevation View**: A **triangular prism**, but its top is not flat. 3. **Overall Form**: * It resembles a house built on a triangular foundation. * The **floor** is black ($Y=0$). * The **walls** stand vertically, representing colours of maximum saturation. * The **roof** is a collapsed, tent-like curved surface. The spire of the roof is white, the three corners (lowest points) of the roof are the Red, Green, and Blue primaries, and the \u0026#34;eaves\u0026#34; of the roof are higher at Yellow, Cyan, and Magenta, and lower at the primaries. **Visual Metaphor**: Imagine a triangular slice of cake, but the icing (top surface) is not smoothed flat. Instead, it is pulled up in the centre (White) and slides down from the centre towards the three corners (RGB), forming a lid composed of three concave curved surfaces. ","permalink":"https://jackchou.top/en/posts/llm-for-colour/","summary":"Sharing tips for collaborating with general-purpose LLMs in this field, and tests.","title":"Using LLMs for Colour Science: Best Practices and Test Questions"},{"content":"Two JPEGs Most modern mobile phones now support Gainmap photos. The ProXDR photos often mentioned by OPPO are, in fact, Gainmaps. Previously, it was a proprietary format, but they later adopted Google\u0026rsquo;s UltraHDR.\nIf you search for the JPEG start marker FF D8 and end marker FF D9 in binary mode, you\u0026rsquo;ll find that this type of image file contains more than one JPEG image. Typically, right after the first end marker, you\u0026rsquo;ll find the start marker for the next one. If you separate them directly from the byte stream and save them as two separate JPEG files, you can see the base image and the gainmap. Additionally, within the APPn marker segment of the gainmap, you can find the metadata needed to decode it in XMP format, such as Min, Max, Gamma, and Offset.\nInterestingly, the straight-out-of-camera JPEGs from Sigma\u0026rsquo;s latest camera, the Sigma BF, also use this format, making it the first camera to produce HDR JPEGs directly. It\u0026rsquo;s a pleasant surprise to see this format in the traditional camera industry.\nThis is a JPG photo taken with an OPPO Find X6 Pro that complies with the UltraHDR standard. The building at Huawei\u0026rsquo;s LianQiu Lake campus was illuminated by sunlight from one side, creating a large contrast between light and shadow. On the left is the first JPG segment, which is the primary image in UltraHDR—a standard SDR image. This is what you see on viewers that don\u0026rsquo;t support UltraHDR or on non-HDR displays. In the middle is the second JPG segment, the Gainmap. As it\u0026rsquo;s not designed to be viewed directly, it\u0026rsquo;s just rendered here in RGB as a way to \u0026lsquo;visualise\u0026rsquo; it.\n{ \u0026#34;Version\u0026#34;: 1.0, \u0026#34;GainMapMin\u0026#34;: 0, \u0026#34;GainMapMax\u0026#34;: 2.16048, \u0026#34;Gamma\u0026#34;: 1, \u0026#34;OffsetSDR\u0026#34;: 0, \u0026#34;OffsetHDR\u0026#34;: 0, \u0026#34;HDRCapacityMin\u0026#34;: 0, \u0026#34;HDRCapacityMax\u0026#34;: 2.16048, \u0026#34;BaseRenditionIsHDR\u0026#34;: false, \u0026#34;OplusScale\u0026#34;: 4.47065 } This is the metadata extracted from the APP1 segment of the second JPG. The pixel values in the gainmap need to be calculated into a \u0026lsquo;multiplier\u0026rsquo; based on this metadata to produce the HDR effect on the right. OplusScale is a proprietary field from OPPO; its exact purpose is unclear. The rest are standard elements of UltraHDR.\nOn the right is the composited HDR result. For demonstration purposes, this image is actually a pure HDR AVIF image, decoded and then re-encoded using PQ; it is not an UltraHDR file. If the peak luminance of the display allows, the visual effect is identical; otherwise, it may look different. Below is the original version in UltraHDR format. The file size is relatively large, so it might load a bit slowly.\nRecording the Difference A Gainmap records the difference between SDR and HDR. This can be understood from many perspectives.\nImagine you have an image, and on an HDR device, you want certain areas of the picture to be brighter than the standard SDR white point, creating a highlight effect. Without altering the SDR display effect, you can use another image to mark the areas that need to be brightened and by how much, and then encode the SDR image and the difference map together.\nAlternatively, say you have an HDR image, but its format has poor compatibility, and relying on devices to tone map it down to SDR also yields unsatisfactory results. You could first tone map it down to a pleasing SDR image, encode it into a traditional, mature format, and then use another image to record the difference between the original HDR and the SDR version, and encode them together.\nIn short, a Gainmap itself only records the difference. You could, for instance, take an image with actual content, treat it as a gainmap, and package it with a solid-colour base image. This would create something where the picture is only visible on a device that can successfully decode it for HDR display.\nThe two examples above both use an SDR base image. In reality, the base image can also be in HDR, with the Gainmap recording how to tone map it down to SDR. However, in most cases, an SDR base image is used because even if the decoder doesn\u0026rsquo;t support it, the file can still be displayed as a regular SDR image, offering the best compatibility.\nTherefore, a Gainmap is simply an encoding method to be used after you have both an HDR and an SDR image. It shouldn\u0026rsquo;t be seen as a method for adjusting the image itself. For example, performing some operation on the Gainmap to make the image brighter, or generating a Gainmap with some algorithm without an existing HDR image to up-convert an SDR image to HDR.\nCalculating the Gainmap Regarding how to generate a Gainmap, I believe readers should refer to the documentation for specific platforms, especially for a new technology like Gainmaps with many competing standards. This article focuses more on introducing the concept.\nA few recommended documents and web pages:\nGoogle\u0026rsquo;s UltraHDR. This is currently the most widely used dual-layer HDR format. Version 1.1 is also compatible with the ISO 21496-1 international standard. If you\u0026rsquo;re an Android developer, it\u0026rsquo;s a must-read. Adobe\u0026rsquo;s gainmap. Adobe is also a major proponent of this format. The demo app they released is the most convenient way to view various HDR images. That page also has a detailed spec document containing some very valuable notes. Before generating a Gainmap, you need to prepare your SDR and HDR images in a linear light space. For instance, you might define the SDR image\u0026rsquo;s pixel value range from 0 to 1, with the HDR image aligned to the same scale but allowing for negative numbers or values greater than 1. This is because both SDR and HDR need to share the same colour primaries (e.g., P3). If a smaller set of primaries is chosen (like sRGB), or if the HDR image has a very wide colour gamut, negative values can occur. Don\u0026rsquo;t worry, Gainmaps can record negative values. Values over 1 are a characteristic of HDR itself.\n$$\rG = \\log_2 \\frac{\\text{HDR}+k_{\\text{hdr}}}{\\text{SDR}+k_{\\text{sdr}}}\r$$Here, the two k values are offsets used to shift potential negative values into the positive range, ensuring the logarithm operation is valid.\nNext, find the maximum and minimum values of G, which are used to map G into the 0-1 range. For better reliability, you can choose to exclude a small percentage of the lowest and highest values before selecting the min and max, and then clip any values outside the 0-1 range.\nThis gives you an image in the 0-1 range. You can apply a gamma curve to it to improve quantisation efficiency and choose a suitable bit depth (usually 8-bit) and encoding method to encode it.\nAt the same time, you must also encode the offsets, maximum and minimum values, gamma, and other metadata together according to the specification.\nSingle-Channel vs. Three-Channel To save space, a Gainmap can be greyscale, which is what most Android phones currently use. The \u0026ldquo;True Colour ProXDR\u0026rdquo; photos on the OPPO X8 Ultra, for instance, refer to an upgrade to three channels.\nThe main difference between a greyscale Gainmap and a three-channel colour Gainmap lies in the correlation or degree of freedom between the SDR and HDR versions.\nA three-channel Gainmap offers almost complete freedom. The HDR and SDR versions can be unrelated, as the Gainmap can establish a link between any two arbitrary points. A single-channel Gainmap, however, requires the RGB pixel values in the SDR and HDR images to have the same ratio, or in other words, to lie on the same line from the origin in a spatial coordinate system.\nWithout constraints on the SDR image, both types of gainmaps can achieve the same HDR colour gamut volume. However, the three-channel version allows for more flexible tone mapping methods, whereas a single-channel gainmap can only perform tone mapping in terms of luminance. When keeping the SDR image the same, a three-channel Gainmap can control the HDR image with more finesse.\nWhat\u0026rsquo;s the impact? According to the Bezold–Brücke and Hunt effects, as brightness changes, the hue and saturation of colours also change. A single-channel gainmap can only control luminance. If you preserve the HDR appearance, the tone mapping down to SDR is restricted. If you preserve the SDR appearance, the HDR content you can store is limited, as their hues (referring to the RGB space, not perceptual hue) cannot differ.\nFurthermore, in terms of quantisation precision, a three-channel Gainmap also has certain advantages. By recording the individual maximum and minimum values for each of the three channels in the metadata, it allows control over the compression level and gamma for each channel during normalisation.\nCompressing the Gainmap When compressing a Gainmap, especially with downscaling operations (most phones downscale to 50% of the original width and height), misalignment with the base image can occur, causing the composite HDR image to exhibit false colour artefacts and banding. Similarly, compressing the base image can also lead to misalignment with the Gainmap, particularly in high-frequency areas, which becomes more apparent in typical HDR scenes like point light sources.\nThe methods for compressing a gainmap are largely the same as for a regular image, such as JPG, JPEG XL, and HEIC. However, gainmaps are not quite the same as regular images, so some have proposed specialised methods. The simplest involves applying an additional gamma function to the gainmap. There are also methods that use machine learning, like this paper that uses an MLP to compress gainmaps.\nT. D. Canham, S. Tedla, M. J. Murdoch, and M. S. Brown, “Gain-MLP: Improving HDR gainmap Encoding via a Lightweight MLP,” Mar. 14, 2025, arXiv:2503.11883.\nOther Packaging Methods JPEG XL is the latest JPEG format, which boasts a very high compression ratio and supports many new features, including dual-layer formats.\nHEIF and AVIF also support dual-layer formats, storing the gainmap as an auxiliary image in the metadata. For example, HDR photos taken with an iPhone are saved in this manner. During decoding, you first need to find the corresponding tag to extract the Gainmap. Photos taken on OPPO phones with \u0026lsquo;High Efficiency\u0026rsquo; storage enabled are also stored as HEIF, but the tags are different.\nTIFF can also save multiple frames together, so it can also store gainmaps.\nWhen exporting an HDR image from Adobe Camera Raw or Lightroom, if you select \u0026lsquo;Maximum Compatibility\u0026rsquo;, the exported file will be in one of the formats mentioned above.\nSummary and Future The Gainmap has been widely adopted. It\u0026rsquo;s forward-compatible, widely supported by various new container formats, and conveniently stores the result of tone mapping.\nFrom an HDR perspective, however, it\u0026rsquo;s based on relative luminance, just like SDR, but it\u0026rsquo;s also constrained by the peak brightness of the HDR display. When a user adjusts the screen brightness, the displayed result may change significantly. If a display\u0026rsquo;s HDR capability is less than the maximum specified in the gainmap, the practical implementation of tone mapping for these intermediate cases is somewhat ambiguous.\nAdditionally, storing a gainmap requires two 8-bit images, which might be less compression-efficient than a single 10 or 12-bit pure HDR image. Most cloud services that process JPEGs (like image compression on CDNs or moderation on social media) might also miss the gainmap because it\u0026rsquo;s located after the end of the first JPG stream.\nIn the next post, I\u0026rsquo;ll write about the characteristics and usage of common Gainmap formats, and I\u0026rsquo;ll also organise and provide some Python code.\n","permalink":"https://jackchou.top/en/posts/gainmap-image-intro/","summary":"Achieving HDR while ensuring excellent backward compatibility by packaging SDR and HDR images within a single file.","title":"Decoding HDR Image Formats (I): Basic Concepts of Gainmap"},{"content":"Introduction to CAT16 When two colours appear the same under two different illuminants, they are called corresponding colours. A chromatic adaptation transform (CAT) can predict the tristimulus values of a corresponding colour under one illuminant given the other illuminant and the tristimulus values of a colour under it. CAT16 is the latest chromatic adaptation transform model.\nIts basic structure is similar to the von Kries hypothesis: first convert the tristimulus values to a special cone response space, then apply independent gains to each channel in that space, and finally convert back to tristimulus values. These three steps can all be expressed by matrices.\nThe matrix that converts from tristimulus values to cone space is called $\\mathrm{M}_{16}$, and its inverse $\\mathrm{M}_{16}^{-1}$ converts from cone space back to tristimulus values. The independent channel gains are described by a diagonal matrix $\\Lambda$, that is:\n$$ \\Phi_{r,t} = \\mathrm{M}_{16}^{-1} \\cdot \\Lambda_{r,t} \\cdot \\mathrm{M}_{16} $$In use, we call the two illuminants Test and Reference. The subscripts in the equation indicate a transform from Test to Reference (r,t means from t to r). When a known colour sample is on the Test side, this is the forward transform, denoted by $\\Phi$.\nIf the known colour is on the Reference side, it is the inverse transform, denoted by $\\Psi$. For example, to predict the corresponding colour of a known Reference-side colour, use the following matrix.\n$$ \\Psi_{t,r} = \\Phi_{r,t}^{-1} = \\mathrm{M}_{16}^{-1} \\cdot \\Lambda_{r,t}^{-1} \\cdot \\mathrm{M}_{16} $$Since many symbols will follow, for clarity please first understand: “In CAT16, forward and inverse transforms are not the same as swapping illuminants”:\n$$ \\Psi_{t,r} = \\Phi_{r,t}^{-1} \\ne \\Phi_{t,r} $$In this article, we ignore luminance adaptation in the model and set $Y_w=Y_{wr}$.\nWhy one-step CAT16 is not reversible Reversible means: after a forward transform from A to B, followed by another forward transform from B back to A, do we get the original values? Or, is a forward transform A to B followed by B to C equivalent to a direct forward transform A to C?\nThe one-step method does not satisfy reversibility or transitivity. The culprit is the degree of adaptation D, a factor between 0 and 1 that describes adaptation level. It can be directly specified or computed from field luminance, etc. Let’s look closely at the structure of the diagonal gain.\n$$ \\text{Gain} = D \\cdot \\frac{\\text{RGB}_{\\textit{Ref}}}{\\text{RGB}_{\\textit{Test}}} + (1 - D) $$This diagonal matrix is actually a linear combination of the identity matrix and a diagonal matrix related to the white point transform, weighted by the adaptation degree D.\nConsider a forward transform from A to B, then a forward transform from B back to A. Let $\\text{RGB}_{B}/\\text{RGB}_{A} = x$.\n$$ (Dx+(1-D))\\cdot(D\\frac{1}{x}+(1-D)) $$Multiplying these two linear combinations produces quadratic terms, so it only equals the direct transform when D is 0 or 1.\nWhy two-step CAT16 is reversible Because it consists of one forward step followed by one inverse step, with an equal-energy white as the intermediate illuminant.\n$$ \\Pi_{r,t} = \\Psi_{r,e} \\cdot \\Phi_{e,t} = \\mathrm{M}_{16}^{-1} \\cdot \\Lambda_{e,r}^{-1} \\cdot \\Lambda_{e,t} \\cdot \\mathrm{M}_{16} $$When you need transitivity or reversibility, what actually happens is “forward then inverse, then forward then inverse,” and the two middle steps are inverses of each other.\nFor example, for A to B to C, what happens is: A forward to E, E inverse to B, B forward to E, E inverse to C. The two middle steps cancel out as inverses, which is equivalent to the two-step A to C.\n$$ \\Pi_{C,A} = \\mathrm{M}_{16}^{-1} \\cdot \\Lambda_{E,C}^{-1} \\cdot \\Lambda_{E,B} \\cdot \\Lambda_{E,B}^{-1} \\cdot \\Lambda_{E,A} \\cdot \\mathrm{M}_{16} $$Therefore, two forward steps or two inverse steps still do not yield a reversible two-step method (even with equal-energy white). You must have one forward and one inverse so the middle parts cancel during chaining.\nNote that the intermediate illuminant can be arbitrary mathematically; it does not have to be equal-energy white.\nWhy this matters The one-step CAT16 directly links the test and reference fields, but due to the adaptation degree it is neither transitive nor symmetric, so under incomplete adaptation it is “incorrect.” The two-step version introduces an intermediate illuminant, making it mathematically transitive and reversible.\nSymmetry and transitivity are often considered desirable properties for chromatic adaptation models, though some research argues otherwise.\nThe one-step and two-step methods perform very similarly on various visual datasets. In practice, it is recommended to use the two-step CAT16 adaptation. This also matches the behavior of CIECAM16: its first step adapts the colour stimulus to an equal-energy white field, and using forward plus inverse CIECAM16 is equivalent to the two-step CAT16.\nIn practice, some applications that use CATs need transitivity and symmetry to avoid error accumulation, such as colour management systems and display pipelines.\nSome issues in the paper Li, C.; Li, Z.; Wang, Z.; Xu, Y.; Luo, M. R.; Cui, G.; Melgosa, M.; Brill, M. H.; Pointer, M. Comprehensive colour Solutions: CAM16, CAT16, and CAM16-UCS. colour Research \u0026amp; Application 2017, 42 (6), 703–718. https://doi.org/10.1002/col.22131.\nIn Equation (5), Y_w should be RGB_wr, otherwise it contradicts the part later stating Y_w=Y_wr=100.\nEquation (24) and Equation (23) are not equivalent. The two-step CAT16 in Equation (23) is reversible; the one in (24) is not.\nA supplementary paper discussing CAT16 mentions this issue: among the four permutations of forward/inverse in the two-step method, only two are transitive and reversible. One of those was adopted—forward first, then inverse—which is the Equation (23) above.\nLi, C.; Xu, Y.; Wang, Z.; Luo, M. R.; Cui, G.; Melgosa, M.; Brill, M. H.; Pointer, M. Comparing Two‐step and One‐step Chromatic Adaptation Transforms Using the CAT16 Model. colour Research \u0026amp; Application 2018, 43 (5), 633–642. https://doi.org/10.1002/col.22226.\nHere comes the code I previously said I would open-source some code examples I used, but as I kept writing, it turned into a draft notebook. Now I’ve opened a new repository specifically for storing the code examples used in the blog. This is the first post with attached code.\nThere are two files:\ncat16_comparison.py: compares the reversibility of one-step and two-step methods. cat16_paper_implementation.py: the two-step implementation and test case from the paper’s appendix (done by Gemini) Open-source address: JacksBlog Examples: 01\n","permalink":"https://jackchou.top/en/posts/cat16-reversibility/","summary":"Details in colour adaptation models similar to CAT16: one forward and one reverse, achieving reversibility and transference","title":"Understanding CAT16: Why Are Two Steps Needed to Ensure Reversibility?"},{"content":" I\u0026rsquo;m exploring the use of HDR images. In some pictures, you might observe:\nHDR images with up to 4x the brightness of SDR. On SDR devices, unknown tone mapping and unpredictable display effects. It\u0026rsquo;s hot, so let\u0026rsquo;s start with a glass of iced coffee. By placing the ice cube tray in an insulated bag before putting it in the freezer, you can get clearer and harder ice. Thanks to my senior, Leo, for the coffee beans.\nClouds, Sunset Glow This July, Hangzhou had a sunset glow almost every day. If the forecast for fiery clouds and sunset glow showed a high index, the day seemed to hold a little more anticipation.\nGiant, white cumulonimbus clouds, their strong reflection making them exceptionally bright.\nA rainbow that lasted the entire evening, cutting through fast-moving clouds.\nAnother Saturday with a very high index, featuring a pure, golden sunset glow.\nAn evening with great air quality, an elegant blue hour.\nLight and Cats at Yuquan OPPO\u0026rsquo;s new feature: soft light simulation. Sometimes it gives the feeling that the lens hasn\u0026rsquo;t been wiped clean.\nStill the soft light filter.\nNaughty cat, shamelessly chilling at Teaching Building No. 3, loves to randomly lie on pedestrians\u0026rsquo; shoes.\nA Not-So-Boring Weekend Usually, my weekends are for sleeping two full days. Recently, I\u0026rsquo;ve switched to sleeping two half-days, then going out to play with a cat that\u0026rsquo;s as big as a truck and take a boat ride.\nPlastic Figurines When it rains, staying home is a good opportunity to photograph some plastic figurines. A slightly counter-intuitive fact is: the closer the light source is to the object, the larger its relative area, and the softer the light. If your light isn\u0026rsquo;t very big, you can try shooting figurines. Be bold and get close; you\u0026rsquo;ll be pleasantly surprised.\nA knock-off figure of Ijichi Nijika with a fantastic face sculpt.\nYou can even get so close that the light source appears in the frame. It\u0026rsquo;s very easy to remove against a pure black background.\n","permalink":"https://jackchou.top/en/photos/monthly-2025-07/","summary":"(25 Images) Sunsets, Fiery Clouds, Rainbows, and Cats","title":"📷 July Collection"},{"content":"How I update Hugo is a tool that converts Markdown files into static HTML pages. In theory, you just create or edit Markdown, then run hugo and it’s done. In practice, there are some tiny details worth sharing.\nGithub Actions For convenience, my blog uses two repos: one stores Markdown and the Hugo theme and other files; the other stores the generated site files (i.e., Github Pages). The first repo has Github Actions that run the hugo command, build static pages, then push to the second repo.\nThe Action is triggered whenever commits are pushed to the main branch. Each run takes about a minute. With 3,000 Action minutes per month on Github, I’ll never run out.\nDraft Sometimes a post is half-written or I don’t want to publish it yet. In that case I set draft to true in the front matter, so hugo won’t build it. But if it’s in main, it still triggers the Action—even though nothing changes. So I made a draft branch to keep those posts. That way I can use Git like cloud sync and also avoid triggering the Action.\nLLM post-processing After I finish the draft, I’ll have AI read and check it—like spotting typos, checking fluency, and giving suggestions (which I mostly skim, or let models like GPT-4o say some nice words). If I haven’t written the summary, description, or even the title yet, I’ll ask it for options—though most of the time they’re not that useful.\nI use GPT 4.1 for this step. Stronger models are fine too, but LLMs aren\u0026rsquo;t great at precisely controlling the word count of the generated text.\n输入将会是完整的 markdown，是由简体中文创作的个人博客的一篇文章。 你需要首先查看是否存在错别字、错误的用词和用语，表述不清晰，不通顺的地方。清晰的指出在何处、有什么问题、如何修改。注意，这是一个个人博客，适当的口语化是可接受的，但不能冒犯。 然后，你需要结合你的知识和理解，仔细的阅读文章内容，给出一些内容上的建议和读后感。 最后，你需要结合内容和你的理解，给出几个 front 部分的 title，summary 和 description 的备选项，符合以下要求： - title：这将是显示在文章列表中和文章顶部的“标题”，最好不超过 25 个汉字。 - summary：这将是显示在文章列表中，标题下方的文章“总结”或“副标题”，既补充说明标题，又代表文章内容，最好不超过 50 个汉字。 - description：这将显示在文章页面的标题下方第一段，是“摘要”，是开头和预告全文。最好不超过 80 个汉字。 以及，给出一个文件名，它将作为该文章的 URL，比如 xxx.md，将会成为 baseURL/posts/xxx After the Chinese version is done, for a bit of internationalization, I also generate an English version. In Hugo, you just create a file with the same name xxx.en.md. The English draft is also written by an LLM. If the post is about color science, I’ll provide a glossary to help the AI with terms. Aside from a few similar terms (which we often don’t strictly distinguish anyway), larger models translate quite well. Good options include Gemini 2.5 Pro and DeepSeek R1-0528; the latter sometimes gives better translation quality, but it may alter format and paragraphs. After translating, I’ll read it once and make small edits if I have time.\nRecentlt, A model named Horizon on OpenRouter also works very well for this task. Update: Horizon appears to be gpt-5-mini, while gpt-5 performs relatively well on both tasks.\nHere’s the prompt I use. Supposedly, for reasoning models, a clear prompt is enough, you don’t need a super complicated one:\nI found that OpenAI has a Prompt Optimizer. It’s kind of like ByteDance’s old Prompt Pilot. Here is a prompt tuned for GPT-5 but works fine with other models too.\n输入内容为完整的 Markdown，内容为简体中文创作的个人博客文章。 你的任务是将内容翻译为英文，生成对应的英文版本。 要求： 1. 保持原有格式和段落完全不变。 2. 译文表达需自然、贴近日常习惯，避免生僻或不常用表达。 3. 保持 Markdown 语法，图片链接和代码部分只需翻译图片描述和注释。 4. 保持原文语气和风格，网络用语和俚语需准确对应。 在开始翻译前，生成一份 3-5 步的简要任务清单。 翻译完成后仅输出英文内容，便于直接复制粘贴，无需包含其他信息。 遇到专业术语时，以下是供你参考的词典： - 亮度：Luminance - 明度：Lightness - 视明度：Brightness Sometimes you need to update a post after it\u0026rsquo;s published. After adding or editing the Chinese version, how do you update it while keeping everything else unchanged, and still make the updates blend in naturally? A one-shot chat with an LLM usually won’t cut it. The best way is to lean on an agent, such as GitHub Copilot or Codex, with a tiny bit of guidance.\n使用 `git diff` 检查我对文件 `ccontent\\posts\\badge-and-blog-workflow.md` 做的更改，依据以下规则，将修改同步到英文版的文章中 `content\\posts\\badge-and-blog-workflow.en.md`，只修改有改动的部分，保持其他部分不变，要求风格与原本接近。 翻译规则： 1. 保持格式和段落严格不变。 2. 注意用语和用词，不要出现生僻的单词，符合日常习惯。 3. 保持 markdown 语法，图片链接、代码部分只要翻译图片描述和注释。 4. 保持语气，使用的俚语和网络用语需要准确的翻译。 It will read the files, run commands to view the diffs, and accurately translate only the changed parts.\nA Badge for visitor stats Source Found this little gadget while surfing: show visitor counts through a Badge, like on this site. Did some digging and found it’s just an image; each load increments a counter by 1.\nThe image comes from visitor-badge.laobi.icu, a free service. You only need an image URL.\nThe image link format is below. Replace page.id with a unique string. I’m using jacksblog now—just avoid collisions. Super convenient. To reset the count, just swap in a new string.\nhttps://visitor-badge.laobi.icu/badge?page_id=page.id\nOf course, this is just a fun toy. Every image load increments the counter. If you add it to the footer like I did, it shows on every page. This way, from visiting the homepage, to entering the article list, to opening a specific article, the counter might increase three times. Additionally, refreshing the page also adds to the count.\nAdd to the page Next is adding the Badge to the page. I chose to place it below the footer text. Claude Code handled this part; here’s its own summary of the work.\nStep 1: Config separation Separate the visitor badge from the footer text and configure it in hugo.yaml:\nLocation: hugo.yaml\nfooter: text: \u0026#34;footer text here.\u0026#34; visitorsImage: \u0026#34;https://visitor-badge.laobi.icu/badge?page_id=\u0026#34; Add a new visitorsImage param to store the badge URL Keep the footer text clean Step 2: Theme extension Modify the extend_footer.html of the PaperMod theme to display the badge:\nLocation: themes/PaperMod/layouts/partials/extend_footer.html\n{{- if .Site.Params.visitorsImage -}} \u0026lt;div style=\u0026#34;text-align: center; margin-top: 10px; margin-bottom: 20px; width: 100%; display: flex; justify-content: center;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ .Site.Params.visitorsImage }}\u0026#34; alt=\u0026#34;visitors\u0026#34; style=\u0026#34;max-width: 120px; height: auto;\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; {{- end -}} {{- if .Site.Params.visitorsImage -}}: Conditional display; only show when visitorsImage is set {{ .Site.Params.visitorsImage }}: Read the param from hugo.yaml {{- -}}: Trim extra whitespace text-align: center: Center align text width: 100%: Container takes full width display: flex; justify-content: center: Flexbox to center horizontally margin-top: 10px; margin-bottom: 20px: Space around the footer, leave some bottom padding max-width: 120px: Limit badge max width to avoid oversized display height: auto: Keep aspect ratio Notes Theme updates: If you update PaperMod, you’ll need to reapply the extend_footer.html changes Third-party dependency: Relies on an external stats service; if it’s down, you’ll see a broken image Privacy: Visitor stats collect access data; make sure it aligns with your privacy policy ","permalink":"https://jackchou.top/en/posts/badge-and-blog-workflow/","summary":"Using a badge to roughly show visitor count, and my general process for updating blog posts","title":"Add a Little Gadget, and Talk About How I Update My Articles"},{"content":"The method described in this article may only be effective for Sony cameras.\nSending Signals to the Screen Display measurement, or characterization, aims to establish a relationship between two spaces, for example, the pixel values in an image and the tristimulus values of the displayed light. For a camera, the difficulty lies in how to input a specific pixel value to be displayed on the camera\u0026rsquo;s screen, because the camera\u0026rsquo;s screen cannot directly accept a display signal, and we can\u0026rsquo;t easily determine if the internal system has colour management.\nMy first thought was to write an image to the memory card to see if the camera could be used as a kind of \u0026ldquo;image viewer\u0026rdquo;. In my test, if a Sony camera\u0026rsquo;s card contains an image not taken by the camera, it will report a \u0026ldquo;Cannot read memory card\u0026rdquo; error.\nFollowing this, could we find a way to modify the contents of a camera-generated JPG file to make the camera think the picture was taken by itself, and thus display it? In practice, the method isn\u0026rsquo;t to modify the image data of the camera\u0026rsquo;s JPG file, but rather to extract the metadata from it, generate a solid colour image, and embed the metadata into it.\nThere are many ways to extract metadata. A well-known one is Pillow, which can extract ICC (which camera JPEGs don\u0026rsquo;t have) and EXIF, but that\u0026rsquo;s not enough. Sony\u0026rsquo;s JPEGs also have an XMP section that records two UUIDs. If you don\u0026rsquo;t copy this over, you\u0026rsquo;ll also get an error. The tool that can handle this part of the APP segment is Exiftool.\nUse the following command to transfer the metadata, where pure_colour_image is the temporary image providing the content, which can be created by Pillow or similar tools. The thumbnail can be cleared or kept.\nexiftool -tagsFromFile input_path -all:all -o output_path pure_colour_image After writing a simple tool to transfer metadata to new solid colour images, I could display them on the camera\u0026rsquo;s screen.\nA Simple Measurement Practice Next, I used the same method to process 30 images for a simple hands-on test. The camera was set to the default manual brightness (a7c2\u0026rsquo;s normal/sunny mode and zve10mk2\u0026rsquo;s sunny mode show similar performance in colour).\nThe selected test patches included:\nPrimary colours: Two on each R/G/B axis (120, 255), for a total of 6. Mixed primary colours: Yellow, cyan, magenta, 3 in total. Grayscale: 0–255 in steps of 15, for a total of 18. Reference colours: Light skin tone (241,149,108); a bluish-green (140,253,153); a purplish-blue (7,47,122), for a total of 3. This was to evaluate gamut coverage, grayscale stability and transfer function, and colour accuracy.\nBrightness and Contrast The default manual luminance was 267.86 nits, with the black level at 0.15 nits, giving a contrast ratio of 1823:1.\nAfter switching to \u0026lsquo;Sunny Weather\u0026rsquo; mode, the luminance was 951.19 nits, with a black level of 0.54 nits, and a contrast ratio of 1771:1.\nGamut Coverage and Volume Using the chromaticity coordinates of the three primary colours, we can calculate the gamut coverage and volume compared to sRGB:\nConverting to CIE 1976 UCS (i.e., u\u0026rsquo;v\u0026rsquo;) and calculating the ratio of the triangular areas, the gamut coverage compared to sRGB is 95.90%, and the gamut volume is 101.14%.\nGrayscale and Transfer Function This shows the luminance values of the grayscale steps. After normalizing to the maximum luminance and comparing with the standard sRGB transfer function, this screen\u0026rsquo;s transfer function does not fit a Gamma curve well. If forced to fit, the gamma exponent would be close to 2.6.\nColour Accuracy Since the images we input were modified from the original sRGB JPGs, their theoretical values should correspond to the tristimulus values of the sRGB space. After normalizing by the absolute luminance of the displayed white, we calculated the colour difference for all 30 colours compared to the standard sRGB.\nCalculating the colour difference using CIEDE 2000, the average deltaE is 3.12, and the maximum deltaE is 7.34 (for magenta).\nAbout Adobe RGB Most cameras allow setting the \u0026lsquo;Colour Space\u0026rsquo; in-camera, usually offering sRGB and Adobe RGB options. This option only affects JPG files. Generally, an ICC profile is used to identify the colour space, but Sony cameras use a different method, relying on EXIF tags.\nInteroperability Index: R03 - DCF option file (Adobe RGB) Interoperability Version: 0100 Adobe\u0026rsquo;s Gain Map Demo can recognize this tag and identify the image as being in the Adobe RGB space.\nHowever, Exiftool\u0026rsquo;s default behavior does not transfer these two tags; you need to add the -unsafe parameter.\nConclusion This Sony in-camera screen, while not excellent, isn\u0026rsquo;t terrible either. Its only drawback is perhaps the overly high gamma value of the transfer function (if you can call it \u0026lsquo;gamma\u0026rsquo;). The peak luminance of nearly 1000 nits isn\u0026rsquo;t low either. The poor viewing experience is more attributable to the small size, low resolution, and poor viewing angles.\nAs a preview screen, making the gamut larger and showing a more vibrant, higher-contrast image, combined with a larger display area, high resolution, and good viewing angles, can significantly improve the actual shooting experience. In fact, the screen on the Hasselblad X2D is optimized to some extent in this way, and it\u0026rsquo;s one of the best-looking camera screens I\u0026rsquo;ve ever seen (where \u0026lsquo;best\u0026rsquo; here means it looks good rather than accurate).\nI don\u0026rsquo;t think anyone would seriously judge a photo\u0026rsquo;s quality on in-camera screen. I\u0026rsquo;d prefer a camera that looks better, giving the user a more pleasing image during shooting. As for the large difference between the internal screen and computer displays, this can be addressed in the companion software by providing a preset that adjusts the image to match the look of the in-camera screen. This is not difficult to achieve in colour management systems similar to ICC.\n","permalink":"https://jackchou.top/en/posts/sony-camera-screen/","summary":"A small, practical method for testing the internal screen of Sony cameras and a hands-on example.","title":"Beyond Visual Appraisal: Objective Test Procedures for Sony Camera Displays"},{"content":"Secret Garden In October 2023, to use Cloudflare\u0026rsquo;s Tunnel feature, I purchased the jackchou00.icu domain for one year, also serving as a custom domain for GitHub. Later, when I got the motivation to blog and wanted to extend it for a few more years, I found AliCloud couldn\u0026rsquo;t extend it for unknown reasons. Thus I switched to jackchou.top bought elsewhere with cheaper renewal, extending it until 2035.\nFor a static site hosted on GitHub, changing domains is remarkably simple - just purchasing a domain and updating DNS records. Yet disappearing from the internet is equally easy. With minimal SEO and rankings, this obscure personal blog about niche tech topics could vanish if domains change or expire.\nLooking through bookmarks of senior bloggers\u0026rsquo; sites, most haven\u0026rsquo;t updated in years or became inaccessible. Maintaining a personal blog relies purely on willpower - abandonment comes naturally. When I start working, switch fields, or simply lose motivation, this place may fall silent.\nI don\u0026rsquo;t like the feeling of \u0026ldquo;managing\u0026rdquo; social media; I find myself unconsciously focusing on data and traffic. I also dislike clickbait titles and content. Writing this blog, on the other hand, gives me a thrill of documenting and sharing. Here, data doesn\u0026rsquo;t matter because I don\u0026rsquo;t track it at all. I just assume and hope that visitors can get something out of their visit. Here, I only need to be responsible to myself. It feels like tending to a small garden that is all my own, yet open to the entire internet. This subtle feeling of being in a quiet corner in a very public space is quite interesting.\nBut at least until 2035, you can visit this place at jackchou.top. After that, you can also use jackchou00.github.io. I have full faith in the availability of Cloudflare and Github, you could say this place will be around for as long as the internet itself.\nDomain Migration Details I initially thought changing the domain would be a hassle, but looking back, the workload was basically just changing the DNS and CNAME records. The bigger concern was how to let people who already knew about this place know how to find it again, I don\u0026rsquo;t really care about SEO and all that.\nFor now, I\u0026rsquo;ve redirected all my other domains here. You can reach this site via jackchou00.icu, zms.im, and jackchou.xyz. However, only jackchou.top is registered for the long term.\nIf you visited here between July and October 2025, you might have noticed a banner. This was the work of Claude Code. If you\u0026rsquo;re interested, you can read the log he wrote himself below.\nBackground The blog\u0026rsquo;s domain is migrating from jackchou00.icu to jackchou.top. A prominent banner is needed to notify visitors of the domain change, with consideration for easy removal after three months.\nImplementation Plan Chosen Method Modify the extend_head.html file in the PaperMod theme. This is a custom extension point provided by the theme, which is loaded on every page.\nFile Location themes/PaperMod/layouts/partials/extend_head.html:15-157\nCore Features Fixed Top Banner\nUses position: fixed to stay at the top of the page Gradient background for visual appeal High z-index to ensure it displays above all other content Bilingual Support\nAutomatically detects the page language (via document.documentElement.lang and URL path) Chinese: 👋 再见，jackchou00.icu 🚀 新的旅途，从 jackchou.top 起程 English: 👋 Goodbye, jackchou00.icu 🚀 A new chapter will begin at jackchou.top User Interaction\nClickable ✕ button to close Closed state is saved to localStorage, so it won\u0026rsquo;t reappear on page refresh Automatically adds padding-top to the body when the notice is shown to prevent content from being obscured Automatic Expiration\nHardcoded expiration date: 2025-10-31 (after three months) Automatically stops displaying after the expiration date, no manual action required Responsive Design\nDifferent layouts and spacing for desktop and mobile Text and button are stacked vertically on mobile Technical Details CSS Styles Gradient background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) Link color: Yellow #ffeb3b for better readability Shadow effect: box-shadow: 0 2px 10px rgba(0,0,0,0.1) JavaScript Logic Wrapped in an IIFE (Immediately Invoked Function Expression) to avoid polluting the global scope Uses the DOMContentLoaded event to ensure execution after the page is fully loaded Uses insertAdjacentHTML to dynamically insert HTML Removal Method There are two ways to remove it:\nMethod 1: Automatic Expiration (Recommended) No action required It will automatically stop displaying after October 24, 2025 Method 2: Manual Removal Delete lines 15-157 from the themes/PaperMod/layouts/partials/extend_head.html file:\n\u0026lt;!-- Domain Migration Notice --\u0026gt; \u0026lt;style\u0026gt; ... \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; ... \u0026lt;/script\u0026gt; That is, delete all code from \u0026lt;!-- Domain Migration Notice --\u0026gt; to the final \u0026lt;/script\u0026gt; tag.\nNotes Theme Updates: If you update the PaperMod theme, you will need to re-add this code. Caching Issues: Users\u0026rsquo; browsers may have the old version cached, but new visitors will see the notice immediately. SEO Impact: The banner will not affect SEO because it is added dynamically via JavaScript. ","permalink":"https://jackchou.top/en/posts/domain-migration/","summary":"Like mayflies in the universe, insignificant as a grain in vast sea","title":"Changing Domains and Random Thoughts About Personal Blog"},{"content":"To be honest, I do this so often myself that I had to write something as a reminder (and for my own future reference).\nWhen you accidentally commit a large file, a binary file, or a file containing sensitive information (like passwords or keys) to a Git repository, simply deleting it in a new commit is not enough. It still exists in the Git history, continuing to take up repository space or posing a security risk. This guide will explain how to completely erase a file from your Git history.\nModifying Git history is a destructive operation. It will change the hash values of many commits in the repository. Before performing this on a shared repository, be sure to communicate with all collaborators and make a backup clone first.\nAnalyse the Repository Before deleting, you need to locate the unwanted files, especially those that take up a lot of space.\nYou can also analyse space usage with pure Git, but it requires a very long command line. Here is a version for PowerShell.\ngit rev-list --objects --all | git cat-file --batch-check=\u0026#39;%(objecttype) %(objectname) %(objectsize) %(rest)\u0026#39; | Where-Object { $_ -match \u0026#39;^blob \u0026#39; } | ForEach-Object { $_ -replace \u0026#39;^blob \u0026#39; } | Sort-Object { [int]($_.Split(\u0026#39; \u0026#39;)[1]) } | Select-Object -Last 10 macOS or Git Bash\ngit rev-list --objects --all | git cat-file --batch-check=\u0026#39;%(objecttype) %(objectname) %(objectsize) %(rest)\u0026#39; | sed -n \u0026#39;s/^blob //p\u0026#39; | sort --numeric-sort --key=2 | tail -n 10 After a standard Git installation, you usually have LFS. Using the git lfs command, you can intuitively list space usage by file type.\ngit lfs migrate info --everything git-sizer is a tool officially released by GitHub for analysing the \u0026ldquo;health\u0026rdquo; of a Git repository. It provides a more comprehensive report but requires separate installation.\nTo install git-sizer, you need to download the executable for your system from its Releases page and then add its directory to your system\u0026rsquo;s PATH environment variable.\nAfter installation, navigate to your repository directory and run:\ngit-sizer --verbose This tool doesn\u0026rsquo;t just look at file sizes; it also analyses the number of blobs, tree depth, and more, helping you get a comprehensive understanding of how \u0026ldquo;bloated\u0026rdquo; your repository is.\nRemove the File from History We will use git-filter-repo, a modern tool, to rewrite the history. It is the officially recommended next-generation tool, replacing git filter-branch and BFG Repo-Cleaner.\nInstall via pip:\npip install git-filter-repo If you have made the modern switch to uv, you can use:\nuv tool install git-filter-repo Tip: For safety, git-filter-repo requires you to operate on a fresh, clean clone of the repository. It automatically disconnects from the remote repository to prevent you from accidentally pushing incorrect changes.\nWhen using git-filter-repo to delete a specific file, you must not forget the --invert-paths parameter! Otherwise, everything in the repository except for that file will be deleted. (Calling out gemini-2.5-flash for this one.)\nTo delete a single file or folder:\ngit filter-repo --path path/to/your/example.jpg --invert-paths To delete files of a specific type or pattern:\ngit filter-repo --path-glob \u0026#39;*.jpg\u0026#39; --invert-paths To delete all files with the same name:\ngit filter-repo --path \u0026#39;.DS_Store\u0026#39; --invert-paths After execution, git-filter-repo will automatically clean up the relevant commits.\nPushing Again to the Remote Repository Now you need to force-push the changes to the remote repository.\nTo prevent accidentally committing these types of files again in the future, it\u0026rsquo;s best to add them to your .gitignore file first.\nRe-link the remote repository and force-push git-filter-repo will have removed the origin configuration. You need to add it again and use a forced mirror push to the remote. --mirror is a powerful operation, so make sure you have backed up your repository beforehand.\ngit remote add origin \u0026lt;your-repo-url\u0026gt; git push --force --mirror Synchronising All Collaborators\u0026rsquo; Local Repositories This step is crucial and also the costliest part of the entire process.\nAll collaborators (including yourself on other devices) must not use git pull to update their local repositories anymore. Because the local history and the rewritten remote history are now completely different, they must git clone a fresh repository from the remote.\nTherefore, before performing a history cleanup, be sure to think twice and coordinate fully with all project participants.\n","permalink":"https://jackchou.top/en/posts/git-rm-files/","summary":"To be honest, I do this so often myself that I had to write something as a reminder (and for my own future reference).","title":"How to Completely Remove a File from Git History"},{"content":"I remember trying mate tea at Cotti a long time ago, and the taste was\u0026hellip; let\u0026rsquo;s just say, hard to describe. This little shop on Ningbo Road, however, specialises in mate tea, and it\u0026rsquo;s actually quite nice.\nThere\u0026rsquo;s a black cat in the shop, along with many more black cat-themed items, like stickers, merchandise, and plush toys.\n","permalink":"https://jackchou.top/en/photos/cat-250628/","summary":"(9 Images) Some cat energy","title":"📷 Cats in a Mate Tea Shop"},{"content":"This will document content related to the image blog, including how to improve image quality and experience, and colour management within limited bandwidth and object storage.\nIf you are also interested in these topics, you are welcome to continue reading or discuss with me. Otherwise, this content should be \u0026ldquo;transparent\u0026rdquo; to most readers.\nFormat AVIF compression parameters, image resolution, and file size are still being adjusted.\nBased on visual observation, a 4096px image with a 500KB AVIF file achieves a good balance of visual quality, loading speed, and cost.\nOSS Having considered Alibaba Cloud OSS, with the bucket located in Hangzhou, enabling public read would result in significant outbound traffic and average access speeds overseas. Using a CDN would require domain name registration, which is essentially no less troublesome than using R2 and then adding a CDN.\nThe cost for Alibaba Cloud\u0026rsquo;s same-city redundant storage is 0.15 RMB per GB per month, while R2 is 0.015 USD per GB per month, with a 10GB free allowance.\nMost importantly, R2 does not charge for outbound traffic. Although domestic access is slower, it won\u0026rsquo;t break the bank. Therefore, we will temporarily inconvenience domestic users by using R2. If there are indeed many domestic readers in the future, we will consider adding a CDN.\nSpecific Format of Articles The difficulty lies in the article filenames, as they become part of the URL. For now, we will use the inelegant but relatively efficient method of combining the theme and date.\nAnother key point is that there may not be many opportunities to produce large quantities of single-topic images (in layman\u0026rsquo;s terms, no long trips). If the content is too fragmented, it\u0026rsquo;s also not ideal. Therefore, we tentatively plan to indicate the number of images in the summary (similar to the \u0026rsquo;nP\u0026rsquo; notation from the era of image-heavy pages). We will add a brief text description to form a travelogue or diary format centred around images.\nHDR Even RedNote supports HDR images, so what am I waiting for?\nFortunately, since we are using a browser, supporting HDR images is a breeze. Moreover, with an open-source project nearing completion, converting between various HDR image formats is no longer a challenge. The initial idea is to use AVIF with the PQ transfer function (dubbed ISO-22028-5), which will unfortunately inconvenience Android users.\nColour Management During an interview, the interviewer asked: How can we ensure that images are displayed consistently on everyone\u0026rsquo;s terminal?\nUpon careful consideration, my answer would be: Perhaps it\u0026rsquo;s impossible. The situations of terminal devices are far too complex; each device\u0026rsquo;s display technology, colour management, and user settings differ, making it difficult to guarantee a \u0026ldquo;consistent\u0026rdquo; display effect. To give an extreme example, I have seen a considerable number of devices with multiple \u0026ldquo;PC Manager\u0026rdquo; eye protection modes enabled simultaneously, and the resulting display effect is imaginable.\nHowever, what I can do is provide a solution at the content provider end that is as unified and compatible as possible. Currently, this might be the nclx identifier. It is simple enough, lightweight enough, and testing shows good browser compatibility. The rest is to trust the browser and the system\u0026rsquo;s colour management.\n","permalink":"https://jackchou.top/en/photos/test-photos/","summary":"Click five times on the version number to enter developer mode","title":"Instructions and Test Page"},{"content":"WYSIWYG (ˈwɪziwɪɡ)\nWhat You See Is What You Get\nThe last few chapters of this book feel a bit unrestrained, with few references provided, making them difficult to understand. However, to maintain the integrity of the content (and because I\u0026rsquo;ve already renewed this book three times), I\u0026rsquo;ll push through and finish writing this.\n“Attributes” First, a series of \u0026ldquo;attributes\u0026rdquo; for colour reproduction are defined:\nScope: Divided into \u0026ldquo;related to the colour reproduction itself\u0026rdquo; and \u0026ldquo;related to the relationship between the colour reproduction and the original colour\u0026rdquo;. I didn\u0026rsquo;t quite grasp this; most fall into the latter category, while those related to preferred colours belong to the former. This probably refers to whether the goal of colour reproduction is to restore the original colour to varying degrees (be it in terms of spectrum, tristimulus values, or colour appearance) or if it has other objectives. For example, when photographing a colour chart under low colour rendering light, should the reproduction show the inaccurate colours, or the colours of the chart under a standard illuminant.\nApplication: Divided into application-dependent or application-independent. I didn\u0026rsquo;t understand this either; most are in the latter category, with only creative colour reproduction being in the former. This is likely more \u0026ldquo;radical\u0026rdquo; than the previous case. Although preferred or memory colours also involve adjustments that deviate from an \u0026ldquo;accurate\u0026rdquo; state, they still align with psychophysical phenomena (memory colour is also considered a type of colour appearance phenomenon). Application-dependent might refer to more creative and exaggerated processing for specific application scenarios, such as converting to black and white or artistic colourisation.\nNature: Divided into deterministically describable (where the colour reproduction is known to have such properties) and \u0026ldquo;ideal state reproduction\u0026rdquo;. I didn\u0026rsquo;t quite get this either. The general idea seems to be that the former can be determined (described using relationships like \u0026ldquo;equal\u0026rdquo; or \u0026ldquo;relative\u0026rdquo;), while the latter is more ambiguous (like memory colours, preferred colours, etc.).\nExpression: Divided into what can be quantitatively measured by physical methods and what can be qualitatively measured by psychovisual methods. This is easy to understand. The book mentions that \u0026ldquo;quantitatively measurable\u0026rdquo; and \u0026ldquo;related to the original colour\u0026rdquo; always appear together, but this seems neither sufficient nor necessary. For example, a colour reproduction with equal colour appearance is related to the original colour but cannot be quantitatively measured.\nThese attributes also have their own codes (a capital letter), but I can\u0026rsquo;t see any relationship between them. The codes don\u0026rsquo;t even match the examples that follow, and I can\u0026rsquo;t find any literature that mentions these things.\nHunt\u0026rsquo;s Classification Hunt divides the goals of colour reproduction into several categories, from Chapter 11 of his book The Reproduction of Colour.\nSpectral Colour Reproduction: The spectra of the original and the reproduction are identical. An example is Lippmann photography (a 1908 Nobel Prize winner), which is achieved through interference. The Swiss Federal Institute of Technology Lausanne (EPFL) replicated and analysed this in detail in 2021 1.\nExact Colour Reproduction: The tristimulus values (in absolute terms) of the original and the reproduction are identical. This means that under the same viewing conditions, the absolute colour appearance attributes (lightness and colourfulness) are identical.\nColorimetric Colour Reproduction: The colour coordinates (tristimulus values normalised to the corresponding white) of the original and the reproduction are identical. This means that under the same viewing conditions, the relative colour appearance attributes (lightness and chroma) are identical.\nThe three types above are physically measurable, but it is important to note that good colour reproduction results can only be achieved under identical viewing conditions, as they do not account for the influence of viewing conditions on colour perception. A good example is reproducing a white sheet of paper under incandescent light on a self-luminous display. If the tristimulus values or colour coordinates are identical, the colour on the display will appear more yellow. This is possibly because a self-luminous display does not induce the discounting-the-illuminant phenomenon, leading to a lower degree of chromatic adaptation than in an incandescent lighting environment.\nEquivalent Colour Reproduction: Under their respective viewing conditions, the absolute quantities of the colour appearance attributes are the same, but the tristimulus values are different. This takes into account the effects of viewing conditions on colour perception, including chromatic adaptation.\nCorresponding Colour Reproduction: Under their respective viewing conditions, the relative quantities of the colour appearance attributes are the same, but the colour coordinates are different. The ideal colour reproduction for a camera should fall into this category.\nThese two types are the counterparts of the first two types mentioned above, but they take colour appearance into account. Ignoring the problem of metamerism, it is theoretically possible to achieve a \u0026ldquo;What you see is what you get\u0026rdquo; effect. The original text mentions that in equivalent colour reproduction, although the absolute luminances are different, they should be relatively close. However, in reality, due to the high non-linearity of lightness perception at high brightness levels, the absolute luminances can differ significantly.\nPreferred Colour Reproduction: Colours are adjusted to satisfy preference. For example, making blue skies bluer, skin tones rosier, or increasing lightness to meet people\u0026rsquo;s preferences for pictures. This can be understood as image retouching, and a large part of a camera\u0026rsquo;s colour reproduction falls into this category.\nICC\u0026rsquo;s Four Rendering Intents The goal of colour reproduction in the ICC colour management system is called Rendering Intent, and it is divided into four types:\nPerceptual: Preserves the overall colour appearance of the image. When gamut sizes do not match, it will compress or expand the gamut to fit the target gamut. It changes the hue and saturation of all colours in the image but maintains the overall visual relationship between the colours. Saturation: Aims to make the saturation of the target image as high as possible. When compressing or expanding the gamut, it does not prioritise matching the overall colour appearance. Generally, the target gamut is larger than that of the perceptual intent. Absolute Colorimetric: Matches the colorimetry as accurately as possible. The parts of the source and target gamuts that overlap remain unchanged. Colours outside the target gamut are mapped to the closest possible position while preserving their hue. Relative Colorimetric: The difference from the absolute colorimetric rendering intent is that it first transforms the white point and black point of the original image to the white point and black point of the target gamut. Then, colours that are out of gamut after this transformation are mapped to the nearest position while preserving their hue. Here, the white point and black point refer only to luminance. Both absolute and relative colorimetric intents involve chromatic adaptation. The absolute luminance in the absolute colorimetric rendering intent also remains largely unchanged, even if the target gamut has a brighter white point.\nGamut A gamut refers to the range of colours that a device or system can reproduce. A gamut has different shapes in different spaces. For an additive system like a display, the RGB gamut is a cube. After applying a non-linearity (Gamma) to RGB, it remains a cube because R, G, and B are linearly independent. By using a transformation matrix from RGB to XYZ, the cube is mapped into the XYZ space, where the gamut becomes a parallelepiped.\nWhen we mention \u0026ldquo;closest\u0026rdquo;, \u0026ldquo;minimum difference\u0026rdquo;, or \u0026ldquo;hue\u0026rdquo;, we are referring to these concepts within a uniform colour space. The transformation from XYZ to a uniform colour space includes a non-linear step. Even simple uniform colour spaces like IPT or sUCS consist of two linear transformations with a non-linear transformation sandwiched between them.\nThis non-linear transformation turns the parallelepiped into a very complex shape containing curved surfaces and lines, and it is non-convex in most cases. To describe such a complex boundary, a convex hull or an $\\alpha$-shape can be used as an approximation.\nOnce the shape of the gamut in the uniform colour space is determined, out-of-gamut colours are moved into the gamut along different \u0026ldquo;paths\u0026rdquo; (e.g., lines of constant lightness, constant hue, or other oblique lines). The distance of the move can be determined by a distance parameter. The simplest and most direct method is clipping, where any colour outside the gamut is moved directly to the gamut boundary.\nThat\u0026rsquo;s a Wrap! This concludes the notes on this book. More content on the last two chapters regarding colour management, displays, and capture devices will be introduced in other works (as they are covered too hastily in this book).\nProcrastinating from the winter break to the summer break, I\u0026rsquo;ve managed to re-learn colour science. I hope both you and I have made new discoveries and gains.\nG. Baechler, A. Latty, M. Pacholska, M. Vetterli, and A. Scholefield, “Shedding light on 19th century spectra by analyzing Lippmann photography,” Proceedings of the National Academy of Sciences, vol. 118, no. 17, p. e2008819118, Apr. 2021, doi: 10.1073/pnas.2008819118.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://jackchou.top/en/posts/pcrdi07/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep. 7","title":"Principles of Colour Reproduction in Digital Images Notes: Colour Reproduction and Gamut"},{"content":"I realised I haven\u0026rsquo;t written a blog post in a while. I actually have many drafts, but they are all in a state of being unfinished.\nI\u0026rsquo;m porting over an issue from GitHub that helps with the correct use and understanding of various HDR transfer functions.\nhttps://github.com/colour-science/colour/issues/1348\nThe gist of the issue is that, following Table 1 of BT.2408, if you place diffuse white at a display luminance of 203.0 nits, the calculated display luminance for an 18% grey card is not 26 nits.\nBT.2408 and the Problem BT.2408 is a standard concerning operational practices for HDR. Table 1 recommends the display luminance and corresponding code values under different transfer functions for several objects.\nReflectance Object or Reference (Luminance Factor, %) Nominal Luminance, cd/m² (PQ \u0026amp; 1000 cd/m² HLG) Nominal Signal Level Nominal Signal Level Grey Card (18%) 26 38 38 Greyscale Chart Max (83%) 162 56 71 Greyscale Chart Max (90%) 179 57 73 Reference Level: HDR Reference White (100%) also diffuse white and Graphics White 203 58 75 The question is how to use HDR transfer functions to convert between these numbers, for instance, using Python\u0026rsquo;s colour library.\nimport colour Theory The table is divided into three parts. The first column is reflectance, corresponding to scene-referred linear light. The second column is display luminance, which is display-referred linear light. The last two columns are the code values for the two systems.\nTheir numerical ranges are:\nScene Light: 0.0 - 1.0. This is relative and, for example, in a photography system, is controlled by the camera\u0026rsquo;s exposure. The scene light for a grey card and diffuse white could be 0.18 and 1.0, or it could be 0.018 and 0.1. Display Light: 0.0 - 10000 nits. This is absolute in an HDR system. For the HLG system, a peak luminance must be specified, for example, 1000 nits. Code Value: 0 - 255 or 0 - 1023, depending on the system\u0026rsquo;s bit depth. It can also be represented by floating-point numbers from 0.0 - 1.0. Personally, I find floating-point numbers easier to work with. A transfer function system defines how to perform these conversions between luminance and code values.\nThe conversion from scene linear light to display linear light is called the opto-opto transfer function (OOTF). The conversion from scene linear light to code value is called the OETF (Optical-Electrical Transfer Function). The conversion from code value to display linear light is called the EOTF (Electrical-Optical Transfer Function). It is particularly important to note that the OETF is not the inverse of the EOTF. The conversion from display linear light to code value should use the inverse of the EOTF, not the OETF. Similarly, the conversion from code value to scene linear light should use the inverse of the OETF, not the EOTF.\nThe conversion relationships between scene light, display light, and code values actually only require two transfer functions. PQ uses the EOTF and OOTF, while HLG uses the OETF and OOTF. In BT.2100, the PQ OETF is equivalent to the OOTF followed by the inverse EOTF, and the HLG EOTF is equivalent to the inverse OETF followed by the OOTF.\nThe colour library provides implementations of these transfer functions, including the PQ OETF and the HLG EOTF.\nPQ PQ is display-referred. The complete conversion process is: use the OOTF to convert scene linear light to display linear light, then use the inverse EOTF to convert display linear light to a code value. During display, the EOTF is used to convert the code value back to display linear light.\nLet\u0026rsquo;s start by setting the display luminance of diffuse white to 203 nits. The corresponding scene linear light is obtained using the inverse OOTF. In scene linear light, the ratio of the grey card to diffuse white is 0.18. We can calculate the scene light for the grey card and then apply the OOTF to get its display linear light (26 nits). By applying the inverse EOTF to the display linear light of both diffuse white and the grey card, we get their corresponding code values.\ndisp_lin_white = 203 scene_lin_white = colour.models.ootf_inverse_BT2100_PQ(disp_lin_white) print(\u0026#34;Scene Linear:\u0026#34;, scene_lin_white) # 0.0307311888745 scene_lin_grey = scene_lin_white * 0.18 print(\u0026#34;Scene Linear Grey:\u0026#34;, scene_lin_grey) # 0.00553161399741 disp_lin_grey = colour.models.ootf_BT2100_PQ(scene_lin_grey) print(\u0026#34;Display Linear Grey:\u0026#34;, disp_lin_grey) # 25.6890888306 signal_white = colour.models.eotf_inverse_BT2100_PQ(disp_lin_white) print(\u0026#34;Signal White:\u0026#34;, signal_white) # 0.580688881042 signal_grey = colour.models.eotf_inverse_BT2100_PQ(disp_lin_grey) print(\u0026#34;Signal Grey:\u0026#34;, signal_grey) # 0.378961634455 HLG HLG is scene-referred. The complete conversion process is: use the OETF to convert scene linear light to a code value. During decoding, use the inverse OETF to convert the code value back to scene linear light, and then use the OOTF to convert the scene linear light to display linear light.\nSimilarly, we set the display luminance of diffuse white to 203 nits. The corresponding scene linear light is obtained using the inverse OOTF. In scene linear light, the ratio of the grey card to diffuse white is 0.18. We calculate the scene light for the grey card, apply the OETF to the scene light to get the code value, and apply the OOTF to the scene light to get the display linear light.\ndisp_lin_white = 203 scene_lin_white = colour.models.ootf_inverse_BT2100_HLG(disp_lin_white) print(\u0026#34;Scene Linear:\u0026#34;, scene_lin_white) # 0.264797185624 scene_lin_grey = scene_lin_white * 0.18 print(\u0026#34;Scene Linear Grey:\u0026#34;, scene_lin_grey) # 0.0476634934123 disp_lin_grey = colour.models.ootf_BT2100_HLG(scene_lin_grey) print(\u0026#34;Display Linear Grey:\u0026#34;, disp_lin_grey) # 25.9312256307 signal_white = colour.models.oetf_BT2100_HLG(scene_lin_white) print(\u0026#34;Signal White:\u0026#34;, signal_white) # 0.749877364632 signal_grey = colour.models.oetf_BT2100_HLG(scene_lin_grey) print(\u0026#34;Signal Grey:\u0026#34;, signal_grey) # 0.378140820644 It is worth noting that the HLG OOTF changes with the peak luminance of the display. If no parameters are passed, colour defaults to a peak luminance of 1000 nits, for which the corresponding OOTF is a gamma of 1.2. Additionally, the HLG OOTF should be applied to the RGB components; passing only a luminance value will raise a warning, but the approach and the result are correct.\nA Few Afterthoughts As for why the OETF and EOTF cannot be used as inverse functions of each other, it is precisely because the OOTF is at play. In the most ideal scenario, where the display perfectly reproduces the scene light, the OOTF would be a linear function. In that case, the OETF and EOTF would be inverse functions (with a luminance coefficient).\nThe OOTF used by PQ is a combination of the BT.709 OETF and the BT.1886 EOTF, with a curvature between linear and a gamma of 1.2. The OOTF used by HLG is a luminance-dependent gamma function, which is a gamma of 1.2 at 1000 nits.\nPersonally, I feel that understanding these transfer functions is necessary, especially in HDR systems where conversions between different transfer functions are often required. The table in BT.2408 provides a good reference, but in practical applications, these transfer functions, especially the OOTF, are rarely referenced directly. When creating PQ content, adjustments are typically made with respect to display light, and then the inverse EOTF is used to convert to code values, without involving the OOTF. The design of HLG to use an OOTF is more likely for compatibility with SDR systems and broadcast production.\nThis small experiment also serves as good proof that as long as the calculations are correct, the results obtained using either transfer function will be the same (aside from quantisation differences). It is not the case that using HLG will produce a different effect from PQ.\nEnglish Lesson In the issue, there is a sentence: Now, the penny finally dropped.\nThis expression originates from old-fashioned vending machines, which required a coin to be inserted before they would operate. Sometimes the coin would get stuck and only fall into place after a moment, at which point the machine would start working. Therefore, this phrase is a metaphor for \u0026lsquo;suddenly understanding\u0026rsquo; or \u0026lsquo;grasping something one didn\u0026rsquo;t understand before\u0026rsquo;.\n","permalink":"https://jackchou.top/en/posts/bt2408/","summary":"Some tedious definitions, with love from BT.2408","title":"A Small Experiment to Help Understand HDR Transfer Functions"},{"content":"Objective One of the goals of image processing is Colour Reproduction. When placed in a real scene, the light received can originate from reflection, transmission, scattering, or a mixture of these from different objects, or directly from light sources. Their spectra vary greatly, and their luminance can span a wide range. Displays, on the other hand, can only provide a limited luminance range and spectroscopically can only offer a mixture of three primaries.\nFortunately, metamerism and the complexity of visual colour perception make Colour Reproduction possible. Through the research and development in colour science, what we pursue is no longer merely the reproduction of luminance or colour coordinates, but the reproduction of Colour Appearance. This requires the involvement of Colour Appearance Models.\nColour Appearance Models can be used to predict the Colour Appearance (Lightness, Chroma, etc.) of a tristimulus value under specific Viewing Conditions. For image and Colour Reproduction, it is necessary to consider the vastly different Viewing Conditions between the real scene and the Display. Traditional cinemas can only provide a luminance of about 48 nits, yet they can reproduce vivid scenes. This is partly because cinemas provide a nearly dark, low-luminance Viewing Condition.\nBy using a forward Colour Appearance Model, the Colour Appearance of each pixel is calculated from its tristimulus values. Then, based on the Viewing Conditions of the Display, an inverse Colour Appearance Model is applied to predict what tristimulus values the Display needs to produce to achieve that Colour Appearance. This constitutes a relatively complete image processing pipeline for Colour Reproduction using Colour Appearance Models.\niCAM06 is a Colour Appearance Model proposed by Jiangtao Kuang et al. for rendering HDR images. It incorporates many theories and algorithms from Colour Appearance Models, achieving a relatively scientific image processing approach from scenes with a large luminance range to Displays.\nStarting Point: Input A typical Colour Appearance Model accepts tristimulus values and Viewing Conditions as input.\nTristimulus values differ from the more common RGB space; they are a device-independent space. They are independent of the device, and under the same Viewing Conditions, if two colours have equal tristimulus values, they will achieve a match (look the same). RGB is a device-dependent space; for example, if two different Displays both display a pure red, the RGB values might be equal, but the colours are likely different because different Displays have different red primaries.\nIn a traditional image processing workflow, starting from a raw image and going through White Balance and Colour Correction Matrix (CCM), the colour space can be converted to XYZ tristimulus values. A common implementation is in the raw image processing library rawpy, where output_color = rawpy.ColorSpace(5) specifies the output colour space.\nIt is important to note that the XYZ values obtained this way are not the tristimulus values of the actual scene but are already White Balanced (Chromatic Adapted) tristimulus values. What we need are colours that represent the actual scene; therefore, a slightly modified initial ISP is required.\nFrom a more fundamental perspective, how is a raw image generated from the spectrum?\n$$ R=\\int P(\\lambda)\\,\\bar{r}(\\lambda)\\,\\mathrm{d}\\lambda $$The equation above represents an ideal image Sensor\u0026rsquo;s expression, where $P$ is the spectral power, $\\bar{r}$ is the Sensor\u0026rsquo;s Spectral Sensitivity Function, determined by the spectral characteristics of the photodiode, the transmittance of the colour filter, etc. $R$ is the output pixel value, which can be read from the raw file.\nThe expression for tristimulus values can be written as:\n$$ X=\\int P(\\lambda)\\,\\bar{x}(\\lambda)\\, \\mathrm{d}\\lambda $$Here, $\\bar{x}$ represents the Spectral Sensitivity Function of the human eye. Therefore, if we can linearly combine $\\bar{r}(\\lambda),\\bar{g}(\\lambda),\\bar{b}(\\lambda)$ to produce $\\bar{x}(\\lambda)$, etc., we can estimate the tristimulus values using the camera\u0026rsquo;s raw pixel values. This linear combination process can be represented by a 3x3 matrix.\nThe image above illustrates using a linear combination to predict tristimulus values from the camera\u0026rsquo;s Spectral Sensitivity Functions. The camera\u0026rsquo;s Spectral Sensitivity Functions after linear combination have shapes similar to the Colour Matching Functions of the tristimulus values.\nAdditionally, a coefficient is needed for scaling to convert to tristimulus values representing Absolute Luminance. This coefficient can be calculated from the camera\u0026rsquo;s Aperture, Shutter Speed, and ISO. These parameters used to control the amount of light entering do not affect the linearity or relative relationships of the light.\nAnother scenario involves using High Dynamic Range Images as input, such as HDR images encoded with the PQ Transfer Function. First, the non-linearly encoded RGB pixel values are decoded using the EOTF to obtain linear RGB pixel values. Then, the linear RGB pixel values are converted to the XYZ space to obtain the tristimulus values. Several of the .tif files and image_input.py provided in the project utilize this type of input.\nImage Decomposition According to the visual system\u0026rsquo;s different perception of colour and detail, the image is decomposed into a Base Layer and a Details Layer. Operations on colour, such as Chromatic Adaptation and Tone Compression, are only applied to the Base Layer. The Details Layer, after enhancement or adjustment, is merged with the adjusted Base Layer.\nThe Base Layer is obtained using an edge-preserving Bilateral Filter, a method previously proposed by Durand and Dorsey. Bilateral Filtering is a non-linear filter where the weight of each pixel is jointly determined by a Gaussian filter in the spatial domain and another Gaussian filter in the intensity domain. The latter reduces the weight of pixels with significant intensity differences from the center pixel.\nTherefore, Bilateral Filtering can effectively smooth the image while preserving sharp edges, thereby avoiding the \u0026ldquo;halo\u0026rdquo; artifacts common in local tone mapping operators. The intensity domain calculations are performed in Logarithmic Space, where intensity better represents perceived contrast and facilitates more uniform processing across the entire image.\nThe Details Layer is obtained by subtracting the Base Layer from the original image. Both layers need to be converted back to linear space.\nThe Bilateral Filtering used in iCAM06 is accelerated through piecewise linear approximation and nearest-neighbor downsampling.\nChromatic Adaptation The colour of an object changes under different lighting and Viewing Conditions, but the human visual system can maintain a relatively stable perception of the object\u0026rsquo;s colour to some extent. This phenomenon is called Colour Constancy. This process of maintaining relative stability is called Chromatic Adaptation.\nA Chromatic Adaptation Transform (CAT) is a model used to predict corresponding colours. It takes two Viewing Conditions (usually represented by the tristimulus values of the scene white point) and a colour under one Viewing Condition as input, and predicts the colour under the other Viewing Condition that would be a corresponding colour.\nAccording to von Kries\u0026rsquo; hypothesis, Chromatic Adaptation is independent at the level of the visual organs. The basic structure of a Chromatic Adaptation Transform is:\nConvert the input XYZ to a space representing the visual organs (Cone responses). Process each quantity independently in this space (e.g., multiply by respective gain factors). Convert back to XYZ space to obtain the colour tristimulus values under the other Viewing Condition. There are many Chromatic Adaptation Transforms designed according to this structure, among which CAT02 and CAT16 are two CAT models successively recommended by CIE. In the second step, there is an adaptation degree D, representing the extent of Chromatic Adaptation. In iCAM06, this adaptation degree is multiplied by a coefficient of 0.3, which is equivalent to reducing the degree of adaptation, bringing it closer to the colours in the scene rather than the adapted corresponding colours, to increase the colour saturation of the image.\nThis is a very strange practice. I am more inclined to believe it is a code error that causes numerical errors if this coefficient is not multiplied. This is because the two Cone responses used in the Chromatic Adaptation step in the original code are one normalized and one absolute value.\nIn iCAM06, the target adaptation field for this step of Chromatic Adaptation is D65, because the subsequent uniform colour spaces are designed for the D65 white point. The white point of the adaptation field uses a Gaussian blur of the Base Layer, which is somewhat similar to the Grey World hypothesis.\n$$ \\begin{align*} D \u0026= 0.3 F \\left[ 1 - \\left( \\frac{1}{3.6} \\right) e^{-\\frac{(L_A - 42)}{92}} \\right] \\\\ R_c \u0026= \\left[ \\left( R_{D65} \\frac{D}{R_W} \\right) + (1-D) \\right] R \\end{align*} $$In the original equation, the sign of 42 in the exponent of $e$ in the calculation of adaptation degree D is incorrect.\nThe Adaptation White (Above) and Adapted Image (Below) Tone Compression The human eye\u0026rsquo;s perception of luminance is not linear, but highly non-linear. By applying Tone Compression according to this non-linear characteristic, it is possible to reproduce Colour Appearance with a larger luminance range within a limited Display luminance range.\nThis non-linear relationship is also obtained from visual experiments. iCAM06 uses the post-adaptation part from CIECAM02, which is shaped like a Sigmoid function. When used, it involves converting from tristimulus values to another space representing visual cells, and then applying a response curve called \u0026ldquo;post-adaptation\u0026rdquo;. In iCAM06, the response of Rods under scotopic vision is also added and superimposed on the Cone response to predict luminance in the scotopic-mesopic range; the Rod response is very small.\nThe post-adaptation non-linear relationship for Cones is as follows:\n$$ R'_a = \\frac{400 (F_L R' / Y_W)^p}{27.13 + (F_L R' / Y_W)^p} + 0.1 $$The reference white $Y_{W}$ used in this step is also a Gaussian blur of the Base Layer, but with a greater degree of blur than in Chromatic Adaptation.\nThis step completes the compression of luminance. The original large luminance range, after passing through this Sigmoid function, has a range of 0.1 to 400, although it rarely exceeds 200. Before this step, the relationship was linear with the scene light; after this step, it is linear with the Display light. Therefore, this step can also be understood as an Optic-Opto Transfer Function (OOTF).\nMerging Image and Output After completing Chromatic Adaptation and Tone Compression, the Base Layer is an image that can be displayed relatively normally on a screen. The Details Layer can be enhanced and merged back.\nThe image obtained at this point is still in the linear XYZ tristimulus value space. Converting XYZ to the RGB space suitable for display involves two steps:\nConversion to linear RGB space. Applying the Transfer Function encoding. For the most common sRGB space, the matrix used in the first step is readily available online. The second step is a Gamma Correction, with a coefficient equal to the reciprocal of the Display\u0026rsquo;s Gamma, usually between 0.45 and 0.5.\nAdditional Operations: IPT Space Compressing an original high dynamic range, high luminance image onto a low luminance Display can sometimes result in less vivid colours, and the contrast between light and dark areas also needs to be enhanced.\nThe solution in iCAM06 is to convert to a uniform colour space for enhancement, choosing the IPT space. I represents Lightness, and P and T represent two colour directions, red-green and yellow-blue respectively.\nThe method for enhancing contrast is to apply a Gamma exponent between 1.0 and 1.5 to Lightness, with the value determined by the Viewing Environment. The principle is that perceived contrast changes according to the relative luminance of the Viewing Environment. Dark environments like cinemas require higher contrast, so a higher System Gamma exponent is usually adopted. A potential issue is that past System Gamma was applied to linear light, not to a non-linear scale like Lightness.\nThe method for enhancing Chroma is to stretch the two colour directions. The degree of stretching is related to luminance, based on the Hunt effect: an increase in luminance leads to an increase in perceived Colourfulness.\n$$ P = P \\cdot \\left[ (F_L + 1)^{0.2} \\left( \\frac{1.29C^2 - 0.27C + 0.42}{C^2 - 0.31C + 0.42} \\right) \\right] $$\nResults and Analysis This algorithm addresses two problems:\nHow to reproduce real-world scenes on a display. How to reproduce high dynamic range images on traditional low dynamic range displays. Unlike computer vision, colour science focuses more on human visual perception, aiming to process images from a visual perspective. iCAM06, through methods such as Chromatic Adaptation, Tone Compression, and uniform colour spaces, provides an interpretable solution for image processing from high dynamic range to low dynamic range.\nHowever, iCAM06 also has some shortcomings:\nThe Chromatic Adaptation algorithm has issues, and the effect after correction is not ideal, possibly due to the limitations of the Chromatic Adaptation model and the influence of the Grey World hypothesis. The Sigmoid function used for Tone Compression reduces image contrast too much and cannot balance the effects for both low dynamic range and high dynamic range inputs. Edge-preserving transformation and detail enhancement may introduce artifacts and excessive sharpening. Processing in a uniform colour space lacks reliable theoretical basis, especially the practice of applying a gamma exponent to Lightness. Overall, iCAM06, leveraging research from colour science, proposes an effective method for high dynamic range image processing and is a successful exploration of integrating colour science into image processing.\nReferences [1] M. D. Fairchild and G. M. Johnson, \u0026ldquo;Meet iCAM: A next-generation color appearance model,\u0026rdquo; Proc. 10th Color Imaging Conf., vol. 10, no. 1, pp. 33–38, Jan. 2002.\n[2] J. Kuang, G. M. Johnson, and M. D. Fairchild, \u0026ldquo;iCAM06: A refined image appearance model for HDR image rendering,\u0026rdquo; J. Visual Communication and Image Representation, vol. 18, no. 5, pp. 406–414, Oct. 2007.\n[3] F. Durand and J. Dorsey, \u0026ldquo;Fast bilateral filtering for the display of high-dynamic-range images,\u0026rdquo; in Proc. 29th Annual Conf. Computer Graphics and Interactive Techniques (SIGGRAPH), San Antonio, TX, USA, Jul. 2002, pp. 257–266.\n[4] P. Hung and R. S. Berns, \u0026ldquo;Determination of constant hue loci for a CRT gamut and their predictions using color appearance spaces,\u0026rdquo; Color Research \u0026amp; Application, vol. 20, no. 5, pp. 285–295, Oct. 1995.\n[5] M. R. Luo and C. Li, \u0026ldquo;CIECAM02 and its recent developments,\u0026rdquo; in Advanced Color Image Processing and Analysis, C. Fernandez-Maloigne, Ed., New York, NY, USA: Springer, 2013, pp. 19–58.\n[6] M. D. Fairchild, \u0026ldquo;A revision of CIECAM97s for practical applications,\u0026rdquo; Color Research \u0026amp; Application, vol. 26, no. 6, pp. 418–427, 2001.\n","permalink":"https://jackchou.top/en/posts/icam06-survey/","summary":"Python Implementation of Image Colour Appearance Model (iCAM06)","title":"iCAM06: Colour Appearance Model in Image Processing"},{"content":"What is an \u0026ldquo;Image\u0026rdquo;? An image is essentially a \u0026lsquo;matrix\u0026rsquo; or an \u0026lsquo;array\u0026rsquo;. For example, the most common type of image might be an array with the shape (Height, Width, Channel), where the number at each position is quantised to eight bits, representing 256 different levels as an integer or floating-point number. The data stored here is generally called the code value or pixel value.\nEncoding Storing this array directly can result in a very large file, so compression methods, or \u0026rsquo;encoding\u0026rsquo;, are needed. Encoding methods are constantly evolving. For instance, there\u0026rsquo;s JPG (JPEG) encoding, which uses techniques like chroma subsampling, Discrete Cosine Transform (DCT), and Huffman coding for compression. There\u0026rsquo;s also the highly extensible TIFF (*.tif) format, which is a container format that can internally use various lossless/lossy encoding methods like ZIP, LZW, PackBits, and even JPEG. Then there are more advanced codecs like HEIC (HEIF/HEVC) and AV1, which feature flexible partitioning structures, multi-mode intra-prediction, and more advanced entropy coding, enabling very high encoding efficiency.\nThe encoding process may introduce some loss, which is known as lossy compression. The commonly used JPG standard includes both lossless and lossy modes, although the lossless mode is rarely used. Many later encoding methods also support lossless compression—AV1 even offers a true lossless profile. As long as one is not deliberately pursuing extreme compression ratios, the quality loss from lossy compression is actually difficult to perceive.\nAnother difference between various encoding methods is the quantisation bit depth they allow. The JPG standard supports 8-bit and 12-bit quantisation, but the 12-bit mode is also uncommon. Newer formats often support higher bit depths; for example, HEIC and AVIF can handle 10-bit or even 12-bit, while a TIFF container can hold data with 16 or even 32 bits (floating-point) per channel. This is closely related to the requirements of HDR: to avoid visible banding over a wider brightness range, higher quantisation precision is needed.\nAdditional Information A pile of numbers is meaningless on its own; we also need to know the space in which they are defined. At a minimum, the primaries, white point, and transfer function (i.e., the definition of the RGB space) must be specified.\nThis additional information tells the decoder and the system\u0026rsquo;s colour management the specific meaning of these numbers. Without it, the data is usually treated as sRGB, and if the colour space does not match the actual data, major problems will occur.\nAfter the decoding software extracts the pixel values and additional information, the colour management system converts these pixel values from a known colour space into driving values for the display, allowing them to be displayed correctly. Therefore, in theory, the code values can even be tristimulus values and linear, as long as there is corresponding additional information to define them. The images below are two PMCC colour charts tagged with the XYZ space and a linear transfer function. The pixel values in the images are directly the tristimulus values.\nThis is a tristimulus value image of a colour chart under a D65 illuminant, where values exceeding 1 have been clipped. This is actually an incorrect way of handling it; CIEXYZ in nclx requires an equal-energy white as the white point, and a Bradford CAT from an ICC profile could be used for the conversion.\nThis is the image adapted to an equal-energy white point (done by directly replacing the white point). Because it is adapted to an equal-energy white, no values will exceed 1. When displayed, the system\u0026rsquo;s colour management will perform chromatic adaptation to the display\u0026rsquo;s white point, so its appearance should be close to the background in light mode. The page background in light mode has a code value of 245.\nIf you are using iOS or iPadOS, you may not be able to see these two images. Testing on other systems has shown they are generally visible.\nAdditional information can take many forms, such as an embedded ICC profile, XML statements or nclx data stored within the image file, or it can be stored in the EXIF data.\nThis step is the most critical part of achieving an HDR effect. Once the correct transfer function is specified, the decoder can convert the code values into what is known as \u0026ldquo;HDR\u0026rdquo; content, which can exceed the nominal luminance of SDR.\nThere is a rather special method for implementing HDR called a Gain Map. A single file stores two images (an SDR image and a gain map) along with some corresponding additional information (specific gain coefficients, etc.). The decoder can then compute a new HDR image from the two images. Therefore, the gain map could perhaps also be considered a form of additional information.\nDisplay-Referred Linear Light When performing image format conversions, all references to linear light space should be Display-Referred. This means calculating the luminance (either brightness or absolute tristimulus values) of the image on the display after it has been shown.\nDuring decoding, the EOTF is used. During encoding, the inverse EOTF is used, not the OETF (there is a distinction for transfer functions like PQ).\nPQ or HLG Transfer Functions Similar to HDR video, changing the transfer function from Gamma or Rec. 709 to PQ or HLG can achieve the transition from SDR to HDR. For still images, the international standard ISO-22028-5 already exists.\nCanon was the first to introduce 10-bit HEIC encoding in its mirrorless cameras, using PQ as the transfer function. Sony has HLG for still images. In recent versions of ACR, the AVIF and 16-bit TIF files produced when HDR output is enabled without maximum compatibility are PQ-encoded.\nFor this type of HDR image, one simply needs to apply the correct transfer function to convert to or from linear light.\nGainmap A Gainmap is a method for implementing HDR specifically for still images. Its advantage is excellent compatibility, as it can store both SDR and HDR content simultaneously (rather than relying on dynamic metadata and a TMO) and is very friendly to display drivers.\nJPG, JXL, and AVIF can all store this format. In particular, a JPG with a Gainmap is essentially two JPG files concatenated together. Image viewers that do not support this format will simply read the first file as a standard SDR image. When sending the original image on social media, the subsequent Gainmap can be preserved. Even if the app itself does not support it, saving it to another app may still reveal the HDR effect.\nThe first large-scale application of Gainmap was likely on the OPPO Find X6 Pro. Later, Google promoted the UltraHDR format. The ISO is currently developing the ISO-21496-1 standard, and UltraHDR version 1.1 is already compatible with this standard.\nA Gainmap can be written for luminance only or for all three channels. The \u0026ldquo;ProXDR\u0026rdquo; in the recently released OPPO Find X8 Ultra refers to a three-channel Gainmap.\nA Gainmap can be understood as a form of Supplemental Enhancement Information (SEI) or Colour Remapping Information (CRI), which records the difference between the SDR and HDR sources. Additionally, it stores the absolute luminance relationship of the Gainmap through something akin to static metadata.\nThe metadata includes: content max luminance gain (how much brighter the HDR is compared to the SDR), display max luminance gain (how much brighter the master HDR is compared to the SDR), the Gamma used for encoding the gainmap, and an optional offset.\nRegarding the content max and display max luminance gains, an example is the HDR limiter in ACR, which can limit the HDR headroom during post-production to \u0026rsquo;n\u0026rsquo; stops. For example, if a three-stop limit is set, the maximum display gain during post-production is three stops, but the content may have a luminance gain exceeding three stops, which is simply clipped. The purpose of setting this display max luminance gain metadata is likely to restore the creative intent from the time of production.\nRegarding SDR\u0026rsquo;s Nominal Luminance Although rarely adhered to in practice, SDR actually has a specified white point luminance. For example, sRGB is 80 nits, and ITU-R BT.2035 specifies 100 nits.\nSDR content can be converted to absolute luminance based on this value, and then encoded using the inverse EOTF. More often, the nominal luminance used is 203 nits. This value originates from the recommendations for various luminance levels in ITU-R BT.2408, where diffuse white is 203 nits, but it also states that this diffuse white luminance should not be interpreted as the nominal luminance for SDR.\n","permalink":"https://jackchou.top/en/posts/hdr-format-conversion/","summary":"Conversion of HDR still image formats","title":"HDR Image Format Conversion"},{"content":" Note: I have not used Imatest myself (it\u0026rsquo;s expensive). The following content is based on my subjective and intuitive understanding after observing analyses done by others.\nPipeline sRGB -\u0026gt; XYZ -\u0026gt; CIELAB (Display) \u0026lt;-\u0026gt; CIELAB (Reference)\nThe test described above is for the final result (the output JPG image), not for the intermediate processes. The pipeline within the image processor affects the colours on the chart in various ways; that is to say, processes like auto exposure (AE) also influence the result.\nStarting from the final output JPG image (assuming it is in the sRGB space), it is first converted to XYZ tristimulus values (using the corresponding colour space\u0026rsquo;s conversion matrix and electro-optical transfer function). The resulting XYZ tristimulus values are display-referred and normalised, meaning they are identical to the tristimulus values measured on a perfectly colour-managed or standard display, divided by luminance.\nCalculating colour difference requires a uniform colour space. Converting XYZ to CIELAB also requires a reference white point ($\\text{XYZ}_{n}$). For display-referred colours, the display\u0026rsquo;s white point is a common choice.\nThe CIELAB values for a colour chart can usually be looked up directly, or if the chart\u0026rsquo;s reflectance data is public, its XYZ values can be calculated using the spectral power distribution of a standard illuminant (usually D65) and a standard observer\u0026rsquo;s colour matching functions. Then, using D65 as the reference white point, its CIELAB values can be calculated.\nNext, a colour difference formula is chosen in CIELAB to calculate the difference, such as the most commonly used DeltaE2000. Of course, CIELAB can be replaced with other uniform colour spaces or colour appearance models for more multi-dimensional comparisons.\nResults and Analysis Simply calculating the average colour difference for all colours is not sensible, as the conclusions that can be drawn are very limited (unless your sole goal is perfect colour reproduction).\nWhite Balance and Chromatic Adaptation In the CIELAB values calculated for the colour chart as described above, the chroma of the greyscale patches should be close to 0. Whether the chroma of the greyscale in the actual image is also close to 0 depends on the shooting light source and the camera\u0026rsquo;s white balance processing. If the camera\u0026rsquo;s white balance completely corrects for the light source (equivalent to fully adapting to D65), then the greyscale part of the colour chart in the image should also appear neutral.\nHowever, white balance algorithms do not necessarily perform complete chromatic adaptation for all shooting environments. This also contradicts our subjective perception. The most typical example is an incandescent lamp; under such a yellowish light source, complete colour constancy is not achieved. In this case, a more ideal colour reproduction would be to retain some of the \u0026ldquo;yellowness,\u0026rdquo; i.e., incomplete adaptation. The greyscale patches in the image of the colour chart would then not be neutral but would have a certain amount of chroma.\nIn situations where complete adaptation to D65 is required (the greyscale should appear neutral), the chroma deviation $\\Delta C$ in the greyscale area can be used to describe the accuracy of the white balance.\nIf the image\u0026rsquo;s white point is not D65, the target light source for white balance should be this white point. In this case, one must consider whether the incorrect chromatic adaptation transform in CIELAB will have an impact (CIELAB should ideally only work under D65).\nStylisation Since the test image has gone through the entire image processing pipeline, it will naturally include some stylisation, such as memory colour enhancement or more complex and aggressive processing. At this point, evaluating whether the colours are \u0026ldquo;accurate\u0026rdquo; becomes somewhat unreasonable. However, one can still observe the direction in which typical colours are shifted and adjusted by comparing the standard colours of the chart with the colours of the chart in the image.\nIt is important to note that one should no longer evaluate whether the colours are \u0026ldquo;accurate.\u0026rdquo; Moreover, because the camera does not satisfy the so-called Luther condition, there is already a significant error when converting from the camera\u0026rsquo;s RGB to XYZ. If you train and test using the colour chart on its own, the colour difference calculated on the XYZ values converted from RAW will generally not be less than 3. Subsequent stylisation, space conversions, and gamut and tone compression will only introduce more errors. Therefore, when judging accuracy or stylisation tendencies, one needs to consider whether they are caused by errors or introduced by stylisation. The accumulation of colour difference does not always proceed in one direction; it is possible that the final colour difference becomes very small, and even cases where the difference for individual colours is less than 1 may occur. This does not mean the camera can reproduce that colour so accurately under other conditions.\nWhen analysing stylisation, the ratio of the image\u0026rsquo;s average chroma to the chart\u0026rsquo;s average chroma can be used to observe whether the overall chroma of the image has increased. One can also observe the shift of each typical colour or convert to CIELCh and other spaces for a more in-depth analysis.\nAuto Exposure and Lightness The camera\u0026rsquo;s auto exposure strategy also affects colour difference. Furthermore, the XYZ values converted back from the final image are display-referred. Relative to the reflectance of the colour chart, the original scene\u0026rsquo;s luminance is not necessarily linear. Multiplying these display-referred XYZ values by a coefficient to simulate gain does not represent the exposure control at the time of shooting; it is more like adjusting the display\u0026rsquo;s backlight brightness (without synchronously adjusting the reference white point chosen for the CIELAB calculation). This is a very strange operation, but it can effectively reduce colour difference anomalies caused by exposure errors. In the example below, multiplying the XYZ values by a factor of 0.76 can reduce the colour difference by three units (this might only be meaningful computationally).\nTherefore, Imatest also provides a chroma difference that excludes lightness, calculating only the difference in chroma without considering the effect of lightness. This seems even stranger (because operations like changing backlight brightness or exposure control also affect chroma).\nUneven Illumination and Metamerism When photographing a colour chart, it is best to have uniform illumination. However, when using a light box, it is inevitable that the top part will be closer to the light source and thus brighter. After applying the simulated lightness adjustment mentioned above, one can observe the lightness difference. If the top rows all have positive values and the bottom rows all have negative values, one needs to consider whether this is due to uneven illumination.\nThe correction method on RAW data is to use a uniform surface (a grey card or matte photo paper) for calibration. However, the image has already passed through the entire ISP, and the resulting linear light is display-referred; it cannot represent the scene-referred linear light.\nRegarding the issue of metamerism: the CIELAB values provided by the colour chart are calculated from its reflectance and the D65 standard illuminant. However, in reality, no light source has a spectral power distribution identical to that of a standard illuminant. The camera\u0026rsquo;s spectral sensitivity functions, after correction by a colour matrix, also differ from the standard observer\u0026rsquo;s colour matching functions. Therefore, metamerism is unavoidable. This will also lead to an erroneous increase in colour difference, because the colour chart not only evaluates the camera\u0026rsquo;s colour reproduction but also, to some extent, the light source\u0026rsquo;s colour rendering index and the camera\u0026rsquo;s metamerism.\nColour Charts and Colour Analysis What role do colour charts really play in image processing and evaluation, and what do all those numbers in various spaces mean?\nFor me, the greatest value of a colour chart is that it provides 30 very typical reflectance samples, including memory colours, saturated colours, and neutral colours. The spectral data for standard illuminants and standard observers can be easily obtained, allowing for very flexible calculation of tristimulus values. This is far more meaningful than a set of CIELAB values that can only be used under D65.\nThis is also why using the PMCC is more recommended, as its reflectance data is published. In theory, you could even use it without owning a physical chart, simply treating it as 30 reflectance samples.\nFurthermore, I feel it is highly inappropriate to make subjective colour evaluations directly from an image of a colour chart. Divorced from the physical object, the effect of memory colours should be negligible. Can one really associate a blue patch with the blue sky and perform memory colour enhancement? For an observer, it is unreasonable to judge whether colours are natural, or even accurate, simply by looking at thirty coloured squares.\nAdditionally, when using various colour spaces and colour appearance models for analysis, one should be precise with terminology and not casually use words like \u0026ldquo;saturated\u0026rdquo; or \u0026ldquo;vivid.\u0026rdquo; If a colour appearance model is being used, its precise colour attributes should be used for description.\nRelevant data and articles about the colour chart can be found here:\nM. R. Luo, “The new preferred memory color ( PMC ) chart,” Color Research \u0026amp; Application, p. col.22940, May 2024, doi: 10.1002/col.22940.\nA Simple Example This is a colour chart photographed using a camera\u0026rsquo;s auto white balance and auto-metering. The lighting condition was a normal white LED, not a standard illuminant simulator or a full-spectrum LED.\nCalculating the colour difference directly using the method described above yields an average $\\Delta E_{2000}$ of 6.5. Almost all lightness values are too high, meaning the image is too bright when using the screen\u0026rsquo;s white point as the reference. After multiplying the XYZ values by 0.76, the minimum colour difference of 3.8 is obtained. This operation darkens the image without changing the screen brightness, which is equivalent to lowering the backlight brightness without adjusting the screen white point used for the CIELAB calculation.\nThis means that observing the image above with reduced screen brightness is identical to viewing this image without reducing screen brightness, but the calculated colour difference will be different.\nAnalysis of Greyscale and Lightness Looking at the colour difference for each colour patch, the white patch in the original image has a colour difference of only 0.83, whereas after reducing the brightness, it becomes 6.01. In the greyscale, the lightness difference between the second and second-to-last patches and their references is almost 0. The lightness of the white and black patches is lower than the reference, while the lightness of the two middle patches is higher than the reference. This implies that the ISP may contain a Sigmoid-like tone curve that increases contrast.\n","permalink":"https://jackchou.top/en/posts/colour-checker-for-jpg/","summary":"Reflections on learning to analyse image colour using Imatest and colour charts","title":"Not Just Colour Difference—Evaluating Final Output Images with a Colour Chart"},{"content":"Tools for Reading RAW Data Dcraw The most famous tool for reading RAW data is undoubtedly dcraw. Dcraw can convert RAW files of various encoding formats into TIFF or PPM format.\nUsing the command line dcraw -4 -T -D file_name yields a 16-bit TIFF file that records the direct RAW values, without any processing such as demosaicing, white balancing, or black level subtraction.\nUnfortunately, dcraw\u0026rsquo;s last update was on 1 June 2018. Consequently, it no longer includes additional parameters (like white balance, colour matrices, etc.) for cameras released after that date, and even its core function of extracting RAW data may not work reliably.\nFor instance, with the lossless compressed RAW format introduced by Sony in their fourth-generation cameras (e.g., ILCE-7M4), dcraw reports a \u0026ldquo;cannot decode file\u0026rdquo; error, even though the file extension is still .arw. In contrast, it can still decode the older uncompressed RAW format.\ndcraw.c is the core file of dcraw, consisting of over ten thousand lines of pure C code.\nRawpy/LibRaw Rawpy is a Python wrapper for LibRaw. LibRaw provides a unified interface for accessing RAW data to extract pixel values. It is based on dcraw, having refactored dcraw.c into a more modern and modular library, and has continued to be supported after dcraw ceased development.\nDifferences Between Methods and Brands Sony Uncompressed RAW Test model: ILCE-7CM2:\nThe following methods yield identical results when reading uncompressed RAW files:\nConverting with Dcraw to TIFF and then reading with OpenImageIO Reading directly with Rawpy Converting with Adobe DNG Converter and then reading with Rawpy Converting with Adobe Camera Raw and then reading with Rawpy (same as above, although the dimensions appear different when viewed in ACR) Converting with Adobe DNG Converter, then converting to TIFF with dcraw, and finally reading with OIIO The dimensions read by the above methods are (4688, 7040), which is 33,003,520 pixels, with a value range of 0-16383.\nIn practice, the DNG files converted by Adobe DNG Converter and Adobe Camera Raw are identical, so this will not be detailed further. Reading DNG files with Rawpy and dcraw is also equivalent.\nThe following method produces a different result:\nAfter converting to DNG with Capture One and then reading with Rawpy (or converting to TIFF with dcraw and then reading), the image becomes (4672, 7008) with a value range of 0-65535. The height is reduced by 16 pixels (8 from the top and 8 from the bottom), and the width is reduced by 32 pixels (12 from the left and 20 from the right). Even after cropping, there are differences compared to the other methods. Looking at the ratio (after normalising both to their respective maximums of 65535 or 16383), the maximum ratio is 1.04, the minimum is 0.95, and the average is 1.00. This alignment was performed by eye in Photoshop. Lossless Compressed RAW The principle behind Sony\u0026rsquo;s lossless compressed RAW is to first pad the data with zeros to a multiple of 512, then divide it into blocks, and finally separate it into four sub-images based on the Bayer pattern for differential and Huffman coding.\nFor lossless compressed RAW, the situation is more complex because there is currently no way to convert between uncompressed ARW and lossless compressed ARW files (if two separate shots are taken, even with tethered shooting, the resulting displacement errors would be larger than one pixel). The following are the tested scenarios:\nDcraw does not support lossless compressed RAW (as Sony introduced it after dcraw was no longer being updated). Reading the ARW with Rawpy yields dimensions of (5120, 7168). This is due to the block-based compression (a multiple of 512). Only the top-left (4688, 7040) area contains image data; the rest is filled with zeros (not the black level), and the values range from 0-16383. After converting to DNG with Adobe DNG Converter and then reading with Rawpy, the resulting dimensions are (4686, 7038), which is two pixels smaller in each dimension than the uncompressed RAW. If you crop 2 pixels from the bottom and 2 from the right of the padded image from the previous scenario, the results match perfectly, also with a range of 0-16383. Reading a DNG converted by Capture One with Rawpy also results in dimensions of (4672, 7008) and a range of 0-65535. This is again cropped by 8 pixels top/bottom, and 12 left / 20 right. After cropping the content area of the ARW read by Rawpy, the results are a close match (the average of the differences is even the same, around -5e-6, which requires further testing). Regarding DNGs from Capture One Theoretically, a codec that decodes RAW and encodes to DNG should not introduce complex errors. However, DNGs exported from Capture One not only have different dimensions but also stretch the original 14-bit data to 16-bit, and they do not perfectly match the RAW data read by other methods.\nWith the help of Gemini and DeepSeek, a more detailed analysis was conducted. Regarding the conversion from 14-bit to 16-bit, Capture One appears to perform a left bit shift by two places, which is equivalent to multiplying by 4. After left-shifting the ARW data read by Rawpy and then comparing it with the C1-exported DNG (by division and subtraction), the resulting quotient is 1.000004, and the difference is on the order of e-7. The R and B channels match perfectly; all errors come from the two G channels and are content-dependent. In some images, the maximum error in the G channels can reach up to 10%, though in most cases, it does not exceed 5%.\nBest Practices for Sony RAW In summary, the recommended approach for using Sony RAW files is to shoot in uncompressed RAW and then read them directly with Rawpy. You can use Adobe DNG Converter to conveniently convert uncompressed RAW files into lossless compressed DNGs to reduce file size without any loss. Alternatively, you can read lossless compressed RAW files with Rawpy and crop them, but be aware that converting lossless compressed RAW to DNG will result in the loss of two rows and two columns of pixels. DNG files obtained through other methods have unknown factors and should not be used.\nCanon Test model: 600D, which outputs CR2 files.\nReading with Rawpy and reading after conversion to DNG yield identical results. The image dimensions are (3516, 5344). The 142 pixels on the left and 51 pixels on the top appear to be the optical black area (the part physically masked for black level calibration), which reads out values close to the black level. The remainder is the image.\nDcraw can process files from the 600D. It reads out the part without the optical black area, which matches the cropped data from Rawpy or a DNG conversion.\nThe CR3 output from an R6 Mark II (read with Rawpy or via DNG conversion) is similar. The left 154 pixels and top 96 pixels constitute the optical black area, and there is also a white area of 8 pixels on the right.\nHasselblad The test model is the Hasselblad X2D-100C, which directly outputs RAW files in 3FR format. The sensor model can be confirmed directly from the 3FR file as the Sony IMX461-BQR.\nHasselblad\u0026rsquo;s historical RAW workflow involved two file formats: 3FR and FFF.\nWith the release of the new version of Phocus, the FFF file has been removed from the RAW workflow, and it is now unnecessary to convert to FFF before processing RAW images.\nIn the older workflow, users could convert 3FR to FFF via the Phocus software. There were some optional adjustment settings during conversion, but these did not affect the FFF file\u0026rsquo;s original data (for instance, the results read using rawpy were the same). From the file header, it can be seen that 3FR (49 49 2A 00) follows the little-endian TIFF specification, while FFF (4D 4D 00 2A) is big-endian TIFF.\nThe public specifications for the IMX461-BQR show a total pixel count of 11760×8896 and an effective pixel count of 11664×8750. However, directly parsing the 3FR file using tools like dcraw or rawpy yields an oversized image of 11904×8842. This image contains the following areas:\nImage Content Area: Dimensions are 11664×8750, consistent with the 461\u0026rsquo;s effective pixels. Optical Black: Surrounding the image content area, 48px on the left and right, and 90px on the top. Additional Content: On the outermost periphery, including 76px on the left, 68px on the right, and 2px on the top of non-image data. Analysis shows that the image width including the Optical Black field is 11760px (which matches the sensor\u0026rsquo;s total pixel width), but the height is 8840px, showing a slight discrepancy.\nBoth 3FR and FFF can be converted to the DNG format. The size of the converted DNG file will be cropped to match the effective pixel area. In terms of content, although the image content is aligned, there are some minor numerical differences.\nTherefore, the current best practice for processing Hasselblad 3FR files appears to be directly using libraw to read and extract the effective image area, and utilizing the Optical Black data within the image for precise black level correction.\nFujifilm Test model: X-T5.\nFujifilm\u0026rsquo;s RAW files are rather unique because their colour filter array is not a standard Bayer pattern but an X-Trans pattern, which has a minimum repeating unit of 6x6 pixels. Fortunately, this does not affect our analysis of the raw image data itself.\nFeeding the RAF file directly into Rawpy yields an image with a width of 7872 and a height of 5196. After conversion to DNG, the width is 7728 and the height is 5152. The distribution of the extra 144 pixels in width and 44 pixels in height is as follows:\n12 pixels of image data on the left; 12 pixels of image data and 120 black pixels on the right. 16 pixels of image data and 5 black pixels at the top; 16 pixels of image data and 7 black pixels at the bottom. The pixel values in the overlapping areas are identical.\nAdditionally, Rawpy reads the raw pattern from the RAF file incorrectly, whereas the raw pattern in the DNG is correct.\nNikon To be continued\nI\u0026rsquo;ve found that the return on investment for this research is extremely low, and there is still much to learn. I will pause after a preliminary investigation of Nikon files. For practical use, the recommendation is to convert files to DNG using Adobe DNG Converter and then read them with Rawpy to directly access the image content.\n","permalink":"https://jackchou.top/en/posts/how-to-read-raw/","summary":"Running fast doesn\u0026rsquo;t guarantee a win; avoiding pitfalls is true success.","title":"How to Correctly Read RAW Files"},{"content":"Brighter, and Brighter Still If I recall correctly, from the Realme GT 5 Pro onwards, the peak brightness of mobile phone screens has become increasingly \u0026ldquo;exaggerated\u0026rdquo;.\nClass-leading screen brightness, breaking new industry records. New luminescent materials paired with aggressive tuning algorithms fear no dazzling sunlight, faithfully restoring light and dark. — Realme GT 5 Pro\nIts peak brightness reaches an astonishing 4500 nits, but this is the \u0026ldquo;local peak activated brightness\u0026rdquo;, achievable only under very stringent conditions (HDR content, strong light activation, a small window area), and has almost no practical value. The unfortunate part is that this high brightness, despite its lack of practical value, has become a key marketing focus and a crucial point of competition among manufacturers. Some product specification pages might even list only this brightness figure (the Realme Neo 7, with its 6000-nit peak brightness, is an example where other brightness specs might be omitted).\nSubsequently, the brightness war extended to full-screen activated and full-screen manual brightness. For instance, OnePlus and Xiaomi both provide an additional \u0026ldquo;sunlight mode\u0026rdquo;, allowing users to set the manual brightness even higher. The marketing messages from various companies have unanimously referred to this high brightness as a \u0026ldquo;sunlight display\u0026rdquo;, positing that its most practical application on a mobile phone is to enhance readability in bright environments.\nBrightness vs. Illuminance The readability of a screen in high ambient light is very complex. It is related to the displayed content and the electro-optical transfer function, and is a subjective, psychophysical quantity. One of the simplest ways to quantify this is through contrast: the ratio of the brightness when the screen is displaying white to its brightness when displaying black. Here, \u0026lsquo;brightness\u0026rsquo; refers to the combined luminance, which includes the ambient light reflected by the screen.\n$$ \\text{luminance}=\\frac{\\text{illuminance}*\\text{reflectance}}{\\pi}+\\text{display luminance} $$In the formula above, reflectance is the SCI (Specular Component Included) reflectance. Assuming the screen is under an infinitely large and uniform dome of light (such as outdoors), the luminance of the reflected light can be estimated by dividing by $\\pi$.\nAs illuminance increases, the contrast ratio drops rapidly (note that the y-axis in the graph below is on a logarithmic scale), leading to poor outdoor screen readability. This is perceived as the screen being difficult to see, or not appearing sufficiently \u0026lsquo;clear\u0026rsquo; or \u0026rsquo;translucent\u0026rsquo;.\nThe typical illuminance on a sunny day outdoors is approximately 50,000 lx. The reflectance of uncoated glass, including the specular component, is about 5%. Under these conditions, the contrast ratio is only around 2:1, and increasing the screen\u0026rsquo;s brightness has a limited effect.\nAn Alternative Approach: Reducing Screen Reflectance As the calculation above shows, increasing the contrast ratio is not solely achieved by increasing brightness; it can also be done by reducing the screen\u0026rsquo;s reflectance, which is possible through coatings. In fact, the iPad has featured an anti-reflective coating for a long time, and it is often highlighted in various comparison tests. Its reflection pattern presents as a dim, purplish-red colour. The iPhone, however, does not have this (perhaps due to durability concerns or the difficulty of applying coatings to Ceramic Shield glass). Samsung has already implemented this on its phones, and vivo made an attempt with the X100 Ultra by offering users a screen protector with an anti-reflective coating.\nUsing this technology, the screen\u0026rsquo;s reflectance can be lowered to under 2% (according to tests by DxO). Reducing screen reflectance is a relatively efficient method for improving the contrast ratio, with an effect equivalent to increasing the screen brightness to over 4000 nits.\nIt is worth noting that on modern mobile phones, almost all sensors are located beneath the screen or a screen protector. Vivo mentioned that because of the lower reflectance, phones with an anti-reflective film applied require a special firmware update to recalibrate these sensors.\nSome Findings and Speculations The reflectance including the specular component, when measured with an integrating sphere, yields results similar to those from DxO. I happened to have a brand-new iPhone 16 Pro, and its reflectance was significantly higher than that of a well-used Find X6 Pro without a screen protector. I speculate this is due to the properties of the microcrystalline glass (i.e., Ceramic Shield).\nSome phones also come with a matte surface from the factory, such as those with \u0026lsquo;paper-like\u0026rsquo; displays. Their diffuse reflectance (specular component excluded) is naturally much higher, whereas their total reflectance (specular component included) is comparable to that of standard phones.\nFurthermore, all the measurements above were conducted after wiping the screen completely clean. If the screen is smudged with fingerprints or grease, the reflectance will increase significantly, in particular introducing undesirable textures and anisotropic stray light. The display quality of demonstration phones in retail stores, which have been handled by many people, or of phones belonging to users who are not in the habit of cleaning their screens, will be noticeably degraded.\nA well-worn and heavily-rubbed iPad Pro 2021, which is equipped with an anti-reflective coating, has a reflectance of approximately 2%. However, below 400 nm, its reflectance soars to around 20%. Compared to the Find X6 Pro, its reflection is darker and more blueish.\nThe iPhone 17 and the Future In my previous tests, I found that a brand new iPhone 16 Pro does not have an anti-reflective coating, with its reflectivity being very similar to that of a standard glass surface. For the iPhone 17 series, however, Apple has specifically mentioned:\n6.3-inch Super Retina XDR display — our best ever. 3000 nits peak brightness. ProMotion up to 120Hz. 33% fewer reflections.\nJudging from the hands on pictures and videos, the iPhone 17 series is also equipped with a new type of anti-reflective coating. Meanwhile, IQOO has mentioned that the IQOO 15 will ship with an AR anti-reflective screen protector pre-applied.\nAt the same time, an increasing number of monitors and TVs are adopting this technology. For instance, Hisense\u0026rsquo;s \u0026ldquo;Anti-Reflection Pro\u0026rdquo; (which, translated directly from the Chinese marketing, would be \u0026ldquo;Obsidian\u0026rdquo; — a name I find excellent) gets its name partly from its deep black appearance when turned off, an effect achieved through anti-reflective technology. As the supply chain technology matures, anti-reflective coatings should soon benefit a wider range of products and users.\n","permalink":"https://jackchou.top/en/posts/display-reflectance/","summary":"Potentially a more effective improvement than class-leading screen brightness and aggressive tuning algorithms.","title":"Reflectance: A Second Path to Readability in Bright Environments"},{"content":" Viewing Notice: The abstract and chuunibyou sections were written by DeepSeek R1, which has even more outrageous content.\n\u0026ldquo;Introduction to ISP Forging: Crafting Your \u0026lsquo;Genshin Vision\u0026rsquo; RAW Processor from Scratch\u0026rdquo; \u0026ldquo;Doctor, You Won\u0026rsquo;t Even Call Me an ISP: The Three Primaries and the Foolish Hero\u0026rsquo;s Set\u0026rdquo; \u0026ldquo;Pixel Engineer Training Day 1: How to Make Wild RAW RGB Bow Down to Humanity\u0026rsquo;s Colour Tyranny\u0026rdquo; §0. Prologue: The Hero\u0026rsquo;s Awakening—Finding an \u0026ldquo;ISP Beginner\u0026rsquo;s Guide\u0026rdquo; in the Pixel Wasteland The ISP (Image Signal Processor) is responsible for converting the RAW image output by the sensor into an image that can be displayed on the screen. This typically involves various colour space conversions, processing, and mapping.\nThis series will start from the most basic ISP, gradually adding modules to address encountered issues and improve image quality.\nNext, we will implement the most fundamental two-step ISP to acquire our initial weapon.\n§1. The Ideal RAW Image The ideal starting point is a three-channel image with values ranging from 0 to 1, where 0 represents no input and 1 represents the sensor’s saturation value. However, the RAW image output by the camera contains black level compensation, has not been demosaiced, and is encoded in the camera manufacturer\u0026rsquo;s proprietary format.\nFortunately, there are many open-source tools available to help us with this pre-processing, such as dcraw and libraw. Rawpy is a Python wrapper around LibRaw, and with the following code, we can read a RAW image as an ideal numpy array:\ndef read_raw_image(path): with rawpy.imread(path) as raw: rgb = raw.postprocess( gamma=(1, 1), output_bps=16, use_auto_wb=False, use_camera_wb=False, user_wb=[1, 1, 1, 1], output_color=rawpy.ColorSpace.raw, no_auto_bright=True, half_size=True, ) rgb = rgb / 65535.0 return rgb Here, rgb is a three-dimensional numpy array with the shape (H, W, 3), where H and W are the height and width of the image respectively. This represents the pre-processed ideal RAW image.\nIf you encode this directly as an image, what you get is the \u0026ldquo;original image\u0026rdquo;.\n§2. From RAW RGB to XYZ See the prequel: Colour Space Conversion: RAW and XYZ.\nThe CCM (Colour Correction Matrix) is a 3x3 matrix used to convert RAW RGB to XYZ.\nccm = np.array( [[1.297, 0.558, 0.0596], [0.0793, 0.569, -0.1675], [0.1033, -0.1577, 1.2465]] ) cameraRGB_2D = cameraRGB.reshape(-1, 3) XYZ_2D = np.dot(cameraRGB_2D, ccm) XYZ = XYZ_2D.reshape(cameraRGB.shape) The two reshape steps here are for performing matrix multiplication. If further operations are needed later, you can temporarily retain the vector form.\nAt this point, the resulting XYZ is an estimate of the tristimulus values under the shooting environment. Thus, we have converted from the camera’s unique spectral response to a unified colour space. This operation does not take into account the absolute values of the tristimulus values; if you need to adjust the overall luminance, operating on XYZ is quite reasonable, such as multiplying by a factor to simulate exposure compensation.\n§3. From XYZ to sRGB See the prequel: Colour Space Conversion: XYZ and sRGB.\nM_XYZ2sRGB = np.array( [[3.2406, -1.5372, -0.4986], [-0.9689, 1.8758, 0.0415], [0.0557, -0.2040, 1.0570]] ) sRGB_linear_2D = np.dot(XYZ_2D, M_XYZ2sRGB.T) sRGB_linear = sRGB_linear_2D.reshape(cameraRGB.shape) sRGB_linear_clipped = np.clip(sRGB_linear, 0, 1) sRGB = np.where( sRGB_linear_clipped \u0026lt;= 0.0031308, 12.92 * sRGB_linear_clipped, 1.055 * np.power(sRGB_linear_clipped, 1 / 2.4) - 0.055, ) These operations include colour space conversion and the OETF (opto-electronic transfer function), converting XYZ to sRGB space. sRGB is a three-channel image in the 0–1 range and can be displayed directly on a screen. Note that before applying the OETF, the linear space sRGB_linear must be constrained to the 0–1 range. This step effectively clips any out-of-gamut colours directly, ensuring absolute colour reproduction within the gamut—a simplest form of gamut mapping.\n§4. Initial Weapon Forging Report At this point, we have completed the most basic ISP. Although simple, the key point is that every step is supported by colour science theory.\nTo demonstrate how fragile this ISP is, let’s light this lamp and encounter the first problem.\nHighlight overflow: When the sensor\u0026rsquo;s saturation value is exceeded, the sensor records it as (1, 1, 1). After processing with the initial version of the ISP, such a pixel becomes (1, 0.8, 1) and appears pink. A simple and crude solution is to detect saturated pixels in cameraRGB; if any are found, simply display them as white.\nIn the future, we will supplement the omitted modules from this initial version, resolve the various issues encountered, and gradually improve image quality.\n","permalink":"https://jackchou.top/en/posts/isp-01/","summary":"Decrypting the RAW Colour Cipherbook: CCM Parameter Alchemy and Constructing the sRGB Portal Methodology","title":"Reincarnated into the ISP Isekai Day 0: Lost in the RAW Forest, I Summon RGB with a 3x3 Magic Array"},{"content":"Recap: XYZ Tristimulus Values XYZ is the most important colour space in colour science. Colour appearance models, uniform colour spaces, and various RGB spaces are all derived from XYZ. XYZ values are obtained by multiplying spectral radiance by the colour matching functions, as follows:\n$$\rX = k \\int_{\\lambda} P(\\lambda) \\bar{x}(\\lambda) d\\lambda\r$$Where \\(P(\\lambda)\\) is the spectral radiance, \\(\\bar{x}(\\lambda)\\) is the standard observer\u0026rsquo;s colour matching function, and \\(k\\) is a normalisation coefficient.\nSpectral Sensitivity Function Most cameras use colour filters to split light into three channels. The transmittance of the filters and the wavelength response of the photodiodes jointly determine the sensitivity of the three channels to different wavelengths of light. This wavelength-dependent function is called the Spectral Sensitivity Function (SSF).\nFollowing the calculation process for XYZ, the pixel value output by an ideal sensor can be calculated by the following formula:\n$$\rR = k \\int_{\\lambda} P(\\lambda) \\bar{r}(\\lambda) d\\lambda\r$$Where \\(\\bar{r}(\\lambda)\\) is the spectral sensitivity function of the red channel, k is a coefficient, and \\(P(\\lambda)\\) can be the spectral irradiance on the sensor.\nThe amount of light received by the sensor is determined not only by the light intensity but also by the camera\u0026rsquo;s shutter speed and the lens aperture size. For convenience, these are all represented by k.\nLinear Transformation? Although XYZ and RAW RGB are both linear spaces, this does not mean they can be directly transformed linearly by a single matrix. However, an approximate transformation is possible, which is the role of the CCM (Colour Correction Matrix) in an ISP.\nFrom the formulae above, it is easy to see that the condition for being able to transform RAW RGB to XYZ using a 3x3 matrix is:\n$$ \\begin{aligned}\r\\bar{x}(\\lambda) = a_{11} \\bar{r}(\\lambda) + a_{12} \\bar{g}(\\lambda) + a_{13} \\bar{b}(\\lambda) \\\\\r\\bar{y}(\\lambda) = a_{21} \\bar{r}(\\lambda) + a_{22} \\bar{g}(\\lambda) + a_{23} \\bar{b}(\\lambda) \\\\\r\\bar{z}(\\lambda) = a_{31} \\bar{r}(\\lambda) + a_{32} \\bar{g}(\\lambda) + a_{33} \\bar{b}(\\lambda)\r\\end{aligned}\r$$If the above conditions cannot be met, the least squares method can be used to solve for an optimal CCM matrix.\nReference Implementation Below is the SSF of a camera (from an early experiment, and should not be used for practical purposes), because I\u0026rsquo;m not sure how accurate the measurements are.\nSolving via the least squares method for the 3x3 matrix that linearly combines this SSF to approximate the CIE 1931 XYZ CMF yields:\n$$\r\\begin{bmatrix}\r1.297 \u0026 0.0793 \u0026 0.1033 \\\\\r0.558 \u0026 0.569 \u0026 -0.1577 \\\\\r0.0596 \u0026 -0.1675 \u0026 1.2465\r\\end{bmatrix}\r$$Using this matrix to convert the SSF into an approximate XYZ (represented as Estimated XYZ):\nSome Issues and To Be Continued The above content is based on assumptions such as: RAW is linear, and the camera\u0026rsquo;s spectral response range is 380-780nm. Methods for improving the CCM. Methods for creating a CCM without using an SSF. Method for Evaluating CCM Quality The purpose of a CCM is to convert RAW RGB to XYZ. A description I prefer is to \u0026lsquo;predict\u0026rsquo; XYZ from RAW data. The essence of evaluating CCM quality is to see how accurate the prediction is. For XYZ, a colour difference formula can be used to calculate the difference between the predicted and true values. The colour samples used typically include neutral colours, saturated colours, and colours of common objects, such as skin tones, sky, and foliage. These \u0026lsquo;important\u0026rsquo; colours are the colours found on colour charts. The most famous colour chart is the X-Rite ColorChecker Classic, which contains 24 colour patches, but X-Rite has not published its reflectance data, making it unsuitable for theoretical calculations. A better choice is the PMCC (Preferred Memory Colour Chart), which includes saturated colours, neutral colours, and 18 classic memory colours, and provides spectral reflectance data.\nM. R. Luo, “The new preferred memory colour (PMC) chart,” Colour Research \u0026amp; Application, p. col.22940, May 2024, doi: 10.1002/col.22940.\nCalculating the colour difference between the predicted XYZ and the true XYZ for these 30 colour samples allows for the evaluation of the CCM\u0026rsquo;s quality. By changing the light source (common choices include D65, A, and CWF or TL84), one can evaluate the CCM\u0026rsquo;s performance under different light sources.\n","permalink":"https://jackchou.top/en/posts/ccm-02-raw-xyz/","summary":"Converting RAW to XYZ","title":"Colour Space Conversion: RAW to XYZ"},{"content":"The RGB Colour Space The RGB colour space is arguably the most common colour space. It is an additive system that describes colours through the mixing of three primaries. The most common RGB colour space is sRGB (standard Red Green Blue), jointly introduced by HP and Microsoft in 1996, which has gradually gained widespread support across various industries and software.\nAn RGB colour space, as exemplified by sRGB, is primarily defined by three components: the colorimetric definition of its RGB primaries and white point, a non-linear transfer function, and the viewing environment. For instance, sRGB defines the xy colour coordinates for its red, green, and blue primaries as (0.64, 0.33), (0.30, 0.60), and (0.15, 0.06) respectively, with the white point coordinates at (0.3127, 0.3290); it employs a transfer function that approximates a gamma of 2.2; and it specifies the viewing environment\u0026rsquo;s illuminance, ambient reflectance, and even glare.\nFor details, please see the definition of sRGB.\nBased on the first two parts of the definition—the three primaries and the white point, and the transfer function—the conversion sequence can be established as RGB -\u0026gt; Linear RGB -\u0026gt; XYZ. The conversion from linear RGB to XYZ is achieved using a matrix. This matrix effectively describes the positions of the three primaries within CIEXYZ and the proportions in which they are mixed to produce the white point.\nBuilding the Conversion Matrix Key: Determining the \u0026ldquo;Proportions\u0026rdquo; of the Primaries The definition only provides the \u0026ldquo;colour coordinates\u0026rdquo; for the primaries, and colour coordinates contain no luminance information. Imagine we have red, green, and blue LED light sources. By adjusting their brightness, we can mix them to create different colours. Our goal now is to find the proportions that, when mixed, produce the white point colour coordinates defined in the specification.\nA straightforward method is as follows (provided by Claude 3.7 Sonnet):\nFirst, preset the luminance of the three primaries and the white point to 1 (Y=1), and calculate their corresponding CIEXYZ values (by converting from xyY to XYZ).\nSince CIEXYZ is linear, solve for the proportions in which the three primaries (each with a luminance of 1) must be combined to produce the white point. This gives the actual XYZ values for the primaries.\nFollowing the definition of matrix multiplication, describe the linear combination of the primaries as a matrix to construct the conversion matrix.\nMixing Colour Coordinates The following is a more convoluted process I worked through while thinking about this problem. The method above is sufficiently concise and intuitive, so you can skip the following section and proceed to the part about the transfer function. I\u0026rsquo;m leaving this here purely because I couldn\u0026rsquo;t bring myself to delete it.\nHow do we determine the new colour coordinates after mixing three sets of colour coordinates?\nFirst, convert the xy colour coordinates to CIEXYZ tristimulus values. The Z value is not important for the subsequent mixing, so its transformation is omitted here. Y is the proportion we are looking for.\n$$\r\\begin{align*}\r\u0026X+Y+Z=\\frac{Y}{y} \\\\\r\u0026X=x(X+Y+Z)=\\frac{xY}{y} \\\\\r\\end{align*}\r$$We use subscripts to denote the three components, while variables without subscripts represent the mixed colour. CIEXYZ is linear, so values can be summed. Here, we calculate the X and Y values of the mixed colour, as well as the sum of X, Y, and Z (used for later calculation of the colour coordinates).\n$$\r\\begin{align*}\r\u0026 Y=Y_{1}+Y_{2}+Y_{3} \\\\\r\u0026 X=\\frac{x_{1}Y_{1}}{y_{1}}+\\frac{x_{2}Y_{2}}{y_{2}}+\\frac{x_{3}Y_{3}}{y_{3}} \\\\\r\u0026 X+Y+Z=\\frac{Y_{1}}{y_{1}}+\\frac{Y_{2}}{y_{2}}+\\frac{Y_{3}}{y_{3}} \\\\\r\\end{align*}\r$$Let $X+Y+Z=S$. After summing the tristimulus values, we convert back to xyY colour coordinates. The mixed colour coordinates are:\n$$\r\\begin{align*} \\\\\r\u0026 y=\\frac{Y}{S} \\\\\r\u0026 x=\\frac{X}{S}\r\\end{align*}\r$$That is:\n$$\r\\begin{align*}\r\u0026S=\\frac{Y}{y} \\\\\r\u0026X=xS=\\frac{xY}{y}\r\\end{align*}\r$$When the colour coordinates of the three primaries and the white point are known, the following system of equations can be constructed.\n$$\r\\left\\{\r\\begin{align*}\r\u0026 Y_{1}+Y_{2}+Y_{3}=Y \\\\\r\u0026 \\frac{1}{y_{1}}Y_{1}+\\frac{1}{y_{2}}Y_{2}+\\frac{1}{y_{3}}Y_{3}=\\frac{Y}{y} \\\\\r\u0026 \\frac{x_{1}}{y_{1}}Y_{1}+\\frac{x_{2}}{y_{2}}Y_{2}+\\frac{x_{3}}{y_{3}}Y_{3}=\\frac{xY}{y}\r\\end{align*}\r\\right.\r$$Expressed in matrix form:\n$$\r\\begin{bmatrix}\r1 \u0026 1 \u0026 1 \\\\\r\\frac{1}{y_1} \u0026 \\frac{1}{y_2} \u0026 \\frac{1}{y_3} \\\\\r\\frac{x_1}{y_1} \u0026 \\frac{x_2}{y_2} \u0026 \\frac{x_3}{y_3}\r\\end{bmatrix}\r\\begin{bmatrix}\rY_1 \\\\ Y_2 \\\\ Y_3\r\\end{bmatrix}\r=\r\\begin{bmatrix}\rY \\\\ \\frac{Y}{y} \\\\ \\frac{xY}{y}\r\\end{bmatrix}\r$$Typically, Y is set to 1 (i.e., the white point luminance is normalised). This allows us to solve for the proportions (the luminance Y of each primary) in which the three primaries must be mixed to obtain the specified white point.\nThe Conversion Matrix After finding the xyY values for the three primaries, we convert them to XYZ. It is easy to see that we have now calculated the corresponding XYZ tristimulus values for the basis vectors in the RGB space.\nTherefore, the conversion matrix from RGB to XYZ is as follows. To convert from XYZ to RGB, one simply needs to find the inverse of this matrix.\n$$\r\\begin{bmatrix}\rX_{r} \u0026 X_{g} \u0026 X_{b} \\\\\rY_{r} \u0026 Y_{g} \u0026 Y_{b} \\\\\rZ_{r} \u0026 Z_{g} \u0026 Z_{b} \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rR \\\\ G \\\\ B\r\\end{bmatrix}\r=\r\\begin{bmatrix}\rX \\\\ Y \\\\ Z\r\\end{bmatrix}\r$$The Non-linear Transfer Function To better match the perceptual characteristics of the human eye, values in an RGB space are typically passed through a non-linear transfer function, converting linear RGB to non-linear RGB. For example, sRGB uses a non-linear transfer function that approximates a gamma of 2.2 and corrects for potential issues near zero during the inverse transformation.\nThe conversion function from linear sRGB to non-linear sRGB is as follows. Although the exponent is 2.4, it is actually closer to $x^{2.2}$.\n$$\r\\begin{align*}\r\u0026 R' = \\begin{cases}\r12.92R \u0026 R \\leq 0.0031308 \\\\\r1.055R^{1/2.4}-0.055 \u0026 R \u003e 0.0031308\r\\end{cases}\r\\end{align*}\r$$Thoughts How can one determine if a given set of tristimulus values lies within a specific RGB colour space?\nHow to determine if given XYZ tristimulus values fall within a colour gamut.\nThe gamut is defined by four CIE xy coordinates representing red, green, blue, and white (assume white point luminance has been normalised to 1.0).\nPlease provide the Python code.\nThe first model to answer this question correctly without an internet connection was Deepseek R1 (referring to the first generation). It required an output of about ten thousand tokens, and most platforms deploying R1 do not support such long outputs.\nModels that subsequently answered correctly: Gemini 2.5 Pro, OpenAI o3 Pro, Deepseek V3-0324, Claude 3.7 Sonnet, Grok 4, Kimi K2.\nModels that failed: GPT 4.1, Qwen, Claude Sonnet 4 (while 3.7 could reason its way to the correct answer, 4 could not, even with reasoning).\nThe key issue was whether luminance was correctly considered. The correct approach is to construct the conversion matrix, transform the XYZ values to RGB, and then check if any of the resulting RGB components are less than 0 or greater than 1. A typical mistake is to convert the XYZ values to xy colour coordinates and check if the point lies within the triangle formed by the three primaries.\n","permalink":"https://jackchou.top/en/posts/ccm-01-rgb-xyz/","summary":"Building a matrix to implement conversions between XYZ and RGB spaces defined on XYZ.","title":"Colour Space Conversion: RGB to XYZ"},{"content":"Spectral Separation Whether it is the XYZ tristimulus values or other colour attributes, they are all derived from the spectrum. Directly measuring the Spectral Power Distribution (SPD) is the most fundamental method of colour measurement. From the spectrum, any required colour attribute can be calculated, offering maximum flexibility.\nThe core of spectral measurement is spectral separation. Newton used a prism to spatially separate light of different wavelengths, forming a rainbow, which is a method of spectral separation. Principles like prisms and diffraction gratings can both achieve this, with differences in spectral resolution, spectral bandwidth, and spectral range.\nAfter spectral separation, photodetectors such as photodiodes, CCDs, or CMOS sensors are used to capture and process the spatially separated spectrum.\nAdditionally, depending on whether they have a built-in light source and what specific spectral power distribution (i.e., which radiometric quantity) they measure, spectral measurement instruments can be categorised as spectrophotometers, spectroradiometers, and so on.\nPerhaps the most famous spectroradiometer is the Konica Minolta CS-2000, which can measure radiance at 1nm intervals from 380nm to 780nm.\nIntegration Sometimes, we only need colour attributes rather than the full spectrum, such as the common CIE XYZ tristimulus values. According to the definition of XYZ:\n$$ X = K\\int_{380nm}^{780nm} P(\\lambda) \\cdot \\bar{x}(\\lambda) d\\lambda $$ where $P(\\lambda)$ is the spectral power distribution, $\\bar{x}(\\lambda)$ is the colour matching function, and $K$ is a normalisation coefficient.\nIf there were a colour filter whose transmission spectrum, when multiplied by the detector\u0026rsquo;s spectral sensitivity function, resulted in an overall spectral sensitivity similar to $\\bar{x}(\\lambda)$, then the value of $X$ could be obtained by directly reading the output. This method, which is akin to physically integrating the spectrum, is called integration measurement and can directly yield colour attributes.\nAttributes that can be measured through integration include: tristimulus values (by integrating with the colour matching functions) and luminance (by integrating with the spectral luminous efficiency function). In short, any attribute obtained by integrating over a spectral sensitivity function can be measured using integration. The accuracy depends on how well the instrument\u0026rsquo;s spectral characteristics match the target function.\nA colorimeter, such as the Datacolor SpyderX, uses integration measurement.\nThe problem with integration measurement is that the colour matching functions are not constant. For example, they have evolved from the original CIE 1931 colour matching functions to the later CIE 1964, and then to the CIE 2006 versions. Integration measurement methods are not backwards compatible and lack the ability to calculate other spectrum-related metrics. For instance, two light sources measured by an integration device to have identical tristimulus values might have different blue light energy distributions, thus affecting circadian rhythms differently. Furthermore, poorly matched instruments can lead to metamerism, where the output error changes with the spectrum. To some extent, a camera can also be considered a device for the integration measurement of colour, and its spectral sensitivity function is called the SSF (Spectral Sensitivity Function).\nOther Aspects of Measurement Beyond basic spectral measurement, one must also consider the reflectance, gloss, and texture of real objects at different angles. In industrial applications, there is also a need to measure colours across a two-dimensional plane or even in three-dimensional space at once. Specialised instruments and methods exist for these purposes, though they are generally quite expensive.\nFor example, the EZContrast series from Eldim can measure reflectance or radiance at multiple angles simultaneously, while imaging photometers produced by Topcon can measure the spectrum and colour of multiple points on a two-dimensional plane in a single capture.\nA camera, being an imaging device that performs three types of spectral integration, serves as an excellent low-cost tool for two-dimensional colour measurement. However, special attention must be paid to metamerism and device characterisation.\n","permalink":"https://jackchou.top/en/posts/pcrdi06/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep. 6","title":"Principles of Colour Reproduction in Digital Images Notes: Colour Measurement"},{"content":"Advanced Colorimetry In previous content, we have studied colorimetry, which can quantify and calculate colour perception. According to colorimetry, under specific conditions, two colour stimuli with the same CIE XYZ tristimulus values can match. These conditions include the background, light source, material, etc. In real life, these conditions are not always met, causing two colour stimuli that are identical in colorimetry to fail to match. To address this, corrective methods such as chromatic adaptation transforms have been proposed.\nWith the increasingly widespread application of Cross Media Colour Reproduction, there is a need for models that can take various viewing conditions as parameters to describe colour stimuli more precisely. The colour stimulus perceived under different viewing conditions is called colour appearance. The study and modelling of colour appearance are known as advanced colorimetry, and the resulting models are called colour appearance models (CAM).\nColour Appearance Advanced colorimetry uses colour appearance attributes to describe perceived colour stimuli in more detail. Colour appearance attributes are part of colour science terminology. All standard vocabulary related to colour science can be found at https://cie.co.at/e-ilv. From here on, it is important to use the Chinese and English terms for colour appearance attributes carefully. The index is from CIE S 017:2020 ILV. For precise explanations of colour appearance attributes, it is recommended to consult the descriptions in the international standard.\nCommon colour appearance attributes include: Brightness, Lightness, Colourfulness, Chroma, Saturation, and Hue.\nAdditionally, many new colour appearance attributes are being researched and proposed.\nRelated colour: 17-22-047, colour perceived to belong to an area seen in relation to other colours. This describes a colour that has a relationship with other surrounding colours. The best example is \u0026ldquo;grey\u0026rdquo;, which is always a related colour because it can only be perceived in comparison to brighter colours.\nBrightness: 17-22-059, attribute of a visual perception according to which an area appears to emit, transmit or reflect, more or less light. An absolute attribute describing the degree of perceived light.\nLightness (of a related colour): 17-22-063, brightness of an area judged relative to the brightness of a similarly illuminated area that appears to be white or highly transmitting. An attribute that only related colours have, describing the relationship of an area\u0026rsquo;s brightness relative to the brightness of the brightest or white area in the surroundings.\nSome books state that lightness is the perception of relative luminance compared to a surrounding white point or the brightest area; it should be relative brightness. It also does not mean that Lightness = Brightness / White Point Brightness, as they are all non-linear quantities.\nColourfulness: 17-22-072, attribute of a visual perception according to which the perceived colour of an area appears to be more or less chromatic. An absolute attribute of colour. A brighter colour stimulus may have higher colourfulness.\nChroma: 17-22-074, colourfulness of an area judged as a proportion of the brightness of a similarly illuminated area that appears grey, white or highly transmitting. Chroma is the relationship between the colourfulness of a colour stimulus and the brightness of a neutral colour under similar illumination.\nRegarding the four quantities above, due to colour constancy and cognitive mechanisms, the visual system automatically perceives the red colours in the image as belonging to the same object. In comparison with their respective backgrounds or lighting environments, they have the same lightness and chroma (it could also be said that lightness and chroma are the perception of a sample\u0026rsquo;s \u0026ldquo;reflectance\u0026rdquo; after the visual system has discounted the influence of the light source). However, the absolute attributes of brightness and colourfulness for the top and bottom colours are different.\nSaturation: 17-22-073, colourfulness of an area judged in proportion to its brightness. It is the relationship between a colour\u0026rsquo;s colourfulness and its brightness. The image below intuitively shows the difference between colourfulness, brightness, and saturation.\nIn CIECAM16, saturation is calculated from the ratio of colourfulness to brightness.\n$$ s=100*\\left( \\frac{M}{Q}\\right)^{0.5} $$\nThere are also many everyday words to express concepts related to \u0026ldquo;colour\u0026rdquo;, such as vibrant, saturated, chroma, or terms like Vividness and Brilliance. They all have different meanings in colour science.\nColour Appearance Models Colour appearance models can predict the colour appearance attributes of a colour stimulus under different viewing conditions. These models typically include chromatic adaptation and the prediction of some colour appearance phenomena. Therefore, the inputs to a colour appearance model are a colour stimulus and the viewing conditions, and the outputs are various colour appearance attributes.\nInputs and Outputs Almost all colour appearance models use XYZ tristimulus values as input, ensuring good compatibility with existing colour science. The viewing conditions, however, need to be carefully defined and usually include the background, surround, and adaptation field, among others. Additionally, a white point must be input to calculate chromatic adaptation and relative colour appearance attributes. White points are divided into a reference white point and an adapted white point. The reference white point, also known as the adopted white point, is a computationally defined white point set by a human, such as using the white displayed by a display as the reference white point in display-related calculations. The adapted white point is the white point within the visual system, i.e., what the visual system perceives as white.\nThe adaptation field white point refers to the white point in the adaptation field, for example, the colour stimulus of a white area in the environment. When one is fully adapted to a scene, the adaptation field white point and the adapted white point are the same.\nThe most fundamental output colour appearance attributes are: Lightness, Chroma, and Hue.\nBasic Structure The basic structure of a colour appearance model is:\nConvert the colour stimulus (tristimulus values) into cone responses. Perform a chromatic adaptation transform on the cone responses to obtain adapted cone responses. Derive intermediate signals (higher-level signals) based on visual theory foundations (opponent-colour theory, non-linear compression). Combine with viewing conditions to calculate the colour appearance attributes. That is: a chromatic adaptation transform (but without using the inverse matrix to return to tristimulus values) + intermediate processing + calculation of colour appearance attributes.\nFollowing this basic structure, CIELAB can also be considered a simple colour appearance model.\n$$ \\begin{align*} L^* \u0026= 116 f\\left(\\frac{Y}{Y_n}\\right) - 16 \\\\ a^* \u0026= 500 \\left[f\\left(\\frac{X}{X_n}\\right) - f\\left(\\frac{Y}{Y_n}\\right)\\right] \\\\ b^* \u0026= 200 \\left[f\\left(\\frac{Y}{Y_n}\\right) - f\\left(\\frac{Z}{Z_n}\\right)\\right] \\end{align*} $$Here, $f(t)$ is a non-linear function used to simulate the non-linear compression of the visual system. Hue and chroma can be calculated from a* and b*. Additionally, $Y/Y_n$ can be seen as a type of chromatic adaptation transform, although from a chromatic adaptation perspective, it incorrectly applies a diagonal matrix (the von Kries hypothesis) to the tristimulus values rather than to the cone responses, resulting in its poor chromatic adaptation performance.\nDue to chromatic adaptation, CIELAB-based colour difference formulae may not handle colour differences under various light sources well. This can be addressed by introducing more advanced chromatic adaptation transforms or by directly using a complete colour appearance model. You can experience related content here.\nCommon Colour Appearance Models CIECAM16: The current CIE recommended colour appearance model. CIECAM02: A simplified and improved version of CIECAM97s. CIECAM97s (the simple form of the CIE Colour Appearance Model 1997) Hunt model: The most complete and complex colour appearance model, developed since 1952. It has greatly influenced the CIECAM series and even considers the influence of rods. To be continued Some related websites:\nhttp://www.huevaluechroma.com/index.php\n","permalink":"https://jackchou.top/en/posts/pcrdi05/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep. 5","title":"Principles of Colour Reproduction in Digital Images Notes: Colour Appearance"},{"content":" For more information on chromatic adaptation and chromatic adaptation models, please refer to section 1.2.1 of this paper:\nQ. Zhai, ‘Chromatic Adaptation to Illumination and Colour Quality’, PhD dissertation, Zhejiang University, 2018.\nColour Constancy The colour of an object changes under different lighting conditions and viewing environments, but the human visual system can maintain a stable perception of the object\u0026rsquo;s colour to a certain extent. This phenomenon is known as colour constancy. The process of maintaining this relative stability is called chromatic adaptation.\nWhen the lighting environment changes, chromatic adaptation takes some time to complete. The figure shows the experimental results from Fairchild and Reniff (1995), illustrating the relationship between the proportion of steady-state adaptation and time for three observers, switching from illuminant A to D65.\nM. D. Fairchild and L. Reniff, ‘Time course of chromatic adaptation for color-appearance judgments’, J. Opt. Soc. Am. A, vol. 12, no. 5, p. 824, May 1995, doi: 10.1364/JOSAA.12.000824.\nThe formation mechanism of chromatic adaptation can be broadly divided into two parts: sensory and cognitive.\nThe sensory mechanism suggests that the three types of cone cells on the retina automatically and independently adjust their gain according to the intensity of light. When the response of a certain type of cone cell increases, its gain is reduced, and the gain adjustments of the three types of cone cells are independent. The fundamental hypothesis proposed by von Kries in 1902 posits that the fatigue or adaptation of each component of the visual organ is independent of the others (the concept of cone cells did not exist at the time). The von Kries hypothesis is the foundation of all chromatic adaptation models.\n\u0026ldquo;This can be conceived in the sense that the individual components present in the organ of vision are completely independent of one another and each is fatigued or adapted exclusively according to its own function.\u0026rdquo;\nThe cognitive mechanism is more complex and suggests that a person\u0026rsquo;s perception of an object\u0026rsquo;s colour is also influenced by the object itself. For example, grass is green, apples are red, and the sky is blue; vision can maintain a stable perception of the colours of these objects under various lighting conditions. The formation of this cognitive mechanism may be due to the accumulation of experience with object colours over a lifetime. Chromatic adaptation resulting from cognitive mechanisms is often incomplete.\nA typical example of a cognitive mechanism is \u0026lsquo;discounting the illuminant\u0026rsquo;. This refers to the ability of an observer to judge an object\u0026rsquo;s colour based on its inherent properties (reflectance) rather than the light source. For instance, coal is black during the day and snow is white at night, but in reality, the luminance of coal during the day is higher. The visual system perceives primarily the reflectance of the coal and snow. Discounting the illuminant is very important in cross-media colour reproduction. Cross-media colour reproduction refers to displaying colours using different media, such as a self-luminous display and colours printed on paper. The self-luminous display is itself a light source, so there is no phenomenon of discounting the illuminant, whereas for printed colours, the observer can, to some extent, ignore the influence of the illuminating light source.\nApple\u0026rsquo;s True Tone technology, introduced on the iPhone 8 and iPhone X, adjusts the display\u0026rsquo;s colour based on changes in ambient lighting. This simulates the phenomenon of discounting the illuminant, making the display appear like a printed paper under the same light source, which enhances viewing comfort to some extent.\nChromatic Adaptation Transform A chromatic adaptation transform establishes a relationship between \u0026lsquo;corresponding colours\u0026rsquo;. Corresponding colours are two colours that match under different viewing conditions. Imagine an observer with perfect colour constancy; if the light source in a scene is changed, the colour they perceive remains the same (i.e., it always matches). In this case, the colours of the same object under the different light sources form a pair of corresponding colours. Colours are typically represented by their XYZ tristimulus values. If the tristimulus values under the first viewing condition are $X_1, Y_1, Z_1$, and under the second viewing condition are $X_2, Y_2, Z_2$, then these two sets of tristimulus values are corresponding colours for those two viewing conditions.\nA Chromatic Adaptation Transform (CAT) is a model used to predict corresponding colours. The inputs are two viewing conditions (usually represented by the tristimulus values of the scene white point), and a colour under one of the viewing conditions (represented by its tristimulus values). The model predicts the colour under the other viewing condition that forms a corresponding pair (also represented by tristimulus values).\nBuilding a CAT model requires corresponding colour datasets for training and validation. The following are some common experimental methods for creating these datasets:\nHaploscopic matching: An experimental apparatus is designed to separate the visual fields of the left and right eyes, placing them under different viewing conditions. The observer then compares and matches the colour stimuli from both sides. This method cannot be used to study chromatic adaptation caused by cognitive mechanisms. Memory matching: The subject memorises a colour stimulus under one viewing condition and then matches it under another viewing condition. Magnitude estimation: The subject \u0026lsquo;rates\u0026rsquo; colour stimuli in different viewing environments, for example, by estimating numerical values for lightness, saturation, hue, etc. I have not designed, conducted, or participated in any experiments on chromatic adaptation. From a purely subjective perspective, I believe collecting corresponding colour data is very difficult. Each of the three methods mentioned above has its own drawbacks. For instance, in memory matching, human short-term memory for colour is very limited. In magnitude estimation, having subjects assign scores to a subjective value requires careful experimental design to standardise the rating criteria among subjects.\nBasic Structure of a CAT According to the von Kries hypothesis, chromatic adaptation is independent at the level of the visual organs. The basic structure of a chromatic adaptation transform is as follows:\nTransform the input XYZ values into a space that represents the visual organs. Process each component independently within this space (e.g., by multiplying by its respective gain coefficient). Transform back to the XYZ space to obtain the colour tristimulus values for the other viewing condition. Von Kries himself never provided a specific CAT method, but simple chromatic adaptation models, such as the Ives and Helson models, can be built based on his hypothesis.\nM. H. Brill, ‘The relation between the color of the illuminant and the color of the illuminated object’, Color Research \u0026amp; Application, vol. 20, no. 1, pp. 70–76, Feb. 1995, doi: 10.1002/col.5080200112.\nH. Helson, ‘Some Factors and Implications of Color Constancy*’, J. Opt. Soc. Am., vol. 33, no. 10, p. 555, Oct. 1943, doi: 10.1364/JOSA.33.000555.\nFirst, the light sources or white points of the two viewing conditions and one colour stimulus (\\(\\text{XYZ}_{w1}, \\text{XYZ}_{w2}, \\text{XYZ}_{1}\\)) are transformed into the LMS relative cone response space. This can be done using a 3x3 matrix. Then, each component is multiplied by a gain coefficient, which can be represented as multiplication by a diagonal matrix. Finally, the values are transformed back to the XYZ space by multiplying by the inverse of the first matrix, yielding $\\text{XYZ}_{2}$, sometimes written as $\\text{XYZ}_c$ to denote the corresponding colour.\nThe gain coefficients are the ratio of the cone responses to the light sources or white points of the two viewing conditions, reflecting the hypothesis that \u0026lsquo;a visual organ with a larger stimulus will automatically adjust to reduce its gain\u0026rsquo;. For example:\n$$\rL_2 = \\frac{L_{w2}}{L_{w1}} L_1\r$$Such simple linear models are already capable of predicting the data in corresponding colour datasets quite well.\nImprovements to CATs Some research has proposed using non-linear adjustments or non-independent adjustments with cross-channel interactions in the second step to try to improve chromatic adaptation transforms. For example, Nayatani and Guth used a power function instead of linear gain. However, these methods did not achieve significantly better results.\nChanging the transformation matrix from XYZ tristimulus values to the LMS relative cone space may yield better results. In this case, the space is no longer referred to as LMS, but as RGB. Examples include the HPE transformation matrix used by Fairchild, the BFD transformation matrix by Bradford, and the matrices used in CAT02 and CAT16.\nAdditionally, although non-linear adjustments do not yield better results, improvements can be made to the coefficients used in linear gain. For example, CMC-CAT and CAT02 introduced the concept of a degree of adaptation, D, to control the completeness of the adaptation. D ranges between 0 and 1, where 1 represents complete adaptation and 0 represents no adaptation. In CAT02, D is a value related to the luminance of the adapting field on the input side, and a factor F, representing the ambient surround (bright or dark), is also included.\n$$\rD = F \\cdot \\left[1 - \\frac{1}{3.6} e^{\\frac{-L_A - 42}{92}}\\right]\r$$Here, $L_A$ is the luminance of the adapting field on the input side in cd/m², and $F$ is the surround factor. It is set to 1.0 for an average surround, 0.9 for a dim surround, and 0.8 for a dark surround. The choice is determined by the relative luminance, for example, a bright office, watching television indoors, or a dark cinema.\nCAT16 The current CIE recommended chromatic adaptation transform is CAT16, which is a linear transform. The process for the one-step CAT16 is as follows.\nInputs: White point of the input adapting field \\(\\text{XYZ}_{w}\\), white point of the output (reference) adapting field $\\text{XYZ}_{wr}$, input colour $\\text{XYZ}$, luminance of the adapting field light source $L_A$, and surround factor $F$.\nTransform \\(\\text{XYZ}_{w}\\), \\(\\text{XYZ}_{wr}\\), and $\\text{XYZ}$ into the RGB space using the transformation matrix \\(\\mathbf{M}_{16}\\).\n\\[\r\\begin{bmatrix}\rR \\\\ G \\\\ B\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r0.401288 \u0026 0.650173 \u0026 -0.051461 \\\\\r-0.250268 \u0026 1.204414 \u0026 0.045854 \\\\\r-0.002079 \u0026 0.048952 \u0026 0.953127\r\\end{bmatrix}\r\\begin{bmatrix}\rX \\\\ Y \\\\ Z\r\\end{bmatrix}\r\\] Perform the adaptation transform on each of the three RGB channels separately. The gain coefficients depend on the degree of adaptation, D, which is calculated in the same way as in CAT02. For the G and B gain coefficients, $R_{wr}$ and $R_w$ are replaced accordingly.\n\\[\rk_R = D \\cdot \\frac{Y_w}{Y_{wr}} \\cdot \\frac{R_{wr}}{R_w} + 1 - D \\\\\rR_c = k_R \\cdot R\r\\] Transform the adapted RGB values back to the XYZ space using \\(\\mathbf{M}_{16}^{-1}\\). The subscript c or r is used to denote the corresponding colour or the reference field, respectively.\n\\[\r\\begin{bmatrix}\rX_c \\\\ Y_c \\\\ Z_c\r\\end{bmatrix}\r=\r\\mathbf{M}_{16}^{-1}\r\\begin{bmatrix}\rR_c \\\\ G_c \\\\ B_c\r\\end{bmatrix}\r\\] Due to the presence of the degree of adaptation D, CAT16 is not reversible in most cases. There is also a two-step version of CAT16 designed to solve the problem where the original input cannot be recovered after inverting the one-step method. The two-step method defines an intermediate illuminant, such as an equal-energy white, and works by adapting the input field to the equal-energy white field and then predicting the corresponding colour for the output field in reverse from the equal-energy white field.\nIf we use \\(\\Lambda_{r,t}\\) to denote the linear gain diagonal matrix from the input field t to the output field r, then the total transformation matrix for the one-step method is:\n\\[\r\\Phi_{r,t} = \\mathbf{M}_{16}^{-1} \\Lambda_{r,t} \\mathbf{M}_{16}\r\\]The total transformation matrix for the two-step method is:\n\\[\r\\begin{align*}\r\\Pi_{r,t} \u0026= \\Psi_{r,se} \\Phi_{se,t} \\\\\r\u0026= \\mathbf{M}_{16}^{-1} \\Lambda_{se,r}^{-1} \\mathbf{M}_{16} \\mathbf{M}_{16}^{-1} \\Lambda_{se,t} \\mathbf{M}_{16} \\\\\r\u0026= \\mathbf{M}_{16}^{-1} \\Lambda_{se,r}^{-1} \\Lambda_{se,t} \\mathbf{M}_{16}\r\\end{align*}\r\\]Here, \u0026lsquo;se\u0026rsquo; denotes the equal-energy white. In practice, there is almost no difference in the results between the two-step and one-step methods, and the one-step method is currently more commonly used.\n","permalink":"https://jackchou.top/en/posts/pcrdi04/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep.4","title":"Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform"},{"content":"Black Body A black body is an idealised object that absorbs all incident electromagnetic radiation, without scattering or reflection. Therefore, the electromagnetic waves radiated by a black body are determined solely by its temperature, a phenomenon known as black-body radiation.\n$$\rM_{\\lambda}(T) = \\frac{2hc^2}{\\lambda^5} \\frac{1}{e^{\\frac{hc}{\\lambda k_B T}} - 1}\r$$Here, \\(h\\) is the Planck constant, \\(c\\) is the speed of light, and \\(k_B\\) is the Boltzmann constant. Care must be taken with unit conversions during use (especially between m and nm).\nThe figure shows the spectral radiant exitance of a black body at several temperatures. Black-body radiation has the following characteristics:\nThe spectral radiant exitance curve for each temperature has only one peak. As the temperature increases, the peak of the spectral radiant exitance shifts towards shorter wavelengths. At any given wavelength, the radiant exitance of a hotter black body is always greater than that of a cooler one. The colour of a black body can be calculated from its radiant exitance within the visible spectrum. When the colours of black bodies at various temperatures are plotted on a chromaticity diagram, the resulting curve is called the Planckian locus.\nThe Planckian locus shows that as temperature increases, a black body\u0026rsquo;s colour changes from red, to yellow, passes through a white region, and then becomes blue (you can imagine this progression). In daily life, we often talk about cool and warm coloured light. According to calculations related to black-body radiation, a black body that emits cool-coloured light (bluish light) has a higher temperature, while one that emits warm-coloured light has a lower temperature.\nAlthough a black body is an idealised object and does not exist in reality, the characteristics of a heated metal are close to those of a black body (its spectral emissivity is relatively flat). To understand black-body radiation, one can imagine the process of continuously heating a piece of iron. At room temperature, iron does not glow because its radiant energy is very low. As the iron is gradually heated, according to characteristic 3, its radiant exitance increases continuously, and the iron begins to \u0026ldquo;glow\u0026rdquo;—first appearing dark red, then orange-red. Subsequently, according to characteristic 2, the short-wavelength component of the spectral radiation gradually increases, causing the emitted light to become progressively bluer, appearing cool white or even blue.\nEmissivity of Metals I personally had always thought that metals were quite similar to black bodies, but in fact, there are significant differences between the two.\nAccording to Kirchhoff\u0026rsquo;s law of thermal radiation, at thermal equilibrium, a body\u0026rsquo;s absorptivity (\\(\\alpha\\)) is equal to its emissivity (\\(\\epsilon\\)). For any object, the sum of its absorptivity, reflectivity (\\(\\rho\\)), and transmissivity (\\(\\tau\\)) is 1. For metals, the transmissivity can be considered to be 0, so \\(\\rho + \\alpha = 1\\), which means \\(\\epsilon = 1 - \\rho\\).\nA black body has an emissivity of 1. Therefore, the lower a metal\u0026rsquo;s reflectivity, the closer its characteristics are to a black body. Examples include dark-coloured, rough-surfaced metals (like tungsten and iron). Additionally, the reflectivity of metals is relatively uniform across different wavelengths, which is why their radiation spectra have a shape similar to that of a black body.\nFor instance, CIE standard illuminant A is defined by the spectral distribution of a black body at a temperature of 2856 K, which is very close to the spectral distribution of a tungsten filament incandescent lamp of the same colour temperature.\nColour Temperature and Correlated Colour Temperature Light radiated by black bodies at different temperatures has different colours. Temperature can be used to describe a light source that has the same colour (same xy coordinates), which is known as its Colour Temperature.\nThe spectral power distribution of light at a certain colour temperature does not need to match that of a black body, nor does it need to have the same physical temperature; it only needs to have the same chromaticity. For most light sources, their colour coordinates do not necessarily fall on the Planckian locus. In such cases, we find the point on the Planckian locus that is closest to the light source\u0026rsquo;s colour coordinates, and this is called the Correlated Colour Temperature (CCT). \u0026ldquo;Closest\u0026rdquo; here refers to perceptual colour proximity, so the calculation must be performed in a uniform colour space, typically the CIE 1960 uv chromaticity diagram. Lines connecting colour coordinates with the same correlated colour temperature are called isotherms.\nIf the deviation from the Planckian locus is too great, the correlated colour temperature becomes meaningless, as two isotherms can even intersect. The degree of deviation from the Planckian locus is known as Duv (\\(\\Delta_{uv}\\)). CCT (Tcp) and Duv together can describe the colour characteristics of a light source more intuitively.\nStandard Illuminants Ultimately, colour is the human perception of light, and the light source fundamentally determines everything related to colour. Defining a light source is therefore extremely important in the study of colour science.\nIn addition to natural light sources, technological advancements have led to several revolutions in artificial light sources. For ease of communication and standardisation, the CIE has defined a number of standard illuminants. A standard illuminant is a series of spectral power distributions, not a specific light source. In fact, a light source that can produce such a spectrum may not even exist.\nCIE Standard Illuminant D Some standard illuminants are derived from natural light sources, the most important of which is daylight. By collecting multiple sets of spectral data from daylight, the CIE proposed a method to calculate the spectrum of daylight based on its colour temperature. The resulting spectral distribution is known as a CIE daylight illuminant. The daylight illuminant with a correlated colour temperature of 6504 K is defined as the D65 standard illuminant. Additionally, D50, D55, and D75, with correlated colour temperatures of 5003 K, 5500 K, and 7504 K respectively, serve as supplementary illuminants. Currently, there is no light source that can directly produce the D65 spectrum.\nThe specific calculation process is as follows:\nGiven a correlated colour temperature \\(T_{cp}\\), calculate the corresponding \\(x_D\\). The following formula for \\(T_{cp}\\) is applicable in the range of 4000 K to 7000 K. $$\rx_D = -4.6070 \\times 10^9 / T_{cp}^3 + 2.9678 \\times 10^6 / T_{cp}^2 + 0.09911 / T_{cp} + 0.244063\r$$ Calculate \\(y_D\\). The formula below is actually a second-order polynomial fit of the daylight locus. \\(x_D\\) and \\(y_D\\) are the colour coordinates of the daylight illuminant corresponding to \\(T_{cp}\\). $$\ry_D = -3.000 x_D^2 + 2.870 x_D - 0.275\r$$ The CIE used statistical methods to perform a principal component analysis on the collected daylight spectral data, dividing the daylight spectrum into three parts. \\(M_1\\) and \\(M_2\\) are calculated from \\(x_D\\) and \\(y_D\\). $$\rS_D(\\lambda) = S_0(\\lambda) + M_1 S_1(\\lambda) +M_2 S_2(\\lambda)\r$$$$\rM_1 = \\frac{-1.3515 - 1.7703 x_D + 5.9114 y_D}{0.0241 + 0.2562 x_D - 0.7341 y_D}\r$$$$\rM_2 = \\frac{0.0300 - 31.4424 x_D + 30.0717 y_D}{0.0241 + 0.2562 x_D - 0.7341 y_D}\r$$The resulting \\(S_D(\\lambda)\\) is the spectral power distribution of the CIE daylight illuminant, which is a function of the correlated colour temperature \\(T_{cp}\\).\nD. B. Judd et al., ‘Spectral Distribution of Typical Daylight as a Function of Correlated Color Temperature’, J. Opt. Soc. Am., vol. 54, no. 8, p. 1031, Aug. 1964, doi: 10.1364/JOSA.54.001031.\nCIE Standard Illuminant A Illuminant A represents the light emitted by an incandescent lamp; its spectral power distribution is that of a black-body radiator at a temperature of 2856 K. The CIE specifies a tungsten filament incandescent lamp with a colour temperature of 2856 K as standard illuminant A.\nCIE Standard Illuminant E Equal-energy white light is an artificially defined spectral distribution. It is widely used in theoretical calculations for colour spaces, where it serves as a neutral reference. There is no light source that can produce illuminant E.\nOther Standard Illuminants Standard illuminants B and C, which simulated daylight, have been deprecated.\nBesides incandescent lamps, fluorescent lamps were also a primary lighting device for a considerable period, especially in the United States and Europe. The standard illuminant FL series represents the spectral distributions of fluorescent lamps, with FL1 to FL12 corresponding to different types of fluorescent lamps.\nFL2, also known as CWF (Cool White Fluorescent), is mainly used in shopping centres and offices in the United States. It has a correlated colour temperature of 4150 K (so its colour temperature is not particularly cool).\nFL11, also known as TL84, is a proprietary product of Philips and is the most important commercial light source in Europe. It has a correlated colour temperature of 4000 K.\nAlthough fluorescent lamps are now being gradually replaced by LEDs, the standard illuminants for fluorescent lamps are still used in quality control and colour matching booths for their respective markets. Due to their emission mechanism, fluorescent lamps have narrow-band spectra, giving them an important role in the study of metamerism.\nColour Rendering Most objects do not emit light themselves but become visible when illuminated by a light source. Therefore, the light source directly affects the colour of an object as perceived by the human eye. The effect of a light source on colour is known as Colour Rendering. The colour rendering properties of a light source are a crucial metric for its evaluation and can be broadly divided into colour fidelity indices and gamut indices.\nColour Fidelity Index The colour of an object under a given light source should be consistent with its colour under a \u0026ldquo;familiar\u0026rdquo; reference illuminant, which is typically daylight or a standard illuminant from the D series. Therefore, the smaller the colour difference between its appearance under the test source and the reference illuminant, the higher the colour fidelity of the test source.\nSimply put, the method for testing colour fidelity is as follows: select a reference illuminant and a series of colour samples (with known reflectance). Calculate the tristimulus values of the colour samples based on the light source\u0026rsquo;s spectrum and the samples\u0026rsquo; reflectance. Then, choose a uniform colour space and a colour difference formula to calculate the colour difference of a sample under the two light sources.\nFurthermore, even if the reference illuminant and the test light source have the same correlated colour temperature, their colours may still differ. Therefore, a chromatic adaptation model is needed to correct for the influence of the difference in the light sources\u0026rsquo; own colours.\nThe Colour Rendering Index (CRI) is a method specified by the CIE for evaluating the colour rendering of light sources. For sources with a CCT of 5000 K and above, a D series standard illuminant of the same CCT is used as the reference. Below 5000 K, black-body radiation is used. There are 14 colour samples. The chromatic adaptation method used is the Von Kries transformation model, and the uniform colour space and colour difference formula are from CIE 1964.\nWhen evaluating some narrow-band light sources, the CRI correlates poorly with actual visual perception. CRI2012 was an improvement on this. Light source manufacturers can increase their CRI values by specifically tailoring the spectral shape, but the source may not actually render other colours well. CRI2012 switched to using 17 mathematically generated, centrally symmetric reflectance samples to avoid this \u0026ldquo;cheating\u0026rdquo; problem. Additionally, the uniform colour space and colour difference formula were replaced with CIECAM02-UCS, and the method for calculating the average colour difference was changed from an arithmetic mean to a root mean square (RMS) average.\nBecause the reference illuminant used has the same correlated colour temperature as the light source being tested, the CCT itself does not affect the colour rendering evaluation.\nGamut Index A light source can also have the effect of making an object\u0026rsquo;s colours appear more vibrant and vivid, which can improve the subjective evaluation of the object\u0026rsquo;s colour. Therefore, the gamut index is also an important metric for evaluating light sources.\nSome evaluation systems have introduced assessments of the gamut index and other more specific metrics, such as the Colour Quality Scale (CQS) from the National Institute of Standards and Technology (NIST) in the US, and TM-30-15 from the Illuminating Engineering Society (IES). These introduce evaluations for metrics like gamut, hue fidelity, and skin tone fidelity.\nColour Rendering in Practice How should we understand colour rendering? Consider whether the spectrum is \u0026ldquo;complete\u0026rdquo;. If a spectrum has no power distribution at all within a certain wavelength range, then an object whose reflectance peaks within that range cannot be illuminated by this light source. LEDs generally have lower energy distribution at long wavelengths, and the ninth colour sample in the CRI (R9) is a relatively saturated red. Consequently, the R9 value for LEDs is typically low.\nThe white point of a display is typically D65. This white is created by mixing the three red, green, and blue primaries. The spectral distributions of display primaries are relatively narrow, so if the white displayed on a screen were used as a light source, its colour rendering would also be poor.\nSome sellers of lighting fixtures advertise their lights as having high colour rendering or being \u0026ldquo;full-spectrum\u0026rdquo;. In the CRI system, the general colour rendering index, Ra, is the average of the first eight test colour samples and does not include R9, which is a weak point for LEDs. As a result, the nominal colour rendering index for LED fixtures is often very high, reaching 90 or even over 95.\nFrom 2024, China no longer permits the use of \u0026ldquo;pork lamps\u0026rdquo; in the market. These lamps use a special spectrum that makes pork underneath them look fresher. They can be seen as lights specifically optimised for the \u0026ldquo;pork colour sample,\u0026rdquo; and their colour rendering index for other colours is very low.\nColour Rendering Index (CRI Ra) and Correlated Colour Temperature (CCT) of common light sources:\nHigh-pressure sodium lamp: 24, 2100 K FL2 / CWF: 62, 4150 K FL11 / TL84: 85, 4000 K Full-spectrum LED (blue/violet-pumped): 95-99, 2700-5000 K Incandescent lamp: 100, 2856 K Regarding incandescent lamps, their very low colour temperature and extremely low luminous efficacy make them unsuitable for general lighting. There are quite a few \u0026ldquo;incandescent lamp cults\u0026rdquo; online, but this attitude actually demonstrates a fundamental misunderstanding of colour rendering.\nLight Source Consistency The Colour Rendering Index is a human-centric evaluation metric; it assesses the colour performance of samples under different light sources as perceived by the human eye. For film and television production, however, it is the camera, not the human eye, that samples and records the light. Therefore, using the CRI to evaluate lighting in film and television is not appropriate (although many companies selling film and TV lighting fixtures still use CRI for evaluation).\nThe TLCI (Television Lighting Consistency Index) and TLMF (Television Luminaire Matching Factor) proposed by the European Broadcasting Union are methods specifically designed to evaluate the colour rendering performance of light sources in television production. Similar to the CRI, they also require the selection of a reference light source, etc., but the tristimulus values used for comparing colour differences are replaced with the tristimulus values captured by a camera, processed, and then displayed on a screen, rather than those calculated directly from the colour samples\u0026rsquo; reflectance.\nThe colour samples used are the 18 (excluding greyscale) or 24 colours from a ColorChecker®. This is a widely used set of colour samples in film and television production and is the de facto industry standard.\nThe TLCI evaluates the consistency between a single light source and a standard illuminant (the reference illuminant). The TLMF evaluates the consistency between a mixture of light sources and another actual light source (the reference illuminant).\nThought Exercise What is the difference between an illuminant and a light source?\nAnswer\r- An illuminant refers to a specific spectral distribution, which may not exist in reality, such as the standard illuminant D series and E.\r- A light source is a physical object that emits light, which actually exists, such as incandescent lamps, fluorescent lamps, and LED lamps.\r- Some standard illuminants have corresponding light sources, for example, standard illuminant A corresponds to an incandescent lamp.\r","permalink":"https://jackchou.top/en/posts/pcrdi03/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep.3","title":"Principles of Colour Reproduction in Digital Images Notes: Illuminants and Light Sources"},{"content":"We have already covered the quantitative description of light, including its Spectral Power Distribution and radiometric quantities. This chapter deals with the quantitative description of colour, known as Colorimetry.\nColour Mixing Mixing: Colour mixing is divided into additive and subtractive mixing. When lights of various colours are shone onto a white sheet of paper, they can mix to produce white light. In contrast, when various pigments are applied to a white sheet of paper, the result is black. Additive mixing is more commonly used in experiments, and it is the principle behind displays.\nMatching: One half of an area is illuminated by one light, and the other half by another. If an observer cannot perceive a dividing line in the middle, meaning the two sides \u0026ldquo;look\u0026rdquo; the same, then the two lights (or their corresponding colours) are a match for that observer.\nColour and Light: Humans perceive light, which produces the sensation of colour. Light is a physical quantity, whereas colour is a psychophysical quantity. One light corresponds to one colour, but one colour does not necessarily correspond to only one light.\nThe Quantity of Colour: Colour is produced by light. Just as the quantity of light can be measured by radiometric quantities, the quantity of colour can be measured relatively by radiometric quantities. For instance, if the radiometric quantity is doubled, the \u0026ldquo;quantity of colour\u0026rdquo; is also doubled.\nUnder certain conditions, the human eye\u0026rsquo;s perception of colour mixing is linear, encompassing the following aspects:\nIf two lights match, they have the same colour. If the quantities of both colours are changed by the same factor, they will still match. If colour A matches colour B, and colour C matches colour D, then a mixture of colours A and C will also match a mixture of colours B and D. If a mixture of colours A and B matches colour C, and a mixture of colours X and Y matches colour B, then a mixture of colours A, X, and Y will match colour C. In simple terms, colour mixing is like the addition you learned in primary school, because colour mixing is essentially the superposition of light spectra, and radiometric quantities are naturally additive. Furthermore, once a match is made, it does not change with environmental variations. For example, changing the brightness or warmth of the background will not disrupt the match between the two lights on the white paper.\nAlthough colour mixing is linear, it does not mean that the perception of colour is linear. Taking brightness as an example, while 1 unit of light added to 1 unit of light can match 2 units of light, it does not mean that a person\u0026rsquo;s perception of brightness increases linearly.\nCIE 1931 RGB Colour Matching Functions CIE: The International Commission on Illumination (Commission Internationale de l\u0026rsquo;Eclairage) is an international organisation dedicated to the study of light, colour, and illumination. The CIE has established a series of standards in the field of colour science. The previously mentioned spectral luminous efficiency function $V(\\lambda)$ is one of the standards set by the CIE. Its presence can be seen in all aspects of colour science.\nFor detailed information on CIE 1931 RGB and colour matching functions, please see this excellent blog post: https://yuhaozhu.com/blog/cmf.html\n$$ C(\\lambda) = \\bar{r}(\\lambda) R + \\bar{g}(\\lambda) G + \\bar{b}(\\lambda) B $$ For some wavelengths of monochromatic light, a match cannot be achieved. In such cases, one of the primaries must be moved to the monochromatic light\u0026rsquo;s side, and its coefficient becomes negative.\nThis process yields a set of curves known as the Colour Matching Functions (CMFs), which are $\\bar{r}(\\lambda)$, $\\bar{g}(\\lambda)$, and $\\bar{b}(\\lambda)$. In the 1920s, W.D. Wright and J. Guild, among others, obtained data through experiments. In 1931, the CIE compiled and recommended a standard set of colour matching functions, known as the CIE 1931 RGB colour matching functions.\nHere, we are omitting countless details, such as whether the energy of the monochromatic light at each wavelength was the same in the experiments; the fact that the wavelengths of the primaries used by Wright and Guild were not identical, nor were they the same as the primaries ultimately recommended by the CIE, and how this data was converted; whether they were normalised, how they were normalised, and what the final units of the colour matching functions are. Although textbooks are often verbose, they choose to gloss over these points. I recommend reading the article at this link.\nThe function values here are called tristimulus values. The CIE 1931 RGB colour matching functions only contain data for a 2° field of view, which corresponds to the fovea of the human eye, the area where cone cells are most densely distributed. Wright\u0026rsquo;s experiment involved 10 observers, and Guild\u0026rsquo;s involved 7. This means that the experimental data from these 17 individuals laid the foundation for nearly a century of colour science, while also leaving behind potential issues and problems.\n$$ r = \\frac{R}{R+G+B} $$ The same applies to g and b. The resulting values, where $r + g + b = 1$, are called chromaticity coordinates.\nIn the 1931 RGB system, when the luminance ratio of the three primaries is $1:4.5907:0.0601$, they can be mixed to produce a colour that matches equal-energy white light. Furthermore, when the matching functions are added together in this proportion, they yield the previously mentioned spectral luminous efficiency function $V(\\lambda)$, meaning luminance can be calculated from the tristimulus values. This was likely the reference for the normalisation of the CIE 1931 RGB system.\nEqual-energy white light: Light in which all wavelengths have equal radiant energy. In the 1931 RGB system, the tristimulus values for equal-energy white light are (0.33, 0.33, 0.33).\nThe CIE 1931 XYZ Colorimetric System The CIE 1931 RGB colour matching functions were based on real primaries, which resulted in some negative values in calculations. This posed computational difficulties at the time. To solve this problem, the CIE recommended the CIE 1931 XYZ standard colorimetric system, which used imaginary primaries to not only ensure all tristimulus values were positive but also achieve other objectives.\nThe XYZ colorimetric system is derived from the RGB colorimetric system through a linear transformation:\n$$ \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} = \\begin{bmatrix} 2.7689 \u0026 1.7517 \u0026 1.1302 \\\\ 1.0000 \u0026 4.5907 \u0026 0.0601 \\\\ 0.0000 \u0026 0.0565 \u0026 5.5943 \\end{bmatrix} \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} $$The transformation matrix satisfies the following conditions:\nIn the RGB system, the spectral locus from 560-700 nm forms a straight line because short-wavelength light is no longer needed for matching in this range. It was desired for two of the new primaries to lie on this line. It was desired to use the Y value in the XYZ system to represent luminance, with X and Z having zero contribution to luminance. Therefore, X and Z should lie on the alychne (the line of zero luminance), i.e., $r + 4.5907g + 0.0601b = 0$. The final edge is a line tangent to the spectral locus at 503 nm. The equal-energy white point remains at (0.33, 0.33, 0.33). The resulting XYZ colour matching functions are all positive and represent the luminances of three imaginary primaries. Among them, $Y(\\lambda)$ is identical to the spectral luminous efficiency function $V(\\lambda)$. The Y value is equivalent to luminance.\nNormalising XYZ gives the chromaticity coordinates and luminance, $xyY$, which can be plotted to create the CIE 1931 xy chromaticity diagram.\nThe colouring on the chromaticity diagram is for aesthetic purposes; the points on the diagram contain only chromaticity information and no luminance information, which is why colours like grey are not shown. Because the chromaticity diagram is still linear, selecting two points on it allows for the mixing of their corresponding coloured lights to produce any colour on the line segment between them. If three points are chosen, colours within the resulting triangle can be mixed, which is also how the range of colours a display can show (its gamut) is represented. The diagram also shows that no set of three primaries can mix to produce all the colours perceptible to the human eye.\nUniform Colour Spaces Colour Space: A mathematical representation of colour, using a few quantities (usually three) to specify a colour. For example, CIE 1931 XYZ is a colour space that uses the three tristimulus values X, Y, and Z to represent colours. Colour spaces can be linear, meaning colours can be added and subtracted, or non-linear. Different colour spaces serve different functions. In colour science, colours need to be processed in the appropriate colour space.\nThe Non-linearity of Luminance Firstly, the human eye\u0026rsquo;s perception of brightness is non-linear. In an experiment similar to the matching experiment, subjects observe two colours with the same chromaticity coordinates but different luminances. Their luminances are $L$ and $L+\\Delta L$. When the difference in luminance is very small, a visual match can still be achieved. When the luminance difference exceeds a certain threshold, a difference can be perceived. This luminance difference is called the \u0026ldquo;Just Noticeable Difference\u0026rdquo; (JND). The JND varies at different luminance levels, so the luminance Y in the XYZ colour space is non-uniform. A change in Y from 10 to 20 is perceived differently from a change from 90 to 100. To address this, two new concepts were introduced: Lightness and Brightness. Brightness is the perception of absolute luminance, i.e., how \u0026ldquo;bright\u0026rdquo; a light is. Lightness is the relative value of brightness, representing the perceived \u0026ldquo;brightness\u0026rdquo; of an object relative to a perfect reflecting diffuser under the same illumination. For example, a sheet of white paper has a lightness of 90 (relative to 100) whether it is outdoors on a sunny day or indoors, but its brightness will change. The goal of lightness is that for a given change in lightness value, the perceived change in brightness is the same.\nBased on different experimental results, many lightness models have been proposed. The simplest lightness model is just a power function, such as Hunter\u0026rsquo;s lightness $L_H = Y ^ {0.5}$. Although lightness models vary in complexity, their basic shape is similar: in dark areas, the eye\u0026rsquo;s perception of brightness increases more rapidly, while in bright areas, it slows down. For example, the grey perceived by the human eye as being halfway between black and white has a reflectance of about 18-25%, not 50%.\nThe Non-linearity of Chromaticity Similarly, if two points that are close together are chosen on the CIE 1931 xy chromaticity diagram, a subject may not be able to distinguish their colours, i.e., the difference is less than one JND. MacAdam conducted experiments where subjects performed additive colour mixing from a single colour in different directions (i.e., deviating in different directions), recording the new position when a difference could be discerned. The experiments found that these new positions could be well-fitted by an ellipse, hence they are called MacAdam ellipses.\nThe MacAdam ellipses plotted on the CIE 1931 xy chromaticity diagram show that the chromaticity uniformity of this colour space is rather poor, as the ellipses vary in size and orientation. Many attempts have been made to improve chromaticity uniformity. For instance, noticing that the ellipses in the upper part of the chromaticity diagram are rather elongated, one might vertically compress the diagram. However, simple transformations have not yielded satisfactory results. Notable attempts include the CIE 1960 UCS and 1976 UCS. By applying simple transformations to xy to convert them into new colour coordinates uv or u\u0026rsquo;v\u0026rsquo;, some improvement in uniformity was achieved. Here, UCS stands for Uniform-Chromaticity-Scale.\nUniform Colour Spaces A lightness model (or uniform lightness scale) and a uniform chromaticity scale together form a three-dimensional colour space, known as a Uniform Colour Space. A well-known example is CIE 1976 \\(L^*a^*b^* \\), or CIELAB. It is composed of \\(L^*\\), which represents lightness, and \\(a^*b^*\\), which represent chromaticity on red-green and yellow-blue axes. Note that the asterisk is part of the symbol, and notations like Lab should be avoided to prevent confusion with symbols from other colour spaces.\nCIELAB is a relative colour space, requiring a \u0026ldquo;reference white point\u0026rdquo; to be defined first. This is the tristimulus values $X_n, Y_n, Z_n$ of a perfect reflecting diffuser under the illuminant.\n$$ L^* = 116 f(\\frac{Y}{Y_n}) - 16 $$$$ a^* = 500 [f(\\frac{X}{X_n}) - f(\\frac{Y}{Y_n})] $$$$ b^* = 200 [f(\\frac{Y}{Y_n}) - f(\\frac{Z}{Z_n})] $$Where,\n$$ f(t) = \\begin{cases} t^{1/3} \u0026 t \u003e (24/116)^{3} \\\\ (841 / 108)t + 16/116 \u0026 t \\leq (24/116)^{3} \\end{cases} $$CIELAB remains the most widely used uniform colour space today.\nColour Difference In industrial applications, we need to quantitatively measure the difference between colours. For example, in quality control, a batch of products is considered acceptable if the surface colour difference is below a certain threshold.\nIf a uniform colour space exists, the best way to measure colour difference would be to simply take the Euclidean distance between two colours in that space. Applied to the CIELAB space, this gives:\n$$ \\Delta E_{ab}=\\sqrt{(\\Delta L^*)^2+(\\Delta a^*)^2+(\\Delta b^*)^2} $$Unfortunately, the uniformity of CIELAB is not strong enough to directly apply such a distance formula to measure colour difference accurately. In the following decades, various patches were developed to calculate colour difference in the CIELAB space. The most famous of these is the CIE 2000 colour difference formula, or CIEDE2000, proposed by M.R. Luo, with the symbol $\\Delta E_{00}$. If you frequently follow digital or display technology, you have likely seen DE2000 used as a common tool for measuring display colour accuracy.\nAlthough CIEDE2000 is computationally complex, it is currently one of the best-performing formulas across all datasets and is the latest colour difference formula recommended by the CIE.\nThe Munsell Colour System Munsell was an American artist who, as early as 1905 (before the CIE 1931 XYZ system), developed a Colour Order System by summarising the experience of his predecessors and combining it with his own perspective as a painter. A colour order system is a system of colours formed by classifying and arranging various colour samples in a specific order, starting from visual perception.\nFrom the time Newton separated white light into the colours of the rainbow, we have been able to classify colours into red, orange, yellow, green, blue, indigo, and violet. This is, in fact, a classification of colours by their Hue.\nMunsell arranged colours into a three-dimensional space, called a colour solid, according to three dimensions: Value (Lightness), Hue, and Chroma. The colour chips within this solid are considered to be perceptually equidistant in all three dimensions. It is one of the most widely used colour systems today.\nBecause the Munsell Colour System is considered perceptually uniform, one can measure the tristimulus values of its colours and transform them into later uniform colour spaces. By observing whether the distribution of these points in the uniform colour space remains relatively \u0026ldquo;uniform\u0026rdquo;, the performance of the uniform colour space can be evaluated to some extent.\nM. Li and M. R. Luo, ‘Simple color appearance model (sCAM) based on simple uniform color space (sUCS)’, Opt. Express, vol. 32, no. 3, p. 3100, Jan. 2024, doi: 10.1364/OE.510196.\n","permalink":"https://jackchou.top/en/posts/pcrdi02/","summary":"Notes for Principles of Color Reproduction in Digital Images, Ep.2","title":"Principles of Colour Reproduction in Digital Images Notes: Fundamentals of Colorimetry"},{"content":"Light Light refers to a specific wavelength range within the electromagnetic spectrum. The short-wave end of visible light has a wavelength of about 360-400nm, and the long-wave end is about 760-830nm. Generally, the range is taken as 380-780nm.\nDifferent wavelengths of light can evoke different colour sensations. Isaac Newton used a prism to spatially separate light of different wavelengths. Different positions in a rainbow correspond to different wavelengths, and their \u0026ldquo;colours\u0026rdquo; also differ.\nMost light we encounter in daily life is complex light, meaning it is not of a single wavelength but is a mixture of multiple wavelengths. Here, an important concept must be mentioned: Metamerism. For the same person, a specific light corresponds to a definite colour, but a specific colour does not correspond to a definite light. Metamerism is the foundation and prerequisite for colour reproduction, which is why it is possible for us to reproduce colours from the natural world on a display, even if the spectra are completely different.\nSpectrum The spectrum is our method for describing the components of complex light.\nLight is a form of electromagnetic radiation. We use a radiometric quantity $X$ to measure the amount of radiation. In photometry, radiometric quantities include radiant flux, radiant intensity, irradiance, and radiance, but for now, we can refer to them collectively as radiometric quantity. We want to observe the amount of radiation of different wavelengths within complex light, so we introduce spectral density:\n$$X_\\lambda = \\frac{\\mathrm{d}X}{\\mathrm{d}\\lambda}$$Spectral density can be seen as a function of wavelength. A graph with wavelength on the horizontal axis and spectral density on the vertical axis can vividly represent the spectrum of a light.\nThe spectral density distribution is also known as the Spectral Power Distribution (SPD), which, in a narrow sense, refers to the radiant power distribution.\nBy dividing $X_\\lambda$ by $\\max{X_\\lambda}$, we normalise the SPD so its maximum value is 1. The resulting new function of wavelength is called the relative spectral power distribution.\nThe Human Eye Authors Hptim \u0026amp; Jmarchn. - Own work based on: Schematic diagram of the human eye en.svg by Rhcastilhos, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=13572928\nThe iris automatically adjusts the size of the pupil in the middle according to the intensity of the light. The cornea, lens, aqueous humour, and vitreous humour form the optical structure of the human eye. Cells on the retina perceive light, process and transmit signals, and finally, the nerve impulses are transmitted to the visual cortex via the optic nerve.\nAs a person ages, the optical density of the lens increases (i.e., its transmittance decreases), especially for shorter wavelengths. This means the lens becomes increasingly \u0026ldquo;yellow\u0026rdquo;. Although other parts of the visual system (such as chromatic adaptation) prevent the person from perceiving this change, it can exacerbate metamerism between individuals.\nThe retina not only converts light signals into nerve impulses but also performs initial processing. Therefore, the retina can be considered an \u0026ldquo;intelligent image sensor\u0026rdquo; with on-chip computation. The retina is composed, from inside to out, of receptor cells, bipolar cells, and ganglion cells. The axons of the ganglion cells form the optic nerve, which passes through the blind spot and continues through the retina to transmit signals. This means the retina is actually a \u0026ldquo;front-illuminated\u0026rdquo; sensor. The receptor cells related to vision are the cones and rods. Some receptor cells are not related to vision but can still perceive light and participate in other human physiological activities, such as the ipRGCs which are related to circadian rhythms.\nNote that in the diagram, light enters the eyeball from bottom to top. After passing through layers of cells, photopigments at the ends of the receptor cells undergo a light-triggered bioelectrical reaction, converting the light into bioelectrical signals. These signals are then transmitted back to the horizontal, bipolar, and amacrine cell layers. This is a network structure, divided into inner and outer layers, where information is transmitted and processed, before being passed to the ganglion cells, whose axons transmit it to the brain.\nSpectral Response of the Human Eye The pupil and photoreceptor cells of the human eye can automatically adjust and adapt according to the brightness of the ambient light. Based on the working state of the photoreceptor cells, vision can be divided into photopic vision, where cones work alone; scotopic vision, where rods work alone; and mesopic vision, where they work together. The transition from a dark to a bright environment, i.e., from scotopic to photopic vision, is very quick, taking about a minute. However, the transition from photopic to scotopic vision takes about thirty minutes.\nFor definitions of photopic, scotopic, and mesopic vision, see: A. Stockman and L. T. Sharpe, ‘Spectral Sensitivity’, in The Senses: A Comprehensive Reference, Elsevier, 2008, pp. 87–100. doi: 10.1016/B978-012370880-9.00300-5.\nFor a photodiode, its responsivity can be defined as the output divided by the input radiant energy. By inputting light of different wavelengths and measuring the output, we can obtain the sensor\u0026rsquo;s spectral responsivity. For the human eye, this means that light of the same radiant quantity but different wavelengths will have different perceived brightness. However, the output of the human eye cannot be measured directly, so experiments must be designed to relatively measure the perceived brightness of different wavelengths.\nThe \u0026ldquo;experiments\u0026rdquo; mentioned here are one of the most important methods in colour science. Because colour is a human sensation, we call it a \u0026ldquo;Psychophysical Quantity\u0026rdquo;. It is not an absolute, objective physical quantity and is also related to a person\u0026rsquo;s subjective factors. Experiments to measure psychophysical quantities are called psychophysical experiments. Generally, subjects are asked to complete tasks such as rating or matching. If the experiment is poorly designed, giving the subject tasks that are too abstract or difficult, the results will not be good. Furthermore, for experiments that need to represent the common characteristics of all humanity, factors like region and ethnicity must also be considered.\nAs mentioned earlier, different wavelengths of light give people different colour sensations. So, this experiment can be understood as how to quantify the brightness of different coloured lights with the same radiant quantity, for example, which is brighter between red light and blue light of the same radiant power. For colours with such large differences, observers find it difficult to give stable conclusions. A step-by-step method can be used: the subject repeatedly matches two lights of close wavelengths, covering the entire visible spectrum. Alternatively, the flicker method can be used: two wavelengths of light are flashed alternately. At a frequency of 30-50Hz, if there is a noticeable difference in perceived brightness, the subject will observe a flickering mixed colour. This can be used to judge whether the perceived brightness of the two wavelengths matches. Both methods can yield more stable results.\nLet\u0026rsquo;s temporarily call \u0026ldquo;perceived brightness\u0026rdquo; luminous flux $\\Phi_v$, and radiant flux $\\Phi_e$. Their ratio is the spectral responsivity of the human eye. If we specify a particular wavelength, then:\n$$\\Phi_v=K(\\lambda)*\\Phi_e$$Here, $K(\\lambda)$ is the spectral responsivity or Luminous Efficacy. By normalising the responsivity over the entire visible wavelength range so that its maximum value is 1, we get a function of wavelength $V(\\lambda)$ called the Luminous Efficiency.\nThe currently used luminous efficiency function was determined by the CIE in 1924 based on experiments with 251 observers. Its maximum value is at 555nm, meaning the human eye is most sensitive to yellow-green light with a wavelength of 555nm. It is important to note that this $V(\\lambda)$ recommended by the CIE is fitted from the experimental data of multiple observers; a person with this exact response may not exist in the world. In 1951, the CIE further determined the luminous efficiency function for scotopic vision, $V'(\\lambda)$. In scotopic vision, the maximum luminous efficacy is at 508nm, shifting towards the shorter wavelengths. This means that in scotopic vision, humans are less sensitive to long-wavelength light. Mesopic vision simply uses a linear combination of the photopic and scotopic functions.\nData from http://www.cvrl.org/lumindex.htm\nPhotometry \u0026ldquo;Brightness\u0026rdquo; is actually an ambiguous term. Most lasers are very bright, but if you use a laser to illuminate a room, it is not as bright as a few-watt LED lamp. To eliminate this ambiguity, more precise physical quantities are needed to define \u0026ldquo;bright\u0026rdquo;.\nLight is a form of radiation. The radiometric quantities mentioned earlier include radiant energy $Q_e$, radiant flux $\\Phi_e$, radiant intensity $I_e$, radiance $L_e$, and irradiance $E_e$. Although their names are similar, they are all different physical quantities. After understanding the relationships between these radiometric quantities, photometric quantities can be easily derived, rather than learning a large number of photometric quantities directly as in many textbooks.\nRadiometric quantities are physical quantities related to electromagnetic waves and are independent of the human eye; photometric quantities, however, are related to the human eye and are psychophysical quantities.\nRadiant Energy and Radiant Flux Radiant energy: Unit is the joule. It represents the energy of electromagnetic radiation. Electromagnetic radiation is a stream of particles called photons, and the radiant energy is the sum of the energy carried by these photons. The energy of a single photon can be calculated using Planck\u0026rsquo;s formula: $E=h\\nu$, where $h$ is Planck\u0026rsquo;s constant and $\\nu$ is the photon\u0026rsquo;s frequency.\nRadiant flux: Unit is the watt. It represents the radiant energy per unit time. The radiant flux can be obtained by taking the derivative of radiant energy with respect to time. A more intuitive term is radiant power, but to connect with luminous flux, it is recommended to use radiant flux.\nRadiant Intensity Radiant intensity: Unit is watts per steradian.\nLet\u0026rsquo;s first learn about the concept of a steradian. A steradian, $\\Omega$, is a unit of solid angle. 1 $\\mathrm{sr}$ is defined as the solid angle subtended by an area of $r^2$ on the surface of a sphere. The surface area of a sphere is $4\\pi r^2$, so a full sphere corresponds to $4\\pi \\mathrm{sr}$.\nIf a point source emits light uniformly in all directions, its radiant intensity is $I_e=\\Phi_e/4\\pi$. We often say that lasers are \u0026ldquo;bright\u0026rdquo;, which not only refers to their potentially high radiant power (e.g., pulsed lasers) but more to their high radiant intensity. This is because lasers are highly directional with a very small divergence angle, resulting in a high radiant intensity in the direction of propagation.\nRadiant Exitance and Irradiance The unit for these quantities is watts per square metre. They measure the different radiant fluxes on a \u0026ldquo;surface\u0026rdquo; and can be divided into incident and exiting.\nIncident: The radiant flux arriving at a surface is called irradiance. It is calculated by dividing the incident radiant flux by the area.\nExiting: The radiant flux leaving a surface is called radiant exitance. It is calculated by dividing the radiant flux emitted from the surface by the area.\nFor example, for a piece of grey paper placed under a light, the irradiance is the radiant flux from the light hitting the paper divided by the paper\u0026rsquo;s area, while the radiant exitance is the radiant flux reflected by the paper divided by the paper\u0026rsquo;s area. The direction of the incident or exiting light does not matter here, as long as it is the radiant flux arriving at or leaving the surface.\nRadiance Radiance: Unit is watts per steradian per square metre ($W/(sr \\cdot m^2)$).\nUnderstanding radiance can be broken down into several steps:\nA surface light source, as distinct from the point source used for radiant intensity, with an area of $\\mathrm{d}A$. A direction, forming an angle $\\theta$ with the normal of the surface source, and a solid angle in that direction of $\\mathrm{d}\\Omega$. In this direction, the projected area of the surface source is $\\mathrm{d}A\\cos\\theta$. The radiant flux within this solid angle is $\\Phi_e$. $$L_e=\\frac{\\mathrm{d^2}\\Phi_e}{\\mathrm{d}\\Omega\\mathrm{d}A\\cos\\theta}$$Photometric Quantities Since the human eye\u0026rsquo;s perceived brightness varies with the wavelength of light, radiometric quantities cannot directly reflect perceived brightness. Lights of different wavelengths with the same radiance will have different perceived brightness. By multiplying radiometric quantities by the previously mentioned luminous efficiency function, we can define photometric quantities.\nDefinition: The luminous flux corresponding to a light with a wavelength of 555nm and a radiant power (radiant flux) of 1/683 watts is 1 lumen. By multiplying the radiometric quantities by the luminous efficiency function and the constant 683 lm/W, we obtain the photometric quantities.\nRadiant energy / Radiant flux -\u0026gt; Luminous energy / Luminous flux, with units of lumen-seconds and lumens. Radiant intensity -\u0026gt; Luminous intensity, with units of lumens per steradian, defined as a new unit: the Candela (cd), which is one of the base units in the International System of Units. Irradiance -\u0026gt; Illuminance, with units of lumens per square metre, defined as the lux (lx). Radiant exitance -\u0026gt; Luminous exitance, with units of lumens per square metre. Radiance -\u0026gt; Luminance, with units of lumens per steradian per square metre, or candelas per square metre, defined as the nit (nt). Note that almost every photometric quantity has a new unit defined for it, which can make learning and memorisation difficult. I personally recommend using Candela and Lux in photometry and avoiding Nit.\nTo be continued ","permalink":"https://jackchou.top/en/posts/pcrdi01/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep.1","title":"Principles of Colour Reproduction in Digital Images Notes: Light and Vision"},{"content":"About the Book Thanks to Daniel_Li for recommending this book in the group.\nI consider this an excellent introductory text on colour science. It systematically presents the framework and key concepts of colour science with minimal verbosity, resulting in a concise volume.\nThough not without issues – such as substantial content overlap with Professor Xu Haisong\u0026rsquo;s Colour Information Engineering, disorganised sequencing of topics, and various minor flaws – it remains one of the better textbooks and reference works I\u0026rsquo;ve encountered within China.\nTitle: Principles of Colour Reproduction in Digital Images (数字影像颜色再现原理)\nAuthor: Gu Xiaojuan (顾晓娟) ISBN: 978-7-115-61746-0\nPlan The plan involves distilling and summarising each chapter\u0026rsquo;s content, reproducing key algorithms, providing expanded references, and completing selected exercises.\nThe current abbreviation used is PCRDI (Principles of Colour Reproduction in Digital Images).\nContents The notes follow the book\u0026rsquo;s chapter structure, so here\u0026rsquo;s a brief overview of its contents.\nLight and Vision Serves as an introductory chapter covering fundamental concepts of light and vision, with particular emphasis on photometry.\nFundamentals of Colorimetry Introduces colour matching experiments and derives the CIE standard colorimetric system (tristimulus values, colour coordinates, etc.). Briefly covers uniform colour spaces and the Munsell colour system, followed by colour difference formulae.\nIlluminants and Light Sources Presents common light sources in colour science and their evaluation methods, including CIE standard illuminants, colour rendering assessment, and light quality metrics.\nChromatic Adaptation and Chromatic Adaptation Transforms Building upon light sources and colorimetry fundamentals, this chapter explores human chromatic adaptation phenomena and key adaptation models, laying groundwork for colour appearance models.\nColour Appearance Phenomena and Colour Appearance Models Since basic colorimetry inadequately describes subjective colour perception, colour appearance attributes are crucial.\nCovers colour appearance attributes and common phenomena, followed by significant colour appearance models.\nColour Measurement and Colorimetric Instruments This chapter spans merely four pages\nAs colour science seldom focuses on measurement specifics (spectral separation, sensor technology), it briefly introduces common colorimetric instruments and their basic principles.\nColour Reproduction Colour reproduction represents the ultimate goal of production workflows.\nLeveraging previously introduced models and methods, this chapter covers evaluation metrics and specialised concepts in colour reproduction.\nColour Information Processing in Digital Cameras Explains camera operational principles and characterisation methods.\nProvides detailed coverage of demosaic algorithms and white balance techniques.\nColour Information Processing in Display Devices Examines display device fundamentals and colour rendering workflows. Includes thorough explanations of colour mixing methods for multi-primary displays.\nDigital Colour Management Systems Introduces colour management systems (e.g., ICC, ACES) as comprehensive applications of all preceding concepts.\n","permalink":"https://jackchou.top/en/posts/pcrdi00/","summary":"Notes for Principles of Colour Reproduction in Digital Images, Ep.0","title":"Principles of Colour Reproduction in Digital Images Notes: Introduction"},{"content":"Me I am Jack Chou.\nI am currently a postgraduate student in the field of colour science. For information about me and our work, please refer to my personal page and the laboratory website.\nI have many hobbies, primarily photography, cats, and coffee.\nHere This blog was first created on June 29, 2023, with the aim of having a place to showcase my content during the autumn recruitment period. In reality, with the half-baked experience, insight, and mindset at that time, I could not write anything decent, and that autumn recruitment, in hindsight, was unlikely to succeed. However, there were also some articles with a bit of substance, like Processing Raw Images in MATLAB, but more often, articles were hidden by me as drafts.\nLater, a very fortunate opportunity ended that peculiar autumn recruitment, and I no longer had the motivation or goal to update this blog, occasionally using it for some odd purposes. For example, it was used to share postcard stories with a French couple I met while staying at an Airbnb in Paris.\nThis is the story of the last time the blog was built. Some of the previous content can still be found in the Ancient Relics 🌃.\nSince it is a static website deployed on GitHub, my other pages on GitHub would have the same domain name. Thus, if visitors access the domain directly without entering the subsequent content, they would see a chaotic, long-unupdated blog.\nTo make it more convenient to share the pages of those projects and to utilise CloudFlare\u0026rsquo;s Tunnel function, I purchased the domain jackchou00.icu, making the problem of the abandoned blog even more apparent.\nOn January 20, 2025, to satisfy my own desire to share and a bit of vanity, and to prevent this domain from pointing to an abandoned blog, the revamped JacksBlog was officially pushed. This time, its purpose is to record some aspects of research and interests, sharing images and life.\nThe webpage is generated by Hugo, using a slightly modified version of the PaperMod theme, deployed on GitHub Pages and served via Cloudflare DNS and proxy.\nFuture For some reason, I\u0026rsquo;m unable to renew the domain jackchou00.icu on Alibaba Cloud.\nTo migrate the domain, I am required to pay for one year. After migration, subsequent renewals will cost 12 USD annually. I\u0026rsquo;m still deliberating whether to commit to this 12 annual fee.\nI\u0026rsquo;ve currently migrated to the domain jackchou.top, and both jackchou00.icu and zms.im are now redirecting to it.\nI\u0026rsquo;ve purchased a ten-year registration for the domain jackchou.top. Where will we be in 2035?\nUpdate Log 2025.10.16: Switched the code font to Google Sans Code and slightly increased the font weight. 2025.09.05: Using serif and sans-serif fonts at the same time looked too strange, switched to a font called Sen. 2025.08.12: Attempted to optimise the layout for wide-screen devices, expanding the content width in two tiers based on the total width. The changes are located in themes/PaperMod/assets/css/core/zmedia.css. 2025.08.03: Added a badge to the footer for a rough visitor count. 2025.07.24: Main domain has been changed to jackchou.top. 2025.07.20: Used Gemini 2.5 Pro to translate all articles and changed English font to Lora。 2025.06.29: Migrated images from Alibaba Cloud OSS to Cloudflare R2. 2025.06.11: Used SaaS to accelerate domestic access and for Cloudflare Enterprise support. 2025.04.14: Added multi-language support (Experimental Support for Multiple Languages). 2025.02.17: Discovered that using tags can effectively achieve categorisation, removed the secondary category \u0026lsquo;Colour\u0026rsquo;. 2025.02.11: Deployed GitHub Actions; now, the blog\u0026rsquo;s source files also have version management and automatic deployment. 2025.02.02: Added a button for toggling display/hide, implemented using shortcodes. 2025.01.24: Disabled MiSans to try to speed up loading; the current Chinese font is Source Han Sans from Google Fonts. 2025.01.22: Used Cloudflare R2 to store images. 2025.01.20: Updated fonts, introduced Ubuntu Sans and Ubuntu Mono using Google Fonts. The Chinese font is MiSans, loaded locally. The background colour of code blocks changed to 238. 2025.01.18: Introduced support for $\\LaTeX$ formulas, referencing PaperMod: Math Typesetting 2025.01.15: Modified the background colour of PaperMod pages.\nChanged theme in themes\\PaperMod\\assets\\css\\core\\theme-vars.css from pure white to 245. ","permalink":"https://jackchou.top/en/about/","summary":"\u003ch2 id=\"me\"\u003eMe\u003c/h2\u003e\n\u003cp\u003eI am Jack Chou.\u003c/p\u003e\n\u003cp\u003eI am currently a postgraduate student in the field of colour science. For information about me and our work, please refer to \u003ca href=\"http://cel.zju.edu.cn/2024/0923/c27449a2965767/page.htm\"\u003emy personal page\u003c/a\u003e and the \u003ca href=\"http://cel.zju.edu.cn\"\u003elaboratory website\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI have many hobbies, primarily photography, cats, and coffee.\u003c/p\u003e\n\u003ch2 id=\"here\"\u003eHere\u003c/h2\u003e\n\u003cp\u003eThis blog was first created on \u003ca href=\"/posts/old/helloworld/\"\u003eJune 29, 2023\u003c/a\u003e, with the aim of having a place to showcase my content during the autumn recruitment period. In reality, with the half-baked experience, insight, and mindset at that time, I could not write anything decent, and that autumn recruitment, in hindsight, was unlikely to succeed. However, there were also some articles with a bit of substance, like \u003ca href=\"/posts/old/processingrawinmatlab/\"\u003eProcessing Raw Images in MATLAB\u003c/a\u003e, but more often, articles were hidden by me as drafts.\u003c/p\u003e","title":"About"}]