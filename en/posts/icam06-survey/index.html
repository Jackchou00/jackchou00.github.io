<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>iCAM06: Colour Appearance Model in Image Processing | JacksBlog</title>
<meta name=keywords content="HDR image rendering,iCAM06,tone mapping"><meta name=description content="iCAM06: HDR rendering with color models & tone compression for accurate display reproduction."><meta name=author content="Miaosen Zhou"><link rel=canonical href=https://jackchou.top/en/posts/icam06-survey/><link crossorigin=anonymous href=/assets/css/stylesheet.2ff386b10b02a361d472eab220247dbdf4bb7faecac9567e482555e7ba4cc9ba.css integrity="sha256-L/OGsQsCo2HUcuqyICR9vfS7f67KyVZ+SCVV57pMybo=" rel="preload stylesheet" as=style><link rel=icon href=https://jackchou.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jackchou.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jackchou.top/favicon-32x32.png><link rel=apple-touch-icon href=https://jackchou.top/apple-touch-icon.png><link rel=mask-icon href=https://jackchou.top/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-cn href=https://jackchou.top/posts/icam06-survey/><link rel=alternate hreflang=en href=https://jackchou.top/en/posts/icam06-survey/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><link rel=preconnect href=https://cdn.jsdelivr.net><meta property="og:url" content="https://jackchou.top/en/posts/icam06-survey/"><meta property="og:site_name" content="JacksBlog"><meta property="og:title" content="iCAM06: Colour Appearance Model in Image Processing"><meta property="og:description" content="iCAM06: HDR rendering with color models & tone compression for accurate display reproduction."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-13T20:00:00+08:00"><meta property="article:modified_time" content="2025-12-21T00:41:45+08:00"><meta property="article:tag" content="Colour"><meta property="article:tag" content="CAM"><meta property="article:tag" content="ISP"><meta property="article:tag" content="HDR"><meta property="og:image" content="https://img.jackchou.top/jack-img/2025/08/81bb602c03f9704ee42e292468396187.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://img.jackchou.top/jack-img/2025/08/81bb602c03f9704ee42e292468396187.webp"><meta name=twitter:title content="iCAM06: Colour Appearance Model in Image Processing"><meta name=twitter:description content="iCAM06: HDR rendering with color models & tone compression for accurate display reproduction."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts üå≥","item":"https://jackchou.top/en/posts/"},{"@type":"ListItem","position":2,"name":"iCAM06: Colour Appearance Model in Image Processing","item":"https://jackchou.top/en/posts/icam06-survey/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"iCAM06: Colour Appearance Model in Image Processing","name":"iCAM06: Colour Appearance Model in Image Processing","description":"iCAM06: HDR rendering with color models \u0026 tone compression for accurate display reproduction.","keywords":["HDR image rendering","iCAM06","tone mapping"],"articleBody":"Objective One of the goals of image processing is Colour Reproduction. When placed in a real scene, the light received can originate from reflection, transmission, scattering, or a mixture of these from different objects, or directly from light sources. Their spectra vary greatly, and their luminance can span a wide range. Displays, on the other hand, can only provide a limited luminance range and spectroscopically can only offer a mixture of three primaries.\nFortunately, metamerism and the complexity of visual colour perception make Colour Reproduction possible. Through the research and development in colour science, what we pursue is no longer merely the reproduction of luminance or colour coordinates, but the reproduction of Colour Appearance. This requires the involvement of Colour Appearance Models.\nColour Appearance Models can be used to predict the Colour Appearance (Lightness, Chroma, etc.) of a tristimulus value under specific Viewing Conditions. For image and Colour Reproduction, it is necessary to consider the vastly different Viewing Conditions between the real scene and the Display. Traditional cinemas can only provide a luminance of about 48 nits, yet they can reproduce vivid scenes. This is partly because cinemas provide a nearly dark, low-luminance Viewing Condition.\nBy using a forward Colour Appearance Model, the Colour Appearance of each pixel is calculated from its tristimulus values. Then, based on the Viewing Conditions of the Display, an inverse Colour Appearance Model is applied to predict what tristimulus values the Display needs to produce to achieve that Colour Appearance. This constitutes a relatively complete image processing pipeline for Colour Reproduction using Colour Appearance Models.\niCAM06 is a Colour Appearance Model proposed by Jiangtao Kuang et al. for rendering HDR images. It incorporates many theories and algorithms from Colour Appearance Models, achieving a relatively scientific image processing approach from scenes with a large luminance range to Displays.\nStarting Point: Input A typical Colour Appearance Model accepts tristimulus values and Viewing Conditions as input.\nTristimulus values differ from the more common RGB space; they are a device-independent space. They are independent of the device, and under the same Viewing Conditions, if two colours have equal tristimulus values, they will achieve a match (look the same). RGB is a device-dependent space; for example, if two different Displays both display a pure red, the RGB values might be equal, but the colours are likely different because different Displays have different red primaries.\nIn a traditional image processing workflow, starting from a raw image and going through White Balance and Colour Correction Matrix (CCM), the colour space can be converted to XYZ tristimulus values. A common implementation is in the raw image processing library rawpy, where output_color = rawpy.ColorSpace(5) specifies the output colour space.\nIt is important to note that the XYZ values obtained this way are not the tristimulus values of the actual scene but are already White Balanced (Chromatic Adapted) tristimulus values. What we need are colours that represent the actual scene; therefore, a slightly modified initial ISP is required.\nFrom a more fundamental perspective, how is a raw image generated from the spectrum?\n$$ R=\\int P(\\lambda)\\,\\bar{r}(\\lambda)\\,\\mathrm{d}\\lambda $$The equation above represents an ideal image Sensor‚Äôs expression, where $P$ is the spectral power, $\\bar{r}$ is the Sensor‚Äôs Spectral Sensitivity Function, determined by the spectral characteristics of the photodiode, the transmittance of the colour filter, etc. $R$ is the output pixel value, which can be read from the raw file.\nThe expression for tristimulus values can be written as:\n$$ X=\\int P(\\lambda)\\,\\bar{x}(\\lambda)\\, \\mathrm{d}\\lambda $$Here, $\\bar{x}$ represents the Spectral Sensitivity Function of the human eye. Therefore, if we can linearly combine $\\bar{r}(\\lambda),\\bar{g}(\\lambda),\\bar{b}(\\lambda)$ to produce $\\bar{x}(\\lambda)$, etc., we can estimate the tristimulus values using the camera‚Äôs raw pixel values. This linear combination process can be represented by a 3x3 matrix.\nThe image above illustrates using a linear combination to predict tristimulus values from the camera‚Äôs Spectral Sensitivity Functions. The camera‚Äôs Spectral Sensitivity Functions after linear combination have shapes similar to the Colour Matching Functions of the tristimulus values.\nAdditionally, a coefficient is needed for scaling to convert to tristimulus values representing Absolute Luminance. This coefficient can be calculated from the camera‚Äôs Aperture, Shutter Speed, and ISO. These parameters used to control the amount of light entering do not affect the linearity or relative relationships of the light.\nAnother scenario involves using High Dynamic Range Images as input, such as HDR images encoded with the PQ Transfer Function. First, the non-linearly encoded RGB pixel values are decoded using the EOTF to obtain linear RGB pixel values. Then, the linear RGB pixel values are converted to the XYZ space to obtain the tristimulus values. Several of the .tif files and image_input.py provided in the project utilize this type of input.\nImage Decomposition According to the visual system‚Äôs different perception of colour and detail, the image is decomposed into a Base Layer and a Details Layer. Operations on colour, such as Chromatic Adaptation and Tone Compression, are only applied to the Base Layer. The Details Layer, after enhancement or adjustment, is merged with the adjusted Base Layer.\nThe Base Layer is obtained using an edge-preserving Bilateral Filter, a method previously proposed by Durand and Dorsey. Bilateral Filtering is a non-linear filter where the weight of each pixel is jointly determined by a Gaussian filter in the spatial domain and another Gaussian filter in the intensity domain. The latter reduces the weight of pixels with significant intensity differences from the center pixel.\nTherefore, Bilateral Filtering can effectively smooth the image while preserving sharp edges, thereby avoiding the ‚Äúhalo‚Äù artifacts common in local tone mapping operators. The intensity domain calculations are performed in Logarithmic Space, where intensity better represents perceived contrast and facilitates more uniform processing across the entire image.\nThe Details Layer is obtained by subtracting the Base Layer from the original image. Both layers need to be converted back to linear space.\nThe Bilateral Filtering used in iCAM06 is accelerated through piecewise linear approximation and nearest-neighbor downsampling.\nChromatic Adaptation The colour of an object changes under different lighting and Viewing Conditions, but the human visual system can maintain a relatively stable perception of the object‚Äôs colour to some extent. This phenomenon is called Colour Constancy. This process of maintaining relative stability is called Chromatic Adaptation.\nA Chromatic Adaptation Transform (CAT) is a model used to predict corresponding colours. It takes two Viewing Conditions (usually represented by the tristimulus values of the scene white point) and a colour under one Viewing Condition as input, and predicts the colour under the other Viewing Condition that would be a corresponding colour.\nAccording to von Kries‚Äô hypothesis, Chromatic Adaptation is independent at the level of the visual organs. The basic structure of a Chromatic Adaptation Transform is:\nConvert the input XYZ to a space representing the visual organs (Cone responses). Process each quantity independently in this space (e.g., multiply by respective gain factors). Convert back to XYZ space to obtain the colour tristimulus values under the other Viewing Condition. There are many Chromatic Adaptation Transforms designed according to this structure, among which CAT02 and CAT16 are two CAT models successively recommended by CIE. In the second step, there is an adaptation degree D, representing the extent of Chromatic Adaptation. In iCAM06, this adaptation degree is multiplied by a coefficient of 0.3, which is equivalent to reducing the degree of adaptation, bringing it closer to the colours in the scene rather than the adapted corresponding colours, to increase the colour saturation of the image.\nThis is a very strange practice. I am more inclined to believe it is a code error that causes numerical errors if this coefficient is not multiplied. This is because the two Cone responses used in the Chromatic Adaptation step in the original code are one normalized and one absolute value.\nIn iCAM06, the target adaptation field for this step of Chromatic Adaptation is D65, because the subsequent uniform colour spaces are designed for the D65 white point. The white point of the adaptation field uses a Gaussian blur of the Base Layer, which is somewhat similar to the Grey World hypothesis.\n$$ \\begin{align*} D \u0026= 0.3 F \\left[ 1 - \\left( \\frac{1}{3.6} \\right) e^{-\\frac{(L_A - 42)}{92}} \\right] \\\\ R_c \u0026= \\left[ \\left( R_{D65} \\frac{D}{R_W} \\right) + (1-D) \\right] R \\end{align*} $$In the original equation, the sign of 42 in the exponent of $e$ in the calculation of adaptation degree D is incorrect.\nThe Adaptation White (Above) and Adapted Image (Below) Tone Compression The human eye‚Äôs perception of luminance is not linear, but highly non-linear. By applying Tone Compression according to this non-linear characteristic, it is possible to reproduce Colour Appearance with a larger luminance range within a limited Display luminance range.\nThis non-linear relationship is also obtained from visual experiments. iCAM06 uses the post-adaptation part from CIECAM02, which is shaped like a Sigmoid function. When used, it involves converting from tristimulus values to another space representing visual cells, and then applying a response curve called ‚Äúpost-adaptation‚Äù. In iCAM06, the response of Rods under scotopic vision is also added and superimposed on the Cone response to predict luminance in the scotopic-mesopic range; the Rod response is very small.\nThe post-adaptation non-linear relationship for Cones is as follows:\n$$ R'_a = \\frac{400 (F_L R' / Y_W)^p}{27.13 + (F_L R' / Y_W)^p} + 0.1 $$The reference white $Y_{W}$ used in this step is also a Gaussian blur of the Base Layer, but with a greater degree of blur than in Chromatic Adaptation.\nThis step completes the compression of luminance. The original large luminance range, after passing through this Sigmoid function, has a range of 0.1 to 400, although it rarely exceeds 200. Before this step, the relationship was linear with the scene light; after this step, it is linear with the Display light. Therefore, this step can also be understood as an Optic-Opto Transfer Function (OOTF).\nMerging Image and Output After completing Chromatic Adaptation and Tone Compression, the Base Layer is an image that can be displayed relatively normally on a screen. The Details Layer can be enhanced and merged back.\nThe image obtained at this point is still in the linear XYZ tristimulus value space. Converting XYZ to the RGB space suitable for display involves two steps:\nConversion to linear RGB space. Applying the Transfer Function encoding. For the most common sRGB space, the matrix used in the first step is readily available online. The second step is a Gamma Correction, with a coefficient equal to the reciprocal of the Display‚Äôs Gamma, usually between 0.45 and 0.5.\nAdditional Operations: IPT Space Compressing an original high dynamic range, high luminance image onto a low luminance Display can sometimes result in less vivid colours, and the contrast between light and dark areas also needs to be enhanced.\nThe solution in iCAM06 is to convert to a uniform colour space for enhancement, choosing the IPT space. I represents Lightness, and P and T represent two colour directions, red-green and yellow-blue respectively.\nThe method for enhancing contrast is to apply a Gamma exponent between 1.0 and 1.5 to Lightness, with the value determined by the Viewing Environment. The principle is that perceived contrast changes according to the relative luminance of the Viewing Environment. Dark environments like cinemas require higher contrast, so a higher System Gamma exponent is usually adopted. A potential issue is that past System Gamma was applied to linear light, not to a non-linear scale like Lightness.\nThe method for enhancing Chroma is to stretch the two colour directions. The degree of stretching is related to luminance, based on the Hunt effect: an increase in luminance leads to an increase in perceived Colourfulness.\n$$ P = P \\cdot \\left[ (F_L + 1)^{0.2} \\left( \\frac{1.29C^2 - 0.27C + 0.42}{C^2 - 0.31C + 0.42} \\right) \\right] $$\nResults and Analysis This algorithm addresses two problems:\nHow to reproduce real-world scenes on a display. How to reproduce high dynamic range images on traditional low dynamic range displays. Unlike computer vision, colour science focuses more on human visual perception, aiming to process images from a visual perspective. iCAM06, through methods such as Chromatic Adaptation, Tone Compression, and uniform colour spaces, provides an interpretable solution for image processing from high dynamic range to low dynamic range.\nHowever, iCAM06 also has some shortcomings:\nThe Chromatic Adaptation algorithm has issues, and the effect after correction is not ideal, possibly due to the limitations of the Chromatic Adaptation model and the influence of the Grey World hypothesis. The Sigmoid function used for Tone Compression reduces image contrast too much and cannot balance the effects for both low dynamic range and high dynamic range inputs. Edge-preserving transformation and detail enhancement may introduce artifacts and excessive sharpening. Processing in a uniform colour space lacks reliable theoretical basis, especially the practice of applying a gamma exponent to Lightness. Overall, iCAM06, leveraging research from colour science, proposes an effective method for high dynamic range image processing and is a successful exploration of integrating colour science into image processing.\nReferences [1] M. D. Fairchild and G. M. Johnson, ‚ÄúMeet iCAM: A next-generation color appearance model,‚Äù Proc. 10th Color Imaging Conf., vol. 10, no. 1, pp. 33‚Äì38, Jan. 2002.\n[2] J. Kuang, G. M. Johnson, and M. D. Fairchild, ‚ÄúiCAM06: A refined image appearance model for HDR image rendering,‚Äù J. Visual Communication and Image Representation, vol. 18, no. 5, pp. 406‚Äì414, Oct. 2007.\n[3] F. Durand and J. Dorsey, ‚ÄúFast bilateral filtering for the display of high-dynamic-range images,‚Äù in Proc. 29th Annual Conf. Computer Graphics and Interactive Techniques (SIGGRAPH), San Antonio, TX, USA, Jul. 2002, pp. 257‚Äì266.\n[4] P. Hung and R. S. Berns, ‚ÄúDetermination of constant hue loci for a CRT gamut and their predictions using color appearance spaces,‚Äù Color Research \u0026 Application, vol. 20, no. 5, pp. 285‚Äì295, Oct. 1995.\n[5] M. R. Luo and C. Li, ‚ÄúCIECAM02 and its recent developments,‚Äù in Advanced Color Image Processing and Analysis, C. Fernandez-Maloigne, Ed., New York, NY, USA: Springer, 2013, pp. 19‚Äì58.\n[6] M. D. Fairchild, ‚ÄúA revision of CIECAM97s for practical applications,‚Äù Color Research \u0026 Application, vol. 26, no. 6, pp. 418‚Äì427, 2001.\n","wordCount":"2332","inLanguage":"en","image":"https://img.jackchou.top/jack-img/2025/08/81bb602c03f9704ee42e292468396187.webp","datePublished":"2025-05-13T20:00:00+08:00","dateModified":"2025-12-21T00:41:45+08:00","author":{"@type":"Person","name":"Miaosen Zhou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jackchou.top/en/posts/icam06-survey/"},"publisher":{"@type":"Organization","name":"JacksBlog","logo":{"@type":"ImageObject","url":"https://jackchou.top/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jackchou.top/en/ accesskey=h title="JacksBlog (Alt + H)">JacksBlog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jackchou.top/ title=‰∏≠Êñá aria-label=‰∏≠Êñá>‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://jackchou.top/en/posts/ title="Posts üå≥"><span>Posts üå≥</span></a></li><li><a href=https://jackchou.top/en/photos/ title="Photos üì∑"><span>Photos üì∑</span></a></li><li><a href=https://jackchou.top/en/tags/ title="Tags üè∑Ô∏è"><span>Tags üè∑Ô∏è</span></a></li><li><a href=https://jackchou.top/en/about/ title="About üòº"><span>About üòº</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">iCAM06: Colour Appearance Model in Image Processing</h1><div class=post-meta><span title='2025-05-13 20:00:00 +0800 +0800'>2025.05.13</span>&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://jackchou.top/posts/icam06-survey/>‰∏≠Êñá</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#objective aria-label=Objective>Objective</a></li><li><a href=#starting-point-input aria-label="Starting Point: Input">Starting Point: Input</a></li><li><a href=#image-decomposition aria-label="Image Decomposition">Image Decomposition</a></li><li><a href=#chromatic-adaptation aria-label="Chromatic Adaptation">Chromatic Adaptation</a></li><li><a href=#tone-compression aria-label="Tone Compression">Tone Compression</a></li><li><a href=#merging-image-and-output aria-label="Merging Image and Output">Merging Image and Output</a></li><li><a href=#additional-operations-ipt-space aria-label="Additional Operations: IPT Space">Additional Operations: IPT Space</a></li><li><a href=#results-and-analysis aria-label="Results and Analysis">Results and Analysis</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=objective>Objective<a hidden class=anchor aria-hidden=true href=#objective>#</a></h2><p>One of the goals of image processing is Colour Reproduction. When placed in a real scene, the light received can originate from reflection, transmission, scattering, or a mixture of these from different objects, or directly from light sources. Their spectra vary greatly, and their luminance can span a wide range. Displays, on the other hand, can only provide a limited luminance range and spectroscopically can only offer a mixture of three primaries.</p><p>Fortunately, metamerism and the complexity of visual colour perception make Colour Reproduction possible. Through the research and development in colour science, what we pursue is no longer merely the reproduction of luminance or colour coordinates, but the reproduction of Colour Appearance. This requires the involvement of Colour Appearance Models.</p><p>Colour Appearance Models can be used to predict the Colour Appearance (Lightness, Chroma, etc.) of a tristimulus value under specific Viewing Conditions. For image and Colour Reproduction, it is necessary to consider the vastly different Viewing Conditions between the real scene and the Display. Traditional cinemas can only provide a luminance of about 48 nits, yet they can reproduce vivid scenes. This is partly because cinemas provide a nearly dark, low-luminance Viewing Condition.</p><p>By using a forward Colour Appearance Model, the Colour Appearance of each pixel is calculated from its tristimulus values. Then, based on the Viewing Conditions of the Display, an inverse Colour Appearance Model is applied to predict what tristimulus values the Display needs to produce to achieve that Colour Appearance. This constitutes a relatively complete image processing pipeline for Colour Reproduction using Colour Appearance Models.</p><p>iCAM06 is a Colour Appearance Model proposed by Jiangtao Kuang et al. for rendering HDR images. It incorporates many theories and algorithms from Colour Appearance Models, achieving a relatively scientific image processing approach from scenes with a large luminance range to Displays.</p><p><img alt=original_image.jpg loading=lazy src=https://img.jackchou.top/jack-img/2025/05/10092677bb30eb7b9c2394dd9f601f4a.avif></p><h2 id=starting-point-input>Starting Point: Input<a hidden class=anchor aria-hidden=true href=#starting-point-input>#</a></h2><p>A typical Colour Appearance Model accepts tristimulus values and Viewing Conditions as input.</p><p>Tristimulus values differ from the more common RGB space; they are a device-independent space. They are independent of the device, and under the same Viewing Conditions, if two colours have equal tristimulus values, they will achieve a match (look the same). RGB is a device-dependent space; for example, if two different Displays both display a pure red, the RGB values might be equal, but the colours are likely different because different Displays have different red primaries.</p><p>In a traditional image processing workflow, starting from a raw image and going through White Balance and Colour Correction Matrix (CCM), the colour space can be converted to XYZ tristimulus values. A common implementation is in the raw image processing library <code>rawpy</code>, where <code>output_color = rawpy.ColorSpace(5)</code> specifies the output colour space.</p><p>It is important to note that the XYZ values obtained this way are not the tristimulus values of the actual scene but are already White Balanced (Chromatic Adapted) tristimulus values. What we need are colours that represent the actual scene; therefore, a slightly modified initial ISP is required.</p><p>From a more fundamental perspective, how is a raw image generated from the spectrum?</p>$$
R=\int P(\lambda)\,\bar{r}(\lambda)\,\mathrm{d}\lambda
$$<p>The equation above represents an ideal image Sensor&rsquo;s expression, where $P$ is the spectral power, $\bar{r}$ is the Sensor&rsquo;s Spectral Sensitivity Function, determined by the spectral characteristics of the photodiode, the transmittance of the colour filter, etc. $R$ is the output pixel value, which can be read from the raw file.</p><p>The expression for tristimulus values can be written as:</p>$$
X=\int P(\lambda)\,\bar{x}(\lambda)\, \mathrm{d}\lambda
$$<p>Here, $\bar{x}$ represents the Spectral Sensitivity Function of the human eye. Therefore, if we can linearly combine $\bar{r}(\lambda),\bar{g}(\lambda),\bar{b}(\lambda)$ to produce $\bar{x}(\lambda)$, etc., we can estimate the tristimulus values using the camera&rsquo;s raw pixel values. This linear combination process can be represented by a 3x3 matrix.</p><p><img alt="use linear combination to predict tristimulus" loading=lazy src=https://img.jackchou.top/jack-img/2025/03/0510acfc38f56697681c7a8b0a9032cf.svg></p><p>The image above illustrates using a linear combination to predict tristimulus values from the camera&rsquo;s Spectral Sensitivity Functions. The camera&rsquo;s Spectral Sensitivity Functions after linear combination have shapes similar to the Colour Matching Functions of the tristimulus values.</p><p>Additionally, a coefficient is needed for scaling to convert to tristimulus values representing Absolute Luminance. This coefficient can be calculated from the camera&rsquo;s Aperture, Shutter Speed, and ISO. These parameters used to control the amount of light entering do not affect the linearity or relative relationships of the light.</p><p>Another scenario involves using High Dynamic Range Images as input, such as HDR images encoded with the PQ Transfer Function. First, the non-linearly encoded RGB pixel values are decoded using the EOTF to obtain linear RGB pixel values. Then, the linear RGB pixel values are converted to the XYZ space to obtain the tristimulus values. Several of the .tif files and <code>image_input.py</code> provided in the project utilize this type of input.</p><h2 id=image-decomposition>Image Decomposition<a hidden class=anchor aria-hidden=true href=#image-decomposition>#</a></h2><p>According to the visual system&rsquo;s different perception of colour and detail, the image is decomposed into a Base Layer and a Details Layer. Operations on colour, such as Chromatic Adaptation and Tone Compression, are only applied to the Base Layer. The Details Layer, after enhancement or adjustment, is merged with the adjusted Base Layer.</p><p>The Base Layer is obtained using an edge-preserving Bilateral Filter, a method previously proposed by Durand and Dorsey. Bilateral Filtering is a non-linear filter where the weight of each pixel is jointly determined by a Gaussian filter in the spatial domain and another Gaussian filter in the intensity domain. The latter reduces the weight of pixels with significant intensity differences from the center pixel.</p><p>Therefore, Bilateral Filtering can effectively smooth the image while preserving sharp edges, thereby avoiding the &ldquo;halo&rdquo; artifacts common in local tone mapping operators. The intensity domain calculations are performed in Logarithmic Space, where intensity better represents perceived contrast and facilitates more uniform processing across the entire image.</p><p>The Details Layer is obtained by subtracting the Base Layer from the original image. Both layers need to be converted back to linear space.</p><p>The Bilateral Filtering used in iCAM06 is accelerated through piecewise linear approximation and nearest-neighbor downsampling.</p><p><img alt=detail_layer.jpg loading=lazy src=https://img.jackchou.top/jack-img/2025/05/bdb413bcbef2e15d50fb1c09a835c976.avif></p><h2 id=chromatic-adaptation>Chromatic Adaptation<a hidden class=anchor aria-hidden=true href=#chromatic-adaptation>#</a></h2><p>The colour of an object changes under different lighting and Viewing Conditions, but the human visual system can maintain a relatively stable perception of the object&rsquo;s colour to some extent. This phenomenon is called Colour Constancy. This process of maintaining relative stability is called Chromatic Adaptation.</p><p>A Chromatic Adaptation Transform (CAT) is a model used to predict corresponding colours. It takes two Viewing Conditions (usually represented by the tristimulus values of the scene white point) and a colour under one Viewing Condition as input, and predicts the colour under the other Viewing Condition that would be a corresponding colour.</p><p>According to von Kries&rsquo; hypothesis, Chromatic Adaptation is independent at the level of the visual organs. The basic structure of a Chromatic Adaptation Transform is:</p><ol><li>Convert the input XYZ to a space representing the visual organs (Cone responses).</li><li>Process each quantity independently in this space (e.g., multiply by respective gain factors).</li><li>Convert back to XYZ space to obtain the colour tristimulus values under the other Viewing Condition.</li></ol><p>There are many Chromatic Adaptation Transforms designed according to this structure, among which CAT02 and CAT16 are two CAT models successively recommended by CIE. In the second step, there is an adaptation degree D, representing the extent of Chromatic Adaptation. In iCAM06, this adaptation degree is multiplied by a coefficient of 0.3, which is equivalent to reducing the degree of adaptation, bringing it closer to the colours in the scene rather than the adapted corresponding colours, to increase the colour saturation of the image.</p><p>This is a very strange practice. I am more inclined to believe it is a code error that causes numerical errors if this coefficient is not multiplied. This is because the two Cone responses used in the Chromatic Adaptation step in the original code are one normalized and one absolute value.</p><p>In iCAM06, the target adaptation field for this step of Chromatic Adaptation is D65, because the subsequent uniform colour spaces are designed for the D65 white point. The white point of the adaptation field uses a Gaussian blur of the Base Layer, which is somewhat similar to the Grey World hypothesis.</p>$$
\begin{align*}
D &= 0.3 F \left[ 1 - \left( \frac{1}{3.6} \right) e^{-\frac{(L_A - 42)}{92}} \right] \\
R_c &= \left[ \left( R_{D65} \frac{D}{R_W} \right) + (1-D) \right] R
\end{align*}
$$<p>In the original equation, the sign of 42 in the exponent of $e$ in the calculation of adaptation degree D is incorrect.</p><p><img alt=white_adaptation.jpg loading=lazy src=https://img.jackchou.top/jack-img/2025/05/bac872be91ec0d5ac78fddbf998fe66b.avif>
The Adaptation White (Above) and Adapted Image (Below)
<img alt=XYZ_adapted.jpg loading=lazy src=https://img.jackchou.top/jack-img/2025/05/a503e3e840f310c2c7bedc670f0a52cd.avif></p><h2 id=tone-compression>Tone Compression<a hidden class=anchor aria-hidden=true href=#tone-compression>#</a></h2><p>The human eye&rsquo;s perception of luminance is not linear, but highly non-linear. By applying Tone Compression according to this non-linear characteristic, it is possible to reproduce Colour Appearance with a larger luminance range within a limited Display luminance range.</p><p>This non-linear relationship is also obtained from visual experiments. iCAM06 uses the post-adaptation part from CIECAM02, which is shaped like a Sigmoid function. When used, it involves converting from tristimulus values to another space representing visual cells, and then applying a response curve called &ldquo;post-adaptation&rdquo;. In iCAM06, the response of Rods under scotopic vision is also added and superimposed on the Cone response to predict luminance in the scotopic-mesopic range; the Rod response is very small.</p><p>The post-adaptation non-linear relationship for Cones is as follows:</p>$$
R'_a = \frac{400 (F_L R' / Y_W)^p}{27.13 + (F_L R' / Y_W)^p} + 0.1
$$<p>The reference white $Y_{W}$ used in this step is also a Gaussian blur of the Base Layer, but with a greater degree of blur than in Chromatic Adaptation.</p><p>This step completes the compression of luminance. The original large luminance range, after passing through this Sigmoid function, has a range of 0.1 to 400, although it rarely exceeds 200. Before this step, the relationship was linear with the scene light; after this step, it is linear with the Display light. Therefore, this step can also be understood as an Optic-Opto Transfer Function (OOTF).</p><p><img alt=XYZ_tone_compressed.jpg loading=lazy src=https://img.jackchou.top/jack-img/2025/05/aef5ee01b70874562aa5a36f2bf49add.avif></p><h2 id=merging-image-and-output>Merging Image and Output<a hidden class=anchor aria-hidden=true href=#merging-image-and-output>#</a></h2><p>After completing Chromatic Adaptation and Tone Compression, the Base Layer is an image that can be displayed relatively normally on a screen. The Details Layer can be enhanced and merged back.</p><p>The image obtained at this point is still in the linear XYZ tristimulus value space. Converting XYZ to the RGB space suitable for display involves two steps:</p><ol><li>Conversion to linear RGB space.</li><li>Applying the Transfer Function encoding.</li></ol><p>For the most common sRGB space, the matrix used in the first step is readily available online. The second step is a Gamma Correction, with a coefficient equal to the reciprocal of the Display&rsquo;s Gamma, usually between 0.45 and 0.5.</p><h2 id=additional-operations-ipt-space>Additional Operations: IPT Space<a hidden class=anchor aria-hidden=true href=#additional-operations-ipt-space>#</a></h2><p>Compressing an original high dynamic range, high luminance image onto a low luminance Display can sometimes result in less vivid colours, and the contrast between light and dark areas also needs to be enhanced.</p><p>The solution in iCAM06 is to convert to a uniform colour space for enhancement, choosing the IPT space. I represents Lightness, and P and T represent two colour directions, red-green and yellow-blue respectively.</p><p>The method for enhancing contrast is to apply a Gamma exponent between 1.0 and 1.5 to Lightness, with the value determined by the Viewing Environment. The principle is that perceived contrast changes according to the relative luminance of the Viewing Environment. Dark environments like cinemas require higher contrast, so a higher System Gamma exponent is usually adopted. A potential issue is that past System Gamma was applied to linear light, not to a non-linear scale like Lightness.</p><p>The method for enhancing Chroma is to stretch the two colour directions. The degree of stretching is related to luminance, based on the Hunt effect: an increase in luminance leads to an increase in perceived Colourfulness.</p>$$
P = P \cdot \left[ (F_L + 1)^{0.2} \left( \frac{1.29C^2 - 0.27C + 0.42}{C^2 - 0.31C + 0.42} \right) \right]
$$<p><img alt=output.jpg loading=lazy src=https://img.jackchou.top/jack-img/2025/05/6910b5899e4e8fe07ba2a92743d54ccf.avif></p><h2 id=results-and-analysis>Results and Analysis<a hidden class=anchor aria-hidden=true href=#results-and-analysis>#</a></h2><p>This algorithm addresses two problems:</p><ol><li>How to reproduce real-world scenes on a display.</li><li>How to reproduce high dynamic range images on traditional low dynamic range displays.</li></ol><p>Unlike computer vision, colour science focuses more on human visual perception, aiming to process images from a visual perspective. iCAM06, through methods such as Chromatic Adaptation, Tone Compression, and uniform colour spaces, provides an interpretable solution for image processing from high dynamic range to low dynamic range.</p><p>However, iCAM06 also has some shortcomings:</p><ol><li>The Chromatic Adaptation algorithm has issues, and the effect after correction is not ideal, possibly due to the limitations of the Chromatic Adaptation model and the influence of the Grey World hypothesis.</li><li>The Sigmoid function used for Tone Compression reduces image contrast too much and cannot balance the effects for both low dynamic range and high dynamic range inputs.</li><li>Edge-preserving transformation and detail enhancement may introduce artifacts and excessive sharpening.</li><li>Processing in a uniform colour space lacks reliable theoretical basis, especially the practice of applying a gamma exponent to Lightness.</li></ol><p>Overall, iCAM06, leveraging research from colour science, proposes an effective method for high dynamic range image processing and is a successful exploration of integrating colour science into image processing.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] M. D. Fairchild and G. M. Johnson, &ldquo;Meet iCAM: A next-generation color appearance model,&rdquo; <em>Proc. 10th Color Imaging Conf.</em>, vol. 10, no. 1, pp. 33‚Äì38, Jan. 2002.</p><p>[2] J. Kuang, G. M. Johnson, and M. D. Fairchild, &ldquo;iCAM06: A refined image appearance model for HDR image rendering,&rdquo; <em>J. Visual Communication and Image Representation</em>, vol. 18, no. 5, pp. 406‚Äì414, Oct. 2007.</p><p>[3] F. Durand and J. Dorsey, &ldquo;Fast bilateral filtering for the display of high-dynamic-range images,&rdquo; in <em>Proc. 29th Annual Conf. Computer Graphics and Interactive Techniques (SIGGRAPH)</em>, San Antonio, TX, USA, Jul. 2002, pp. 257‚Äì266.</p><p>[4] P. Hung and R. S. Berns, &ldquo;Determination of constant hue loci for a CRT gamut and their predictions using color appearance spaces,&rdquo; <em>Color Research & Application</em>, vol. 20, no. 5, pp. 285‚Äì295, Oct. 1995.</p><p>[5] M. R. Luo and C. Li, &ldquo;CIECAM02 and its recent developments,&rdquo; in <em>Advanced Color Image Processing and Analysis</em>, C. Fernandez-Maloigne, Ed., New York, NY, USA: Springer, 2013, pp. 19‚Äì58.</p><p>[6] M. D. Fairchild, &ldquo;A revision of CIECAM97s for practical applications,&rdquo; <em>Color Research & Application</em>, vol. 26, no. 6, pp. 418‚Äì427, 2001.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://jackchou.top/en/tags/colour/>Colour</a></li><li><a href=https://jackchou.top/en/tags/cam/>CAM</a></li><li><a href=https://jackchou.top/en/tags/isp/>ISP</a></li><li><a href=https://jackchou.top/en/tags/hdr/>HDR</a></li></ul><nav class=paginav><a class=prev href=https://jackchou.top/en/posts/bt2408/><span class=title>¬´ Prev</span><br><span>An Experiment to Understand HDR Transfer Functions</span>
</a><a class=next href=https://jackchou.top/en/posts/hdr-format-conversion/><span class=title>Next ¬ª</span><br><span>HDR Image Format Conversion</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://jackchou.top/en/>JacksBlog</a></span> ¬∑
my friends&rsquo; websites: <a href=https://zhxwu.com/>zhxwu.com</a>, <a href=https://ylqian.com/>ylqian.com</a> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script defer crossorigin=anonymous src=/js/site.min.a576538c52b362e170acc02d53f5ce6045117863354954a8f91f10c7217d94ab.js integrity="sha256-pXZTjFKzYuFwrMAtU/XOYEUReGM1SVSo+R8QxyF9lKs="></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>