<!DOCTYPE html>
<html lang="en"
    dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform | JacksBlog</title>
<meta name="keywords" content="Colour Constancy, Chromatic Adaptation, Chromatic Adaptation Transform, von Kries Hypothesis, Apple True Tone Technology, CAT16 Chromatic Adaptation Transform, Corresponding Colours Dataset, Chromatic Adaptation Gain Coefficients">
<meta name="description" content="Learn chromatic adaptation models (CAT02, CAT16, von Kries) for color constancy‚Äîstable color perception under varying lighting in visual science.">
<meta name="author" content="Miaosen Zhou">
<link rel="canonical" href="https://jackchou.top/en/posts/pcrdi04/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e30af01949f15b214a659db89950589c44040efcfcdeec2087ad8629d77c216e.css" integrity="sha256-4wrwGUnxWyFKZZ24mVBYnEQEDvz83uwgh62GKdd8IW4=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jackchou.top/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jackchou.top/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jackchou.top/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jackchou.top/apple-touch-icon.png">
<link rel="mask-icon" href="https://jackchou.top/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jackchou.top/posts/pcrdi04/">
<link rel="alternate" hreflang="en" href="https://jackchou.top/en/posts/pcrdi04/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Google&#43;Sans&#43;Code:wght@400..700&amp;family=Noto&#43;Sans&#43;SC:wght@400..700&amp;family=Sen:wght@400..700&amp;display=swap">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google&#43;Sans&#43;Code:wght@400..700&amp;family=Noto&#43;Sans&#43;SC:wght@400..700&amp;family=Sen:wght@400..700&amp;display=swap" media="print" onload="this.media='all'">
<noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google&#43;Sans&#43;Code:wght@400..700&amp;family=Noto&#43;Sans&#43;SC:wght@400..700&amp;family=Sen:wght@400..700&amp;display=swap"></noscript>
<meta property="og:url" content="https://jackchou.top/en/posts/pcrdi04/">
  <meta property="og:site_name" content="JacksBlog">
  <meta property="og:title" content="Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform">
  <meta property="og:description" content="Learn chromatic adaptation models (CAT02, CAT16, von Kries) for color constancy‚Äîstable color perception under varying lighting in visual science.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-03T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-07-24T21:31:53+08:00">
    <meta property="article:tag" content="Colour">
    <meta property="article:tag" content="PCRDI">
      <meta property="og:image" content="https://img.jackchou.top/jack-img/2025/08/81bb602c03f9704ee42e292468396187.webp">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://img.jackchou.top/jack-img/2025/08/81bb602c03f9704ee42e292468396187.webp">
<meta name="twitter:title" content="Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform">
<meta name="twitter:description" content="Learn chromatic adaptation models (CAT02, CAT16, von Kries) for color constancy‚Äîstable color perception under varying lighting in visual science.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts üå≥",
      "item": "https://jackchou.top/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform",
      "item": "https://jackchou.top/en/posts/pcrdi04/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform",
  "name": "Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform",
  "description": "Learn chromatic adaptation models (CAT02, CAT16, von Kries) for color constancy‚Äîstable color perception under varying lighting in visual science.",
  "keywords": [
    "Colour Constancy", "Chromatic Adaptation", "Chromatic Adaptation Transform", "von Kries Hypothesis", "Apple True Tone Technology", "CAT16 Chromatic Adaptation Transform", "Corresponding Colours Dataset", "Chromatic Adaptation Gain Coefficients"
  ],
  "articleBody": " For more information on chromatic adaptation and chromatic adaptation models, please refer to section 1.2.1 of this paper:\nQ. Zhai, ‚ÄòChromatic Adaptation to Illumination and Colour Quality‚Äô, PhD dissertation, Zhejiang University, 2018.\nColour Constancy The colour of an object changes under different lighting conditions and viewing environments, but the human visual system can maintain a stable perception of the object‚Äôs colour to a certain extent. This phenomenon is known as colour constancy. The process of maintaining this relative stability is called chromatic adaptation.\nWhen the lighting environment changes, chromatic adaptation takes some time to complete. The figure shows the experimental results from Fairchild and Reniff (1995), illustrating the relationship between the proportion of steady-state adaptation and time for three observers, switching from illuminant A to D65.\nM. D. Fairchild and L. Reniff, ‚ÄòTime course of chromatic adaptation for color-appearance judgments‚Äô, J. Opt. Soc. Am. A, vol. 12, no. 5, p. 824, May 1995, doi: 10.1364/JOSAA.12.000824.\nThe formation mechanism of chromatic adaptation can be broadly divided into two parts: sensory and cognitive.\nThe sensory mechanism suggests that the three types of cone cells on the retina automatically and independently adjust their gain according to the intensity of light. When the response of a certain type of cone cell increases, its gain is reduced, and the gain adjustments of the three types of cone cells are independent. The fundamental hypothesis proposed by von Kries in 1902 posits that the fatigue or adaptation of each component of the visual organ is independent of the others (the concept of cone cells did not exist at the time). The von Kries hypothesis is the foundation of all chromatic adaptation models.\n‚ÄúThis can be conceived in the sense that the individual components present in the organ of vision are completely independent of one another and each is fatigued or adapted exclusively according to its own function.‚Äù\nThe cognitive mechanism is more complex and suggests that a person‚Äôs perception of an object‚Äôs colour is also influenced by the object itself. For example, grass is green, apples are red, and the sky is blue; vision can maintain a stable perception of the colours of these objects under various lighting conditions. The formation of this cognitive mechanism may be due to the accumulation of experience with object colours over a lifetime. Chromatic adaptation resulting from cognitive mechanisms is often incomplete.\nA typical example of a cognitive mechanism is ‚Äòdiscounting the illuminant‚Äô. This refers to the ability of an observer to judge an object‚Äôs colour based on its inherent properties (reflectance) rather than the light source. For instance, coal is black during the day and snow is white at night, but in reality, the luminance of coal during the day is higher. The visual system perceives primarily the reflectance of the coal and snow. Discounting the illuminant is very important in cross-media colour reproduction. Cross-media colour reproduction refers to displaying colours using different media, such as a self-luminous display and colours printed on paper. The self-luminous display is itself a light source, so there is no phenomenon of discounting the illuminant, whereas for printed colours, the observer can, to some extent, ignore the influence of the illuminating light source.\nApple‚Äôs True Tone technology, introduced on the iPhone 8 and iPhone X, adjusts the display‚Äôs colour based on changes in ambient lighting. This simulates the phenomenon of discounting the illuminant, making the display appear like a printed paper under the same light source, which enhances viewing comfort to some extent.\nChromatic Adaptation Transform A chromatic adaptation transform establishes a relationship between ‚Äòcorresponding colours‚Äô. Corresponding colours are two colours that match under different viewing conditions. Imagine an observer with perfect colour constancy; if the light source in a scene is changed, the colour they perceive remains the same (i.e., it always matches). In this case, the colours of the same object under the different light sources form a pair of corresponding colours. Colours are typically represented by their XYZ tristimulus values. If the tristimulus values under the first viewing condition are $X_1, Y_1, Z_1$, and under the second viewing condition are $X_2, Y_2, Z_2$, then these two sets of tristimulus values are corresponding colours for those two viewing conditions.\nA Chromatic Adaptation Transform (CAT) is a model used to predict corresponding colours. The inputs are two viewing conditions (usually represented by the tristimulus values of the scene white point), and a colour under one of the viewing conditions (represented by its tristimulus values). The model predicts the colour under the other viewing condition that forms a corresponding pair (also represented by tristimulus values).\nBuilding a CAT model requires corresponding colour datasets for training and validation. The following are some common experimental methods for creating these datasets:\nHaploscopic matching: An experimental apparatus is designed to separate the visual fields of the left and right eyes, placing them under different viewing conditions. The observer then compares and matches the colour stimuli from both sides. This method cannot be used to study chromatic adaptation caused by cognitive mechanisms. Memory matching: The subject memorises a colour stimulus under one viewing condition and then matches it under another viewing condition. Magnitude estimation: The subject ‚Äòrates‚Äô colour stimuli in different viewing environments, for example, by estimating numerical values for lightness, saturation, hue, etc. I have not designed, conducted, or participated in any experiments on chromatic adaptation. From a purely subjective perspective, I believe collecting corresponding colour data is very difficult. Each of the three methods mentioned above has its own drawbacks. For instance, in memory matching, human short-term memory for colour is very limited. In magnitude estimation, having subjects assign scores to a subjective value requires careful experimental design to standardise the rating criteria among subjects.\nBasic Structure of a CAT According to the von Kries hypothesis, chromatic adaptation is independent at the level of the visual organs. The basic structure of a chromatic adaptation transform is as follows:\nTransform the input XYZ values into a space that represents the visual organs. Process each component independently within this space (e.g., by multiplying by its respective gain coefficient). Transform back to the XYZ space to obtain the colour tristimulus values for the other viewing condition. Von Kries himself never provided a specific CAT method, but simple chromatic adaptation models, such as the Ives and Helson models, can be built based on his hypothesis.\nM. H. Brill, ‚ÄòThe relation between the color of the illuminant and the color of the illuminated object‚Äô, Color Research \u0026 Application, vol. 20, no. 1, pp. 70‚Äì76, Feb. 1995, doi: 10.1002/col.5080200112.\nH. Helson, ‚ÄòSome Factors and Implications of Color Constancy*‚Äô, J. Opt. Soc. Am., vol. 33, no. 10, p. 555, Oct. 1943, doi: 10.1364/JOSA.33.000555.\nFirst, the light sources or white points of the two viewing conditions and one colour stimulus (\\(\\text{XYZ}_{w1}, \\text{XYZ}_{w2}, \\text{XYZ}_{1}\\)) are transformed into the LMS relative cone response space. This can be done using a 3x3 matrix. Then, each component is multiplied by a gain coefficient, which can be represented as multiplication by a diagonal matrix. Finally, the values are transformed back to the XYZ space by multiplying by the inverse of the first matrix, yielding $\\text{XYZ}_{2}$, sometimes written as $\\text{XYZ}_c$ to denote the corresponding colour.\nThe gain coefficients are the ratio of the cone responses to the light sources or white points of the two viewing conditions, reflecting the hypothesis that ‚Äòa visual organ with a larger stimulus will automatically adjust to reduce its gain‚Äô. For example:\n$$ L_2 = \\frac{L_{w2}}{L_{w1}} L_1 $$Such simple linear models are already capable of predicting the data in corresponding colour datasets quite well.\nImprovements to CATs Some research has proposed using non-linear adjustments or non-independent adjustments with cross-channel interactions in the second step to try to improve chromatic adaptation transforms. For example, Nayatani and Guth used a power function instead of linear gain. However, these methods did not achieve significantly better results.\nChanging the transformation matrix from XYZ tristimulus values to the LMS relative cone space may yield better results. In this case, the space is no longer referred to as LMS, but as RGB. Examples include the HPE transformation matrix used by Fairchild, the BFD transformation matrix by Bradford, and the matrices used in CAT02 and CAT16.\nAdditionally, although non-linear adjustments do not yield better results, improvements can be made to the coefficients used in linear gain. For example, CMC-CAT and CAT02 introduced the concept of a degree of adaptation, D, to control the completeness of the adaptation. D ranges between 0 and 1, where 1 represents complete adaptation and 0 represents no adaptation. In CAT02, D is a value related to the luminance of the adapting field on the input side, and a factor F, representing the ambient surround (bright or dark), is also included.\n$$ D = F \\cdot \\left[1 - \\frac{1}{3.6} e^{\\frac{-L_A - 42}{92}}\\right] $$Here, $L_A$ is the luminance of the adapting field on the input side in cd/m¬≤, and $F$ is the surround factor. It is set to 1.0 for an average surround, 0.9 for a dim surround, and 0.8 for a dark surround. The choice is determined by the relative luminance, for example, a bright office, watching television indoors, or a dark cinema.\nCAT16 The current CIE recommended chromatic adaptation transform is CAT16, which is a linear transform. The process for the one-step CAT16 is as follows.\nInputs: White point of the input adapting field \\(\\text{XYZ}_{w}\\), white point of the output (reference) adapting field $\\text{XYZ}_{wr}$, input colour $\\text{XYZ}$, luminance of the adapting field light source $L_A$, and surround factor $F$.\nTransform \\(\\text{XYZ}_{w}\\), \\(\\text{XYZ}_{wr}\\), and $\\text{XYZ}$ into the RGB space using the transformation matrix \\(\\mathbf{M}_{16}\\).\n\\[ \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} = \\begin{bmatrix} 0.401288 \u0026 0.650173 \u0026 -0.051461 \\\\ -0.250268 \u0026 1.204414 \u0026 0.045854 \\\\ -0.002079 \u0026 0.048952 \u0026 0.953127 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} \\] Perform the adaptation transform on each of the three RGB channels separately. The gain coefficients depend on the degree of adaptation, D, which is calculated in the same way as in CAT02. For the G and B gain coefficients, $R_{wr}$ and $R_w$ are replaced accordingly.\n\\[ k_R = D \\cdot \\frac{Y_w}{Y_{wr}} \\cdot \\frac{R_{wr}}{R_w} + 1 - D \\\\ R_c = k_R \\cdot R \\] Transform the adapted RGB values back to the XYZ space using \\(\\mathbf{M}_{16}^{-1}\\). The subscript c or r is used to denote the corresponding colour or the reference field, respectively.\n\\[ \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\end{bmatrix} = \\mathbf{M}_{16}^{-1} \\begin{bmatrix} R_c \\\\ G_c \\\\ B_c \\end{bmatrix} \\] Due to the presence of the degree of adaptation D, CAT16 is not reversible in most cases. There is also a two-step version of CAT16 designed to solve the problem where the original input cannot be recovered after inverting the one-step method. The two-step method defines an intermediate illuminant, such as an equal-energy white, and works by adapting the input field to the equal-energy white field and then predicting the corresponding colour for the output field in reverse from the equal-energy white field.\nIf we use \\(\\Lambda_{r,t}\\) to denote the linear gain diagonal matrix from the input field t to the output field r, then the total transformation matrix for the one-step method is:\n\\[ \\Phi_{r,t} = \\mathbf{M}_{16}^{-1} \\Lambda_{r,t} \\mathbf{M}_{16} \\]The total transformation matrix for the two-step method is:\n\\[ \\begin{align*} \\Pi_{r,t} \u0026= \\Psi_{r,se} \\Phi_{se,t} \\\\ \u0026= \\mathbf{M}_{16}^{-1} \\Lambda_{se,r}^{-1} \\mathbf{M}_{16} \\mathbf{M}_{16}^{-1} \\Lambda_{se,t} \\mathbf{M}_{16} \\\\ \u0026= \\mathbf{M}_{16}^{-1} \\Lambda_{se,r}^{-1} \\Lambda_{se,t} \\mathbf{M}_{16} \\end{align*} \\]Here, ‚Äòse‚Äô denotes the equal-energy white. In practice, there is almost no difference in the results between the two-step and one-step methods, and the one-step method is currently more commonly used.\n",
  "wordCount" : "1923",
  "inLanguage": "en",
  "image": "https://img.jackchou.top/jack-img/2025/08/81bb602c03f9704ee42e292468396187.webp","datePublished": "2025-02-03T00:00:00+08:00",
  "dateModified": "2025-07-24T21:31:53+08:00",
  "author":{
    "@type": "Person",
    "name": "Miaosen Zhou"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jackchou.top/en/posts/pcrdi04/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JacksBlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jackchou.top/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jackchou.top/en/" accesskey="h" title="JacksBlog (Alt + H)">JacksBlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jackchou.top/" title="‰∏≠Êñá"
                            aria-label="‰∏≠Êñá">‰∏≠Êñá</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jackchou.top/en/posts/" title="Posts üå≥">
                    <span>Posts üå≥</span>
                </a>
            </li>
            <li>
                <a href="https://jackchou.top/en/photos/" title="Photos üì∑">
                    <span>Photos üì∑</span>
                </a>
            </li>
            <li>
                <a href="https://jackchou.top/en/tags/" title="Tags üè∑Ô∏è">
                    <span>Tags üè∑Ô∏è</span>
                </a>
            </li>
            <li>
                <a href="https://jackchou.top/en/about/" title="About üòº">
                    <span>About üòº</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Principles of Colour Reproduction in Digital Images Notes: Chromatic Adaptation Transform
    </h1>
    
    <div class="post-meta"><span title='2025-02-03 00:00:00 +0800 +0800'>2025.02.03</span>&nbsp;|&nbsp;Translations:
<ul class="i18n_list">
    <li>
        <a href="https://jackchou.top/posts/pcrdi04/">‰∏≠Êñá</a>
    </li>
</ul>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#colour-constancy" aria-label="Colour Constancy">Colour Constancy</a></li>
                <li>
                    <a href="#chromatic-adaptation-transform" aria-label="Chromatic Adaptation Transform">Chromatic Adaptation Transform</a><ul>
                        
                <li>
                    <a href="#basic-structure-of-a-cat" aria-label="Basic Structure of a CAT">Basic Structure of a CAT</a></li>
                <li>
                    <a href="#improvements-to-cats" aria-label="Improvements to CATs">Improvements to CATs</a></li>
                <li>
                    <a href="#cat16" aria-label="CAT16">CAT16</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>For more information on chromatic adaptation and chromatic adaptation models, please refer to section 1.2.1 of this paper:</p>
<p>Q. Zhai, ‚ÄòChromatic Adaptation to Illumination and Colour Quality‚Äô, PhD dissertation, Zhejiang University, 2018.</p>
</blockquote>
<h2 id="colour-constancy">Colour Constancy<a hidden class="anchor" aria-hidden="true" href="#colour-constancy">#</a></h2>
<p>The colour of an object changes under different lighting conditions and viewing environments, but the human visual system can maintain a stable perception of the object&rsquo;s colour to a certain extent. This phenomenon is known as colour constancy. The process of maintaining this relative stability is called chromatic adaptation.</p>
<p>When the lighting environment changes, chromatic adaptation takes some time to complete. The figure shows the experimental results from Fairchild and Reniff (1995), illustrating the relationship between the proportion of steady-state adaptation and time for three observers, switching from illuminant A to D65.</p>
<p><img alt="Degree of chromatic adaptation over time" loading="lazy" src="https://img.jackchou.top/jack-img/2025/02/6434b48a712e4912d051a6785c673ca8.avif"></p>
<blockquote>
<p>M. D. Fairchild and L. Reniff, ‚ÄòTime course of chromatic adaptation for color-appearance judgments‚Äô, J. Opt. Soc. Am. A, vol. 12, no. 5, p. 824, May 1995, doi: 10.1364/JOSAA.12.000824.</p>
</blockquote>
<p>The formation mechanism of chromatic adaptation can be broadly divided into two parts: sensory and cognitive.</p>
<p>The sensory mechanism suggests that the three types of cone cells on the retina automatically and independently adjust their gain according to the intensity of light. When the response of a certain type of cone cell increases, its gain is reduced, and the gain adjustments of the three types of cone cells are independent. The fundamental hypothesis proposed by von Kries in 1902 posits that the fatigue or adaptation of each component of the visual organ is independent of the others (the concept of cone cells did not exist at the time). The von Kries hypothesis is the foundation of all chromatic adaptation models.</p>
<blockquote>
<p>&ldquo;This can be conceived in the sense that the individual components present in the organ of vision are completely independent of one another and each is fatigued or adapted exclusively according to its own function.&rdquo;</p>
</blockquote>
<p>The cognitive mechanism is more complex and suggests that a person&rsquo;s perception of an object&rsquo;s colour is also influenced by the object itself. For example, grass is green, apples are red, and the sky is blue; vision can maintain a stable perception of the colours of these objects under various lighting conditions. The formation of this cognitive mechanism may be due to the accumulation of experience with object colours over a lifetime. Chromatic adaptation resulting from cognitive mechanisms is often incomplete.</p>
<p>A typical example of a cognitive mechanism is &lsquo;discounting the illuminant&rsquo;. This refers to the ability of an observer to judge an object&rsquo;s colour based on its inherent properties (reflectance) rather than the light source. For instance, coal is black during the day and snow is white at night, but in reality, the luminance of coal during the day is higher. The visual system perceives primarily the reflectance of the coal and snow. Discounting the illuminant is very important in cross-media colour reproduction. Cross-media colour reproduction refers to displaying colours using different media, such as a self-luminous display and colours printed on paper. The self-luminous display is itself a light source, so there is no phenomenon of discounting the illuminant, whereas for printed colours, the observer can, to some extent, ignore the influence of the illuminating light source.</p>
<p>Apple&rsquo;s True Tone technology, introduced on the iPhone 8 and iPhone X, adjusts the display&rsquo;s colour based on changes in ambient lighting. This simulates the phenomenon of discounting the illuminant, making the display appear like a printed paper under the same light source, which enhances viewing comfort to some extent.</p>
<h2 id="chromatic-adaptation-transform">Chromatic Adaptation Transform<a hidden class="anchor" aria-hidden="true" href="#chromatic-adaptation-transform">#</a></h2>
<p>A chromatic adaptation transform establishes a relationship between &lsquo;corresponding colours&rsquo;. Corresponding colours are two colours that match under different viewing conditions. Imagine an observer with perfect colour constancy; if the light source in a scene is changed, the colour they perceive remains the same (i.e., it always matches). In this case, the colours of the same object under the different light sources form a pair of corresponding colours. Colours are typically represented by their XYZ tristimulus values. If the tristimulus values under the first viewing condition are $X_1, Y_1, Z_1$, and under the second viewing condition are $X_2, Y_2, Z_2$, then these two sets of tristimulus values are corresponding colours for those two viewing conditions.</p>
<p>A Chromatic Adaptation Transform (CAT) is a model used to predict corresponding colours. The inputs are two viewing conditions (usually represented by the tristimulus values of the scene white point), and a colour under one of the viewing conditions (represented by its tristimulus values). The model predicts the colour under the other viewing condition that forms a corresponding pair (also represented by tristimulus values).</p>
<p>Building a CAT model requires corresponding colour datasets for training and validation. The following are some common experimental methods for creating these datasets:</p>
<ol>
<li>Haploscopic matching: An experimental apparatus is designed to separate the visual fields of the left and right eyes, placing them under different viewing conditions. The observer then compares and matches the colour stimuli from both sides. This method cannot be used to study chromatic adaptation caused by cognitive mechanisms.</li>
<li>Memory matching: The subject memorises a colour stimulus under one viewing condition and then matches it under another viewing condition.</li>
<li>Magnitude estimation: The subject &lsquo;rates&rsquo; colour stimuli in different viewing environments, for example, by estimating numerical values for lightness, saturation, hue, etc.</li>
</ol>
<p><em>I have not designed, conducted, or participated in any experiments on chromatic adaptation. From a purely subjective perspective, I believe collecting corresponding colour data is very difficult. Each of the three methods mentioned above has its own drawbacks. For instance, in memory matching, human short-term memory for colour is very limited. In magnitude estimation, having subjects assign scores to a subjective value requires careful experimental design to standardise the rating criteria among subjects.</em></p>
<h3 id="basic-structure-of-a-cat">Basic Structure of a CAT<a hidden class="anchor" aria-hidden="true" href="#basic-structure-of-a-cat">#</a></h3>
<p>According to the von Kries hypothesis, chromatic adaptation is independent at the level of the visual organs. The basic structure of a chromatic adaptation transform is as follows:</p>
<ol>
<li>Transform the input XYZ values into a space that represents the visual organs.</li>
<li>Process each component independently within this space (e.g., by multiplying by its respective gain coefficient).</li>
<li>Transform back to the XYZ space to obtain the colour tristimulus values for the other viewing condition.</li>
</ol>
<p>Von Kries himself never provided a specific CAT method, but simple chromatic adaptation models, such as the Ives and Helson models, can be built based on his hypothesis.</p>
<blockquote>
<p>M. H. Brill, ‚ÄòThe relation between the color of the illuminant and the color of the illuminated object‚Äô, Color Research &amp; Application, vol. 20, no. 1, pp. 70‚Äì76, Feb. 1995, doi: 10.1002/col.5080200112.</p>
<p>H. Helson, ‚ÄòSome Factors and Implications of Color Constancy*‚Äô, J. Opt. Soc. Am., vol. 33, no. 10, p. 555, Oct. 1943, doi: 10.1364/JOSA.33.000555.</p>
</blockquote>
<p>First, the light sources or white points of the two viewing conditions and one colour stimulus (\(\text{XYZ}_{w1}, \text{XYZ}_{w2}, \text{XYZ}_{1}\)) are transformed into the LMS relative cone response space. This can be done using a 3x3 matrix. Then, each component is multiplied by a gain coefficient, which can be represented as multiplication by a diagonal matrix. Finally, the values are transformed back to the XYZ space by multiplying by the inverse of the first matrix, yielding $\text{XYZ}_{2}$, sometimes written as $\text{XYZ}_c$ to denote the corresponding colour.</p>
<p>The gain coefficients are the ratio of the cone responses to the light sources or white points of the two viewing conditions, reflecting the hypothesis that &lsquo;a visual organ with a larger stimulus will automatically adjust to reduce its gain&rsquo;. For example:</p>
$$
L_2 = \frac{L_{w2}}{L_{w1}} L_1
$$<p>Such simple linear models are already capable of predicting the data in corresponding colour datasets quite well.</p>
<h3 id="improvements-to-cats">Improvements to CATs<a hidden class="anchor" aria-hidden="true" href="#improvements-to-cats">#</a></h3>
<p>Some research has proposed using non-linear adjustments or non-independent adjustments with cross-channel interactions in the second step to try to improve chromatic adaptation transforms. For example, Nayatani and Guth used a power function instead of linear gain. However, these methods did not achieve significantly better results.</p>
<p>Changing the transformation matrix from XYZ tristimulus values to the LMS relative cone space may yield better results. In this case, the space is no longer referred to as LMS, but as RGB. Examples include the HPE transformation matrix used by Fairchild, the BFD transformation matrix by Bradford, and the matrices used in CAT02 and CAT16.</p>
<p>Additionally, although non-linear adjustments do not yield better results, improvements can be made to the coefficients used in linear gain. For example, CMC-CAT and CAT02 introduced the concept of a degree of adaptation, D, to control the completeness of the adaptation. D ranges between 0 and 1, where 1 represents complete adaptation and 0 represents no adaptation. In CAT02, D is a value related to the luminance of the adapting field on the input side, and a factor F, representing the ambient surround (bright or dark), is also included.</p>
$$
D = F \cdot \left[1 - \frac{1}{3.6} e^{\frac{-L_A - 42}{92}}\right]
$$<p>Here, $L_A$ is the luminance of the adapting field on the input side in cd/m¬≤, and $F$ is the surround factor. It is set to 1.0 for an average surround, 0.9 for a dim surround, and 0.8 for a dark surround. The choice is determined by the relative luminance, for example, a bright office, watching television indoors, or a dark cinema.</p>
<h3 id="cat16">CAT16<a hidden class="anchor" aria-hidden="true" href="#cat16">#</a></h3>
<p>The current CIE recommended chromatic adaptation transform is CAT16, which is a linear transform. The process for the one-step CAT16 is as follows.</p>
<p>Inputs: White point of the input adapting field \(\text{XYZ}_{w}\), white point of the output (reference) adapting field $\text{XYZ}_{wr}$, input colour $\text{XYZ}$, luminance of the adapting field light source $L_A$, and surround factor $F$.</p>
<ol>
<li>
<p>Transform \(\text{XYZ}_{w}\), \(\text{XYZ}_{wr}\), and $\text{XYZ}$ into the RGB space using the transformation matrix \(\mathbf{M}_{16}\).</p>
\[
        \begin{bmatrix}
        R \\ G \\ B
        \end{bmatrix}
        =
        \begin{bmatrix}
        0.401288 & 0.650173 & -0.051461 \\
        -0.250268 & 1.204414 & 0.045854 \\
        -0.002079 & 0.048952 & 0.953127
        \end{bmatrix}
        \begin{bmatrix}
        X \\ Y \\ Z
        \end{bmatrix}
    \]</li>
<li>
<p>Perform the adaptation transform on each of the three RGB channels separately. The gain coefficients depend on the degree of adaptation, D, which is calculated in the same way as in CAT02. For the G and B gain coefficients, $R_{wr}$ and $R_w$ are replaced accordingly.</p>
\[
        k_R = D \cdot \frac{Y_w}{Y_{wr}} \cdot \frac{R_{wr}}{R_w} + 1 - D \\
        R_c = k_R \cdot R
    \]</li>
<li>
<p>Transform the adapted RGB values back to the XYZ space using \(\mathbf{M}_{16}^{-1}\). The subscript c or r is used to denote the corresponding colour or the reference field, respectively.</p>
\[
        \begin{bmatrix}
        X_c \\ Y_c \\ Z_c
        \end{bmatrix}
        =
        \mathbf{M}_{16}^{-1}
        \begin{bmatrix}
        R_c \\ G_c \\ B_c
        \end{bmatrix}
    \]</li>
</ol>
<p>Due to the presence of the degree of adaptation D, CAT16 is not reversible in most cases. There is also a two-step version of CAT16 designed to solve the problem where the original input cannot be recovered after inverting the one-step method. The two-step method defines an intermediate illuminant, such as an equal-energy white, and works by adapting the input field to the equal-energy white field and then predicting the corresponding colour for the output field in reverse from the equal-energy white field.</p>
<p>If we use \(\Lambda_{r,t}\) to denote the linear gain diagonal matrix from the input field t to the output field r, then the total transformation matrix for the one-step method is:</p>
\[
\Phi_{r,t} = \mathbf{M}_{16}^{-1} \Lambda_{r,t} \mathbf{M}_{16}
\]<p>The total transformation matrix for the two-step method is:</p>
\[
\begin{align*}
\Pi_{r,t} &= \Psi_{r,se} \Phi_{se,t} \\
          &= \mathbf{M}_{16}^{-1} \Lambda_{se,r}^{-1} \mathbf{M}_{16} \mathbf{M}_{16}^{-1} \Lambda_{se,t} \mathbf{M}_{16} \\
          &= \mathbf{M}_{16}^{-1} \Lambda_{se,r}^{-1} \Lambda_{se,t} \mathbf{M}_{16}
\end{align*}
\]<p>Here, &lsquo;se&rsquo; denotes the equal-energy white. In practice, there is almost no difference in the results between the two-step and one-step methods, and the one-step method is currently more commonly used.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jackchou.top/en/tags/colour/">Colour</a></li>
      <li><a href="https://jackchou.top/en/tags/pcrdi/">PCRDI</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jackchou.top/en/posts/pcrdi05/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>Principles of Colour Reproduction in Digital Images Notes: Colour Appearance</span>
  </a>
  <a class="next" href="https://jackchou.top/en/posts/pcrdi03/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Principles of Colour Reproduction in Digital Images Notes: Illuminants and Light Sources</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jackchou.top/en/">JacksBlog</a></span> ¬∑ 
        my friends&rsquo; websites: <a href="https://zhxwu.com/">zhxwu.com</a>, <a href="https://ylqian.com/">ylqian.com</a> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><div style="text-align: center; margin-top: 10px; margin-bottom: 20px; width: 100%; display: flex; justify-content: center;">
    <img src="https://visitor-badge.laobi.icu/badge?page_id=jacksblog" alt="visitors" width="88" height="20" style="height: auto;" loading="lazy" />
</div><script>
(() => {
    const images = document.querySelectorAll('.post-content img');
    if (!images.length) return;

    const overlay = document.createElement('div');
    overlay.className = 'image-lightbox';
    overlay.innerHTML = '<img class="image-lightbox__img" alt="" />';

    const overlayImg = overlay.querySelector('img');
    const closeOverlay = () => {
        overlay.classList.remove('is-visible');
        overlayImg.removeAttribute('src');
        document.body.style.removeProperty('overflow');
    };

    overlay.addEventListener('click', closeOverlay);
    document.addEventListener('keyup', (event) => {
        if (event.key === 'Escape' && overlay.classList.contains('is-visible')) {
            closeOverlay();
        }
    });

    document.body.appendChild(overlay);

    images.forEach((img) => {
        img.addEventListener('click', (event) => {
            event.preventDefault();
            event.stopPropagation();

            const source = img.dataset.zoomSrc || img.currentSrc || img.src;
            const cleanSrc = source.split('#')[0];

            overlayImg.src = cleanSrc;
            overlayImg.alt = img.alt || '';

            overlay.classList.add('is-visible');
            document.body.style.setProperty('overflow', 'hidden');
        });
    });
})();
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
